<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Bill's Blog]]></title>
  <link href="http://ibillxia.github.com/atom.xml" rel="self"/>
  <link href="http://ibillxia.github.com/"/>
  <updated>2013-05-24T23:16:12+08:00</updated>
  <id>http://ibillxia.github.com/</id>
  <author>
    <name><![CDATA[Bill Xia]]></name>
    <email><![CDATA[ibillxia@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[大白鼠听人话]]></title>
    <link href="http://ibillxia.github.com/blog/2013/05/24/cctv-news-rat-understand-what-human-says/"/>
    <updated>2013-05-24T22:59:00+08:00</updated>
    <id>http://ibillxia.github.com/blog/2013/05/24/cctv-news-rat-understand-what-human-says</id>
    <content type="html"><![CDATA[<p>最近一直忙着准备给媒体展示的音控大鼠机器人一不小心上了CCTV了，虽然自己感觉没什么了不起的，也不知道网络上是什么评论。
但既然上了CCTV，还是发博纪念一下吧</p>




<p>央视新闻视频链接：<a href="http://tv.cntv.cn/vodplay/e58c8785e00a4ead9b83dfae5b53f12a/860010-1102010100">浙江杭州最新科研成果：大白鼠听人话</a>
真没想到自己居然正面出境这么长时间。</p>




<p>杭州日报的记者写的新闻还挺生动的：<a href="http://hzdaily.hangzhou.com.cn/hzrb/html/2013-05/24/content_1501396.htm">“嫁接”了机器视觉的大白鼠在沙盘迷宫中寻觅阿汤哥的照片</a></p>




<p>PS：感谢CCTV，感谢杭州日报，感谢ZJU，感谢CCNT，感谢生仪的做开颅手术的两位mm，感谢各位在微博帮忙宣传和转发的各位同学！</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[语音信号处理之时域分析-端点检测及Python实现]]></title>
    <link href="http://ibillxia.github.com/blog/2013/05/22/audio-signal-processing-time-domain-Voice-Activity-Detection/"/>
    <updated>2013-05-22T22:22:00+08:00</updated>
    <id>http://ibillxia.github.com/blog/2013/05/22/audio-signal-processing-time-domain-Voice-Activity-Detection</id>
    <content type="html"><![CDATA[<h2>端点检测</h2>


<p>端点检测（End-Point Detection，EPD）的目标是要决定信号的语音开始和结束的位置，所以又可以称为Speech Detection或Voice Activity Detection（VAD）。
端点检测在语音预处理中扮演着一个非常重要的角色。</p>




<p>常见的端点检测方法大致可以分为如下两类：</br>
（1）时域（Time Domain）的方法：计算量比较小，因此比较容易移植到计算能力较差的嵌入式平台</br>
（a）音量：只使用音量来进行端检，是最简单的方法，但是容易对清音造成误判。另外，不同的音量计算方法得到的结果也不尽相同，至于那种方法更好也没有定论。</br>
（b）音量和过零率：以音量为主，过零率为辅，可以对清音进行较精密的检测。</br>
（2）频域（Frequency Domain）的方法：计算量相对较大。</br>
（a）频谱的变化性（Variance）：有声音的频谱变化较规律，可以作为一个判断标准。</br>
（b）频谱的Entropy：有规律的频谱的Entropy一般较小，这也可以作为一个端检的判断标准。
</p>




<p>下面我们分别从这两个方面来探讨端检的具体方法和过程。</p>




<!--more-->




<h2>时域的端检方法</h2>


<p>时域的端检方法分为只用音量的方法和同时使用音量和过零率的方法。只使用音量的方法最简单计算量也最小，我们只需要设定一个音量阈值，任何音量小于该阈值的帧
被认为是静音（silence）。这种方法的关键在于如何选取这个阈值，一种常用的方法是使用一些带标签的数据来训练得到一个阈值，使得误差最小。</p>




<p>下面我们来看看最简单的、不需要训练的方法，其代码如下：

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>import wave
</span><span class='line'>import numpy as np
</span><span class='line'>import matplotlib.pyplot as plt
</span><span class='line'>import Volume as vp
</span><span class='line'>
</span><span class='line'>def findIndex(vol,thres):
</span><span class='line'>    l = len(vol)
</span><span class='line'>    ii = 0
</span><span class='line'>    index = np.zeros(4,dtype=np.int16)
</span><span class='line'>    for i in range(l-1):
</span><span class='line'>        if((vol[i]-thres)*(vol[i+1]-thres)&lt;0):
</span><span class='line'>            index[ii]=i
</span><span class='line'>            ii = ii+1
</span><span class='line'>    return index[[0,-1]]
</span><span class='line'>
</span><span class='line'>fw = wave.open('sunday.wav','r')
</span><span class='line'>params = fw.getparams()
</span><span class='line'>nchannels, sampwidth, framerate, nframes = params[:4]
</span><span class='line'>strData = fw.readframes(nframes)
</span><span class='line'>waveData = np.fromstring(strData, dtype=np.int16)
</span><span class='line'>waveData = waveData*1.0/max(abs(waveData))  # normalization
</span><span class='line'>fw.close()
</span><span class='line'>
</span><span class='line'>frameSize = 256
</span><span class='line'>overLap = 128
</span><span class='line'>vol = vp.calVolume(waveData,frameSize,overLap)
</span><span class='line'>threshold1 = max(vol)*0.10
</span><span class='line'>threshold2 = min(vol)*10.0
</span><span class='line'>threshold3 = max(vol)*0.05+min(vol)*5.0
</span><span class='line'>
</span><span class='line'>time = np.arange(0,nframes) * (1.0/framerate)
</span><span class='line'>frame = np.arange(0,len(vol)) * (nframes*1.0/len(vol)/framerate)
</span><span class='line'>index1 = findIndex(vol,threshold1)*(nframes*1.0/len(vol)/framerate)
</span><span class='line'>index2 = findIndex(vol,threshold2)*(nframes*1.0/len(vol)/framerate)
</span><span class='line'>index3 = findIndex(vol,threshold3)*(nframes*1.0/len(vol)/framerate)
</span><span class='line'>end = nframes * (1.0/framerate)
</span><span class='line'>
</span><span class='line'>plt.subplot(211)
</span><span class='line'>plt.plot(time,waveData,color="black")
</span><span class='line'>plt.plot([index1,index1],[-1,1],'-r')
</span><span class='line'>plt.plot([index2,index2],[-1,1],'-g')
</span><span class='line'>plt.plot([index3,index3],[-1,1],'-b')
</span><span class='line'>plt.ylabel('Amplitude')
</span><span class='line'>
</span><span class='line'>plt.subplot(212)
</span><span class='line'>plt.plot(frame,vol,color="black")
</span><span class='line'>plt.plot([0,end],[threshold1,threshold1],'-r', label="threshold 1")
</span><span class='line'>plt.plot([0,end],[threshold2,threshold2],'-g', label="threshold 2")
</span><span class='line'>plt.plot([0,end],[threshold3,threshold3],'-b', label="threshold 3")
</span><span class='line'>plt.legend()
</span><span class='line'>plt.ylabel('Volume(absSum)')
</span><span class='line'>plt.xlabel('time(seconds)')
</span><span class='line'>plt.show()</span></code></pre></td></tr></table></div></figure>

其中计算音量的函数calVolume参见<a href="http://ibillxia.github.io/blog/2013/05/15/audio-signal-process-time-domain-volume-python-realization/">
音量及其Python实现</a>一文。程序的运行结果如下图：
<center><img src="http://ibillxia.github.com/images/2013/IMAG2013052201.png"></center>
</p>




<p>这里采用了三种设置阈值的方法，但这几种设置方法对所有的输入都是相同的，对于一些特定的语音数据可能得不到很好的结果，比如杂音较强、清音较多或音量
变化较大等语音信号，此时单一阈值的方法的效果就不太好了，虽然我们可以通过增加帧与帧之间的重叠部分，但相对而言计算量会比较大。下面我们利用一些更多的
特征来进行端点加测，例如使用过零率等信息，其过程如下：</br>
（1）以较高音量阈值($\tau _{u}$)为标准，找到初步的端点；</br>
（2）将端点前后延伸到低音量阈值($\tau _{l}$)处；</br>
（3）再将端点前后延伸到过零率阈值($\tau _{zc}$)处，以包含语音中清音的部分。</br>
这种方法需要确定三个阈值($\tau _{u}$,$\tau _{l}$,$\tau _{zc}$)，可以用各种搜寻方法来调整这三个参数。其示意图(参考[1])如下：
<center><img src="http://ibillxia.github.com/images/2013/IMAG2013052202.png"></center>
我们在同一个图中绘制出音量和过零率的阈值图如下：
<center><img src="http://ibillxia.github.com/images/2013/IMAG2013052203.png"></center>
可以看到我们可以通过过零率的阈值来把错分的清音加入到语音部分来。上图使用到的阈值还是和音量的阈值选取方法相同，比较简单直接。
</p>




<p>另外，我们还可以连续对波形进行微分，再计算音量，这样就可以凸显清音的部分，从而将其正确划分出来，详见参考[1]。</p>




<h2>频域的端检方法</h2>


<p>有声音的信号在频谱上会有重复的谐波结构，因此我们也可以使用频谱的变化性（Variation）或Entropy来进行端点检测，可以参见如下链接：
http://neural.cs.nthu.edu.tw/jang/books/audiosignalprocessing/paper/endPointDetection/</p>




<p>总之，端点检测是语音预处理的重头戏，其实现方法也是五花八门，本文只给出了最简单最原始也最好理解的几种方法，这些方法要真正做到实用，还需要针对一些
特殊的情况在做一些精细的设置和处理，但对于一般的应用场景应该还是基本够用的。</p>




<h2>参考（References）</h2>


<p>
[1]EPD in Time Domain: http://neural.cs.nthu.edu.tw/jang/books/audiosignalprocessing/epdTimeDomain.asp?title=6-2%20EPD%20in%20Time%20Domain</br>
[2]EPD in Frequency Domain: http://neural.cs.nthu.edu.tw/jang/books/audiosignalprocessing/epdFreqDomain.asp?title=6-3%20EPD%20in%20Frequency%20Domain
</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[语音信号处理之时域分析-音色及其Python实现]]></title>
    <link href="http://ibillxia.github.com/blog/2013/05/18/audio-signal-processing-time-domain-timbre-python-realization/"/>
    <updated>2013-05-18T21:57:00+08:00</updated>
    <id>http://ibillxia.github.com/blog/2013/05/18/audio-signal-processing-time-domain-timbre-python-realization</id>
    <content type="html"><![CDATA[<h2>音色（Timbre）</h2>


<p>音色是一个很模糊的概念，它泛指语音的内容，例如“天书”这两个字的发音，虽然都是一声（即他们的音高应该是相同或接近的），
但由于音色不同，我们可以分辨这两个音。直觉而言，音色的不同，意味着基本波形的不同，因此我们可以用基本周期的波形来代表音色。
</p>




<p>若要从基本周期的波形来直接分析音色是一件很困难的事情。通常我们的做法是将每一个帧进行频谱分析（Spectral Analysis），算出一个
帧如何分解为不同频率的分量，然后才能进行对比或分析。在频谱分析中，最常用的方法就是快速傅里叶变换（Fast Fourier Transform，FFT），
这是一个相当常用的方法，可以讲在时域（Time Domain）的信号转换成频域（Frequency Domain）的信号，并进而知道每个频率的信号强度。</p>




<p>语谱图（Spectrogram）就是语音频谱图，一般是通过处理接收的时域信号得到频谱图，因此只要有足够时间长度的时域信号就可以(时间长度
为保证频率分辨率)。专业点讲，语谱图就是频谱分析视图，如果针对语音数据的话，叫语谱图。语谱图的横坐标是时间，纵坐标是频率，坐标点
值为语音数据能量，因而语谱图很好的表达了语音的音色随时间变化的趋势。有些经验丰富的人能够通过看语谱图而知道对应的语音信号的内容，
这种技术成为Spectrogram Reading。</p>




<!--more-->




<h2>Python绘制语谱图</h2>


<p>如果是用Matlab，绘制语谱图并不难，网上资料也一堆一堆的。但是，如果要想用Python来绘制呢？网上相关资料很少很少，万幸中找到了参考[4]，
但是，[4]中提供的程序是不能运行的，还需要安装几个库，特别是Audiolab这个，折腾了我好半天，最终安装了，但运行时发现这个audiolab根本无法
import进来，因为ms与numpy的版本有冲突，出现了什么“numpy.dtype does not appear to be the correct type object”，弄了好半天也没有解决，
后来才发现其实不需要audiolab也可以的，因为其实audiolab只是读取不同格式（扩展名）的语音文件的一个lib而已，并不涉及到绘制语谱图的东西。</p>




<p>
闲话少说了，上代码吧，其实看看这代码也挺简单的，就调一个matplotlib.pyplot.specgram()就可以了。

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>import wave
</span><span class='line'>import numpy as np
</span><span class='line'>import matplotlib.pyplot as plt
</span><span class='line'>
</span><span class='line'>fw = wave.open('aeiou.wav','r')
</span><span class='line'>soundInfo = fw.readframes(-1)
</span><span class='line'>soundInfo = np.fromstring(soundInfo,np.int16)
</span><span class='line'>f = fw.getframerate()
</span><span class='line'>fw.close()
</span><span class='line'>
</span><span class='line'>plt.subplot(211)
</span><span class='line'>plt.plot(soundInfo)
</span><span class='line'>plt.ylabel('Amplitude')
</span><span class='line'>plt.title('Wave from and spectrogram of aeiou.wav')
</span><span class='line'>
</span><span class='line'>plt.subplot(212)
</span><span class='line'>plt.specgram(soundInfo,Fs = f, scale_by_freq = True, sides = 'default')
</span><span class='line'>plt.ylabel('Frequency')
</span><span class='line'>plt.xlabel('time(seconds)')
</span><span class='line'>plt.show()</span></code></pre></td></tr></table></div></figure>

</p>




<p>程序运行的效果如下图：
<center><img src="http://ibillxia.github.com/images/2013/IMAG2013051801.png"></center>
虽然程序简单，但还有一些小bug，比如subplot(212)的xlabel和ylabel无法显示，这个问题暂时还没有解决。（更新：这个问题已解决，把mpp.show()放到
最后一行就可以了，顺便图也更新了）</p>




<p>另外，就是关于这个语谱图具体是如何绘制的，这一点涉及到FFT和短时能量的计算，短时能量在<a href="">前文中</a>
已经讲过了，这里不再赘述。关于FFT将在后续文章中讨论。</p>




<h2>参考（References）</h2>


<p>
[1]Timbre (音色): http://neural.cs.nthu.edu.tw/jang/books/audiosignalprocessing/basicFeatureTimber.asp?title=5-5</br>
[2]Wiki - 音色: http://zh.wikipedia.org/wiki/音色</br>
[3]语谱图： http://blog.csdn.net/wuxiaoer717/article/details/6941339</br>
[4]How to plot spectrogram with Python：http://jaganadhg.freeflux.net/blog/archive/2009/07/23/how-to-plot-spectrogram-with-python.html
</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[语音信号处理之时域分析-音高及其Python实现]]></title>
    <link href="http://ibillxia.github.com/blog/2013/05/16/audio-signal-processing-time-domain-pitch-python-realization/"/>
    <updated>2013-05-16T23:10:00+08:00</updated>
    <id>http://ibillxia.github.com/blog/2013/05/16/audio-signal-processing-time-domain-pitch-python-realization</id>
    <content type="html"><![CDATA[<h2>音高（Pitch）</h2>


<p>概念：音高（Pitch）是语音信号的一个很重要的特征，直觉上而言它表示声音频率的高低，这个频率是指基本频率（基频），也即基本周期的倒数。
若直接观察语音的波形，只要语音信号稳定，我们可以很容易的看出基本周期的存在。例如我们取一个包含256个采样点的帧，单独绘制波形图，就可以明显的
看到它的基本周期。如下图所示：
<center><img src="http://ibillxia.github.com/images/2013/IMAG2013051601.png"></center>
其中最上面的波形为|a|的发音，中间的为上图中红色双竖线（位于语音区）所对应的帧的具体波形，而最下面的是上图中绿色双竖线（位于静音区）所
对应的帧的具体波形。很容易看到中间的波形具有明显的周期性。
</p>


<!--more-->


<p>其代码如下：

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>import wave
</span><span class='line'>import numpy as np
</span><span class='line'>import pylab as pl
</span><span class='line'>
</span><span class='line'># ============ test the algorithm =============
</span><span class='line'># read wave file and get parameters.
</span><span class='line'>fw = wave.open('a.wav','rb')
</span><span class='line'>params = fw.getparams()
</span><span class='line'>print(params)
</span><span class='line'>nchannels, sampwidth, framerate, nframes = params[:4]
</span><span class='line'>strData = fw.readframes(nframes)
</span><span class='line'>waveData = np.fromstring(strData, dtype=np.int16)
</span><span class='line'>waveData = waveData*1.0/max(abs(waveData))  # normalization
</span><span class='line'>fw.close()
</span><span class='line'>
</span><span class='line'># plot the wave
</span><span class='line'>time = np.arange(0, len(waveData)) * (1.0 / framerate)
</span><span class='line'>
</span><span class='line'>index1 = 10000.0 / framerate
</span><span class='line'>index2 = 10512.0 / framerate
</span><span class='line'>index3 = 15000.0 / framerate
</span><span class='line'>index4 = 15512.0 / framerate
</span><span class='line'>
</span><span class='line'>pl.subplot(311)
</span><span class='line'>pl.plot(time, waveData)
</span><span class='line'>pl.plot([index1,index1],[-1,1],'r')
</span><span class='line'>pl.plot([index2,index2],[-1,1],'r')
</span><span class='line'>pl.plot([index3,index3],[-1,1],'g')
</span><span class='line'>pl.plot([index4,index4],[-1,1],'g')
</span><span class='line'>pl.xlabel("time (seconds)")
</span><span class='line'>pl.ylabel("Amplitude")
</span><span class='line'>
</span><span class='line'>pl.subplot(312)
</span><span class='line'>pl.plot(np.arange(512),waveData[10000:10512],'r')
</span><span class='line'>pl.plot([59,59],[-1,1],'b')
</span><span class='line'>pl.plot([169,169],[-1,1],'b')
</span><span class='line'>print(1/( (169-59)*1.0/framerate ))
</span><span class='line'>pl.xlabel("index in 1 frame")
</span><span class='line'>pl.ylabel("Amplitude")
</span><span class='line'>
</span><span class='line'>pl.subplot(313)
</span><span class='line'>pl.plot(np.arange(512),waveData[15000:15512],'g')
</span><span class='line'>pl.xlabel("index in 1 frame")
</span><span class='line'>pl.ylabel("Amplitude")
</span><span class='line'>pl.show()</span></code></pre></td></tr></table></div></figure>

</p>




<p>根据参考[1]，可以通过观察一帧的波形图来计算基音频率（感觉这种方法有点奇葩，不过很直观。例如这里的基频为：1/( (169-59)*1.0/framerate )=145.45Hz），
然后还可以计算半音（semitone，可以参见[2]），进而得到pitch与semitone的关系。[1]中还提到了钢琴的半音差，DS表示完全看不懂啊，有木有！！！</p>




<p>参考[2]中还简单介绍了如何改变音高、扩展音域，以及如何改变乐器的振动的弦的音高（通过改变弦长、张力、密度等），感兴趣的可以看看。</p>




<p>另外，由于生理结构的差异，男女性的音高范围不尽相同，一般而言：</br>
·男性的音高范围是35~72半音，对应的频率范围是62~523Hz；</br>
·女性的音高范围是45~83半音，对应的频率范围是110~1000Hz。</br>
然而，我们分辨男女的声音并不是只根据音高，还要根据音色（也即共振峰，下一篇文章中将详细介绍）。
</p>




<p>关于音高的计算，目前有很多种算法，具体将会在后续文章中详细介绍。</p>




<h2>参考（References）</h2>


<p>
[1]Pitch (音高): http://neural.cs.nthu.edu.tw/jang/books/audiosignalprocessing/basicFeaturePitch.asp</br>
[2]Wiki： http://zh.wikipedia.org/wiki/音高
</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[语音信号处理之时域分析-过零率及其Python实现]]></title>
    <link href="http://ibillxia.github.com/blog/2013/05/15/audio-signal-processing-time-domain-ZeroCR-python-realization/"/>
    <updated>2013-05-15T21:44:00+08:00</updated>
    <id>http://ibillxia.github.com/blog/2013/05/15/audio-signal-processing-time-domain-ZeroCR-python-realization</id>
    <content type="html"><![CDATA[<h2>过零率（Zero Crossing Rate）</h2>


<p>概念：过零率（Zero Crossing Rate，ZCR）是指在每帧中，语音信号通过零点（从正变为负或从负变为正）的次数。
这个特征已在语音识别和音乐信息检索领域得到广泛使用，是对敲击的声音的分类的关键特征。</p>




<p>ZCR的数学形式化定义为：
<center>$zcr = \frac{1}{T-1}\sum_{t=1}^{T-1}\pi\{s_{t}s_{t-1}<0\}$.</center>
其中$s$是采样点的值，$T$为帧长，函数$\pi\{A\}$在A为真是值为1，否则为0.
</p>




<p>特性：</br>
(1).一般而言，清音（unvoiced sound）和环境噪音的ZCR都大于浊音（voiced sound）；</br>
(2).由于清音和环境噪音的ZCR大小相近，因而不能够通过ZCR来区分它们；</br>
(3).在实际当中，过零率经常与短时能量特性相结合来进行端点检测，尤其是ZCR用来检测清音的起止点；</br>
(4).有时也可以用ZCR来进行粗略的基频估算，但这是非常不可靠的，除非有后续的修正（refine）处理过程。
</p>




<!--more-->




<h2>ZCR的Python实现</h2>


<p>ZCR的Python实现如下：

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>import math
</span><span class='line'>import numpy as np
</span><span class='line'>
</span><span class='line'>def ZeroCR(waveData,frameSize,overLap):
</span><span class='line'>    wlen = len(waveData)
</span><span class='line'>    step = frameSize - overLap
</span><span class='line'>    frameNum = math.ceil(wlen/step)
</span><span class='line'>    zcr = np.zeros((frameNum,1))
</span><span class='line'>    for i in range(frameNum):
</span><span class='line'>        curFrame = waveData[np.arange(i*step,min(i*step+frameSize,wlen))]
</span><span class='line'>        #To avoid DC bias, usually we need to perform mean subtraction on each frame
</span><span class='line'>        #ref: http://neural.cs.nthu.edu.tw/jang/books/audiosignalprocessing/basicFeatureZeroCrossingRate.asp
</span><span class='line'>        curFrame = curFrame - np.mean(curFrame) # zero-justified
</span><span class='line'>        zcr[i] = sum(curFrame[0:-1]*curFrame[1::]&lt;=0)
</span><span class='line'>    return zcr</span></code></pre></td></tr></table></div></figure>

</p>




<p>对于给定语音文件aeiou.wav，利用上面的函数计算ZCR的代码如下：

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>import math
</span><span class='line'>import wave
</span><span class='line'>import numpy as np
</span><span class='line'>import pylab as pl
</span><span class='line'>
</span><span class='line'># ============ test the algorithm =============
</span><span class='line'># read wave file and get parameters.
</span><span class='line'>fw = wave.open('aeiou.wav','rb')
</span><span class='line'>params = fw.getparams()
</span><span class='line'>print(params)
</span><span class='line'>nchannels, sampwidth, framerate, nframes = params[:4]
</span><span class='line'>str_data = fw.readframes(nframes)
</span><span class='line'>wave_data = np.fromstring(str_data, dtype=np.short)
</span><span class='line'>wave_data.shape = -1, 1
</span><span class='line'>#wave_data = wave_data.T
</span><span class='line'>fw.close()
</span><span class='line'>
</span><span class='line'># calculate Zero Cross Rate
</span><span class='line'>frameSize = 256
</span><span class='line'>overLap = 0
</span><span class='line'>zcr = ZeroCR(wave_data,frameSize,overLap)
</span><span class='line'>
</span><span class='line'># plot the wave
</span><span class='line'>time = np.arange(0, len(wave_data)) * (1.0 / framerate)
</span><span class='line'>time2 = np.arange(0, len(zcr)) * (len(wave_data)/len(zcr) / framerate)
</span><span class='line'>pl.subplot(211)
</span><span class='line'>pl.plot(time, wave_data)
</span><span class='line'>pl.ylabel("Amplitude")
</span><span class='line'>pl.subplot(212)
</span><span class='line'>pl.plot(time2, zcr)
</span><span class='line'>pl.ylabel("ZCR")
</span><span class='line'>pl.xlabel("time (seconds)")
</span><span class='line'>pl.show()</span></code></pre></td></tr></table></div></figure>

</p>




<p>运行以上程序得到下图：
<center><img src="http://ibillxia.github.com/images/2013/IMAG2013051502.png"></center>
</p>




<h2>参考（References）</h2>


<p>
[1]Zero Crossing Rate (過零率): http://neural.cs.nthu.edu.tw/jang/books/audiosignalprocessing/basicFeatureZeroCrossingRate.asp?title=5-3%20Zero%20Crossing%20Rate%20(%B9L%B9s%B2v)&language=english</br>
[2]Wiki: http://zh.wikipedia.org/zh/过零率
</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[语音信号处理之时域分析-音量及其Python实现]]></title>
    <link href="http://ibillxia.github.com/blog/2013/05/15/audio-signal-process-time-domain-volume-python-realization/"/>
    <updated>2013-05-15T19:36:00+08:00</updated>
    <id>http://ibillxia.github.com/blog/2013/05/15/audio-signal-process-time-domain-volume-python-realization</id>
    <content type="html"><![CDATA[<h2>1.概述（Introduction）</h2>


<p>本系列文主要介绍语音信号时域的4个基本特征及其Python实现，这4个基本特征是：</br>
(1)音量（Volume）；</br>
(2)过零率（Zero-Crossing-Rate）；</br>
(3)音高（Pitch）；</br>
(4)音色（Timbre）。
</p>




<h2>2.音量（Volume）</h2>


<p>音量代表声音的强度，可由一个窗口或一帧内信号振幅的大小来衡量，一般有两种度量方法：</br>
（1）每个帧的振幅的绝对值的总和：
<center>$volume = \sum_{i=1}^{n}|s_{i}|$.</center>
其中$s_{i}$为第该帧的$i$个采样点，$n$为该帧总的采样点数。这种度量方法的计算量小，但不太符合人的听觉感受。</br>
（2）幅值平方和的常数对数的10倍：
<center>$volume = 10 * log_{10}\sum_{i=1}^{n}s_{i}^{2}$.</center>
它的单位是分贝（Decibels），是一个对数强度值，比较符合人耳对声音大小的感觉，但计算量稍复杂。
</p>


<!--more-->


<p>音量计算的Python实现如下：</p>




<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>import math
</span><span class='line'>import numpy as np
</span><span class='line'>
</span><span class='line'># method 1: absSum
</span><span class='line'>def calVolume(waveData, frameSize, overLap):
</span><span class='line'>    wlen = len(waveData)
</span><span class='line'>    step = frameSize - overLap
</span><span class='line'>    frameNum = int(math.ceil(wlen*1.0/step))
</span><span class='line'>    volume = np.zeros((frameNum,1))
</span><span class='line'>    for i in range(frameNum):
</span><span class='line'>        curFrame = waveData[np.arange(i*step,min(i*step+frameSize,wlen))]
</span><span class='line'>        curFrame = curFrame - np.median(curFrame) # zero-justified
</span><span class='line'>        volume[i] = np.sum(np.abs(curFrame))
</span><span class='line'>    return volume
</span><span class='line'>
</span><span class='line'># method 2: 10 times log10 of square sum
</span><span class='line'>def calVolumeDB(waveData, frameSize, overLap):
</span><span class='line'>    wlen = len(waveData)
</span><span class='line'>    step = frameSize - overLap
</span><span class='line'>    frameNum = int(math.ceil(wlen*1.0/step))
</span><span class='line'>    volume = np.zeros((frameNum,1))
</span><span class='line'>    for i in range(frameNum):
</span><span class='line'>        curFrame = waveData[np.arange(i*step,min(i*step+frameSize,wlen))]
</span><span class='line'>        curFrame = curFrame - np.mean(curFrame) # zero-justified
</span><span class='line'>        volume[i] = 10*np.log10(np.sum(curFrame*curFrame))
</span><span class='line'>    return volume</span></code></pre></td></tr></table></div></figure>




<p>对于给定语音文件aeiou.wav，利用上面的函数计算音量曲线的代码如下：</p>




<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>import wave
</span><span class='line'>import pylab as pl
</span><span class='line'>import numpy as np
</span><span class='line'>import Volume as vp
</span><span class='line'>
</span><span class='line'># ============ test the algorithm =============
</span><span class='line'># read wave file and get parameters.
</span><span class='line'>fw = wave.open('aeiou.wav','r')
</span><span class='line'>params = fw.getparams()
</span><span class='line'>print(params)
</span><span class='line'>nchannels, sampwidth, framerate, nframes = params[:4]
</span><span class='line'>strData = fw.readframes(nframes)
</span><span class='line'>waveData = np.fromstring(strData, dtype=np.int16)
</span><span class='line'>waveData = waveData*1.0/max(abs(waveData))  # normalization
</span><span class='line'>fw.close()
</span><span class='line'>
</span><span class='line'># calculate volume
</span><span class='line'>frameSize = 256
</span><span class='line'>overLap = 128
</span><span class='line'>volume11 = vp.calVolume(waveData,frameSize,overLap)
</span><span class='line'>volume12 = vp.calVolumeDB(waveData,frameSize,overLap)
</span><span class='line'>
</span><span class='line'># plot the wave
</span><span class='line'>time = np.arange(0, nframes)*(1.0/framerate)
</span><span class='line'>time2 = np.arange(0, len(volume11))*(frameSize-overLap)*1.0/framerate
</span><span class='line'>pl.subplot(311)
</span><span class='line'>pl.plot(time, waveData)
</span><span class='line'>pl.ylabel("Amplitude")
</span><span class='line'>pl.subplot(312)
</span><span class='line'>pl.plot(time2, volume11)
</span><span class='line'>pl.ylabel("absSum")
</span><span class='line'>pl.subplot(313)
</span><span class='line'>pl.plot(time2, volume12, c="g")
</span><span class='line'>pl.ylabel("Decibel(dB)")
</span><span class='line'>pl.xlabel("time (seconds)")
</span><span class='line'>pl.show()</span></code></pre></td></tr></table></div></figure>




<p>运行以上程序得到下图：
<center><img src="http://ibillxia.github.com/images/2013/IMAG2013051501.png"></center>
</p>




<h2>参考（References）</h2>


<p>[1]Volume (音量):http://neural.cs.nthu.edu.tw/jang/books/audiosignalprocessing/basicFeatureVolume.asp?title=5-2%20Volume%20(%AD%B5%B6q)</br>
[2]用Python做科学计算-声音的输入输出:http://hyry.dip.jp:8000/pydoc/wave_pyaudio.html</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[语音信号处理基础学习笔记之时域处理]]></title>
    <link href="http://ibillxia.github.com/blog/2013/05/08/speech-processing-in-time-domain/"/>
    <updated>2013-05-08T23:13:00+08:00</updated>
    <id>http://ibillxia.github.com/blog/2013/05/08/speech-processing-in-time-domain</id>
    <content type="html"><![CDATA[<p>语音信号的分析分为时域、频域、倒谱域等，时域分析简单、运算量小、物理意义明确，但对于语音识别而言，
更为有效的是频域的分析方法，那么为什么还有进行时域的分析呢？</p>




<p>语音信号具有时变特性，但在短时内可以看做是平稳的，所以语音的时域分析是建立在“短时”的条件下的，经研究统计，
语音信号在帧长为10ms~30ms内是相对平稳的。</p>




<p>语音信号是模拟信号，在进行处理之前，要进行数字化，模拟信号数字化的一般方法是采样，按照Nyquist采样定理进行
采样（一般在8K~10KHz）后，在进行量化（一般用8bit，也有16bit等）和编码，变为数字信号。</p>




<p>在语音信号数字化之后，就可以开始对其进行处理了，首先是预处理，由于语音信号的平均功率谱受声门激励和口鼻辐射的影响，
高频端大约在800Hz以上按6dB/倍频程跌落，为此要在预处理中进行预加重。预加重的目的是提升高频部分，是信号变得平坦，
以便于进行频谱分析或声道参数分析。预加重可以用具有6dB/倍频程的提升高频特性的预加重数字滤波器实现。预处理的另一
方面工作是分帧和加窗：分帧的帧长一般在10ms~30ms，分帧既可以是连续的，也可以是有部分over-lap；短时分析的实质是
对信号加窗，一般采用Hamming窗，其他的还有矩形窗、汉宁窗等，如下图所示。
<center><img src="http://ibillxia.github.com/images/2013/IMAG2013050801.png"></center>
</p>




<!--more-->




<p>好了，经过预处理之后就可以真正开始进行时域分析了，这里的时域分析主要包含短时平均能量、短时过零分析、短时自相
关分析以及高阶统计量分析等。</p>




<p>短时平均能量（Short Time Average Energy）可以理解为先计算信号格采样值的平方，然后用一个移动窗h(n-m)选取出一个个
短时平方序列，并将各段的平方值求和，从而得到短时能量序列。短时平均能量（En）可以用来从清音中区分浊音（浊音的En比
清音大得多），可以用来确定声母和韵母、无声与有声、连字等的分界，还可以作为一种超音段信息用于语音识别。但短时平均
能量En对于高电平信号可能产生溢出，此时可以采用短时平均幅度（Short Time Average Magnitude）来度量语音信号幅度的变化。</p>




<p>信号的幅度值从正值到负值要经过零点，从负值到正值也要经过零点，称为过零，统计信号在单位时间（如1s）内过零的次数，
就成为过零率。如果信号按段分割，就成为短时，把各段信号的过零率做统计平均，就是短时平均过零率（Short Time Average Cross 
Zero Ratio）。短时平均过零率（Zn）可以作为“频率”来理解。过零率可以用来定量的分析清音/浊音，特别是在背景噪声电平较大时
更为有效（相比短时平均能量而言），有时还可以同时结合Zn和En来进行判定。</p>




<p>如果说短时平均过零率是描述复杂波形“频率”特征的一个参数，那么短时平均上升过零间隔（Short Time Rise Zero-Crossing Inteval）
就是描述复杂波形“周期”特性的参数。研究表明：在一定噪声背景下，该参数具有很好的稳健性，对不同的语音具有很好的差异性。</p>




<p>自相关函数是偶函数，语音信号的短时自相关函数（Short Time Autocorrelation Function）可以理解为序列[x(n)x(n-k)]通过一个
冲激响应为hk(n)的数字滤波器的输出，即有Rn(k) = [x(n)x(n-k)]*hk(n)。短时自相关函数是语音信号时域分析中的一个重要参量，但是
运算量很大。短时平均幅度差函数AMDF（Short Time Average Magnitude Difference Function）与自相关函数有类似的功效，但运算量
可降低许多，所以在语音信号处理中应用广泛。</p>




<p>最后是高阶统计量了。近来高阶统计量在语音信号处理中应用也越来越多，高阶统计量一般指高阶矩(Moment)、高阶累积量(Cumulant)以及
他们的谱——高阶矩谱和高阶累积量谱。首先定义了随机变量x的（第一）特征函数（也称为矩生成函数），实际为它的密度函数f(x)的傅里叶变换。
然后定义了第二特征函数（也称为累积量生成函数），它是第一特征函数的对数。还有随机变量x的k阶矩（mk）的定义，它是x的k次幂与f(x)的
乘积在x∈R上的积分。类似的还有k阶中心矩（μk）的定义，都与概率论中的定义差不多。现在，可以对第一、二特征函数进行泰勒展开，可以得
到ck（x的k阶累积量）和mk之间的一些关系，可以发现k<4时，ck=μk，此时ck的物理意义与μk的物理意义相同，而k>=4时，则不相等。对于c3，
描述了概率分布的对称性，通过定义一个新的概念——偏度（Skewness，也称为偏态系数）来衡量。对于c4，文中为了简化，假设了x的均值为0，
然后定义了一个称为峰态（也称峰度，Kurtosis）的概念，以表示分布相对于正太分布的尖锐或平坦程度。后面两小节分别对此进行了从单个
随机变量到多个随机变量的推广的分析和随机变量服从高斯分布（正态分布）的特殊情形做了分析。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[五一登高远足]]></title>
    <link href="http://ibillxia.github.com/blog/2013/05/01/go-hiking-International-Labour-Day/"/>
    <updated>2013-05-01T22:01:00+08:00</updated>
    <id>http://ibillxia.github.com/blog/2013/05/01/go-hiking-International-Labour-Day</id>
    <content type="html"><![CDATA[<p>五一天晴气爽，登高望远，强身健体！只可惜“不畏浮云遮望眼，只缘身在最高层” 这句诗在空气严重污染的今天已不适用了！</p>




<p><img src="http://ibillxia.github.com/images/2013/IMAG2013050101.jpg">

<!-- more -->

<img src="http://ibillxia.github.com/images/2013/IMAG2013050102.jpg">

<img src="http://ibillxia.github.com/images/2013/IMAG2013050103.jpg">

<img src="http://ibillxia.github.com/images/2013/IMAG2013050104.jpg">

<img src="http://ibillxia.github.com/images/2013/IMAG2013050105.jpg">

<img src="http://ibillxia.github.com/images/2013/IMAG2013050106.jpg">

<img src="http://ibillxia.github.com/images/2013/IMAG2013050107.jpg">

<img src="http://ibillxia.github.com/images/2013/IMAG2013050108.jpg">

<img src="http://ibillxia.github.com/images/2013/IMAG2013050109.jpg">

<img src="http://ibillxia.github.com/images/2013/IMAG2013050110.jpg">

<img src="http://ibillxia.github.com/images/2013/IMAG2013050111.jpg">

<img src="http://ibillxia.github.com/images/2013/IMAG2013050112.jpg">

<img src="http://ibillxia.github.com/images/2013/IMAG2013050113.jpg">

<img src="http://ibillxia.github.com/images/2013/IMAG2013050114.jpg">

<img src="http://ibillxia.github.com/images/2013/IMAG2013050115.jpg">

<img src="http://ibillxia.github.com/images/2013/IMAG2013050116.jpg">
</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[使用Alize等工具构建说话人识别平台]]></title>
    <link href="http://ibillxia.github.com/blog/2013/04/26/building-speaker-recognition-system-using-alize-etc/"/>
    <updated>2013-04-26T22:07:00+08:00</updated>
    <id>http://ibillxia.github.com/blog/2013/04/26/building-speaker-recognition-system-using-alize-etc</id>
    <content type="html"><![CDATA[<p>前段时间有好几位同学询问如何用Alize实现说话人识别的问题，由于寒假前赶Paper，来不及详细解答，更没时间写Demo。
开学后不久抽时间写了一个Demo，并上传到了GitHub：https://github.com/ibillxia/VoicePrintReco/tree/master/Demo</p>




<p>下面将利用Alize+SPro进行简单的GMM-Based的说话人识别的基本流程总结如下：</br>
1.Features extraction 特征提取</br>
sfbcep.exe（MFCC）或slpcep.exe（LPCC）</br>

2.Silence removal 静音检测和去除</br>
NormFeat.exe 先能量规整</br>
EnergyDetector.exe 基于能量检测的静音去除</br>

3.Features Normalization 特征规整</br>
NormFeat.exe 再使用这个工具进行特征规整</br>

4.World model training</br>
TrainWorld.exe 训练UBM</br>

5.Target model training</br>
TrainWorld.exe 在训练好UBM的基础上训练training set和testing set的GMM</br>

6.Testing</br>
ComputeTest.exe 将testing set 的GMM在training set的GMM上进行测试和打分</br>

7.Score Normalization</br>
ComputeNorm.exe 将得分进行规整</br>

8. Compute EER 计算等错误率</br>
你可以查查计算EER的matlab代码，NIST SRE的官网上有下载（http://www.itl.nist.gov/iad/mig//tools/DETware_v2.1.targz.htm）。</br>
</p>




<!--more-->




<p>关于各步骤中参数的问题，可以在命令行“工具 -help”来查看该工具个参数的具体含义，另外还可参考Alize源码中各个工具的test目录中提供的实例，
而关于每个工具的作用及理论知识则需要查看相关论文。</p>




<p>常见问题及解答: http://mistral.univ-avignon.fr/mediawiki/index.php/Frequently_asked_questions</p>




<p>更多问题请在Google论坛（https://groups.google.com/forum/?fromgroups=&hl=zh-CN#!forum/alize&#8212;voice-print-recognition）提出，大家一起讨论！</p>




<h3>推荐资料</h3>


<p>
[1] ALIZE - User Manual: http://mistral.univ-avignon.fr/doc/userguide_alize.001.pdf</br>
[2] LIA_SPKDET Package documentation: http://mistral.univ-avignon.fr/doc/userguide_LIA_SpkDet.002.pdf</br>
[3] Reference System based on speech modality ALIZE/LIA RAL: http://www-clips.imag.fr/geod/User/laurent.besacier/NEW-TPs/TP-Biometrie/tools/CommentsLBInstall/doc.pdf</br>
[4] Jean-Francois Bonastre, etc. ALIZE/SpkDet: a state-of-the-art open source software for speaker recognition</br>
[5] TOMMIE GANNERT. A Speaker Veri?cation System Under The Scope: Alize</br>
[6] Alize Wiki: http://mistral.univ-avignon.fr/mediawiki/index.php/Main_Page
</p>



]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[VALSE2013]]></title>
    <link href="http://ibillxia.github.com/blog/2013/04/22/VALSE2013/"/>
    <updated>2013-04-22T22:28:00+08:00</updated>
    <id>http://ibillxia.github.com/blog/2013/04/22/VALSE2013</id>
    <content type="html"><![CDATA[<h3>学术研讨</h3>


<p>VALSE是Vision And Learning SEminar的缩写，它主要目的是为计算机视觉、图像处理、模式识别与机器学习研究领域内的中国青年学者（以70后研发
人员为主）提供一个深层次学术交流的舞台。虽然参与会议和做报告的人主要是做视觉的，但很多问题是机器学习和模式识别当中的一般性问题，所以我这
个搞语音的也去打酱油了^_^。</p>




<p>今年的VALSE在南京东南大学召开，参加会议的人数超出预期，会场爆满，仅学校的老师和公司的研究人员就占了会场大半，学生沦落到只能座最后两排，
或者座分会场（这个太不科学了-_-!）。会程安排也很紧凑，中午几乎没有休息时间，吃饭都很赶，而下午也很晚（6点半左右）才结束。这次会议有好几个
perfect的报告，但也有些不太感兴趣的，有的甚至感觉很2。除了一些报告，还有两个主题讨论会，印象中主要包括三个论题：学术界与工业界的Gap及衔接
问题，深度学习是否是计算机视觉的终极解决方案，计算机视觉要不要从生物视觉机理中受启发等。</p>




<p>闲话少说，言归正传，数萝卜下窖的讲讲这两天的经历吧。
第一天上午，第一个做报告的是MSRA的张磊，主要讲了计算机视觉的一些基本问题，从AI的历史将起，提到了Turing Test，是人工智能
的Benchmark。而CV的一个基本问题是Object Recognition，人们的研究经历了从之前的Model Based到如今的Data Driven及Big Data的过程，各种模型和方法可谓
层出不穷，然而对于真正解决问题、真正达到人类一般的视觉智能，还相差甚远。接着他讲了关于在路灯下找钥匙的故事（详询http://tongyanyan.blog.edu.cn/2006/427512.html），
听了这个故事后，感觉那个找钥匙的人很滑稽可笑，然而再想想我们自己正在做的研究，是不是在某种程度上和故事中的这个人一样呢。通过这个故事，他引出自己
的观点：要想解决Object Recognition这个问题或者说要解决CV的问题，就需要More Effective Representation & Match。接下来讲在Representation方面一些研究
人员提出的一些人工设计的Feature，而在Match方面则从Point、Line、Plane、Volume（点线面体）进行了详尽的讲述。最后还提了一下Deep Neural Network在CV中的
应用，可以discover hidden patterns。虽然对CV中的很多概念和模型方法不太了解，但感觉还是挺有收获的。</p>




<p>上午的后两个报告都是讲Sparse的，虽然之前看过关于Sparse Coding的东西，但当他们在上面讲的，主要偏重与Sparse这个问题的优化求解方法及其变形，
涉及到很多数学公式和推导，感觉很枯燥，加之晚睡早起，有点犯困，所以基本没有听进去。贾佳亚的报告还似懂非懂，而陈欢欢的Sparse Bayesian Learning
表示完全没听懂。个人感觉Sparse还是很重要的，所以在弄完Deep Learning这个专题后，我想有必要对这两个报告及其相关论文再做深入的学习和研究。</p>


<!--more-->


<p>中午3个东南大学的同学请我们实验室的在他们学校食堂吃饭，虽然不太记得他们名字了，真心感谢他们！</p>




<p>下午第一个报告是高新波的IQA&VALSE，主要讲了图像质量评价的一些东西，虽然也提到了一些生物视觉方面的东西，感觉很没趣，基本没怎么听，打了个盹。
第二个报告是俞洪波的关于生物视觉方面的东西，很感兴趣，他从深层复杂网络结构、神经元、突触、离子通道、蛋白等多个层面上讲了视觉系统的信息处理流程，
后面还提到了视觉功能柱，指出了视觉神经元具有很强的选择性，不同部位的神经元对不同方向、距离的视觉信息具有选择性的激活增强，最后还讲了一些模拟
视觉系统的计算模型，并描述了一些实验，虽然对报告题目中的Self Organization Model到底是什么还不是很清楚，但对生物视觉系统有了更进一步的了解，
而且知道了他们是怎么获取神经元激活区域的。</p>




<p>下午的第2个Session的第一个报告是颜水成的Fashion Recommendation，包括Hair Style，Makeup，Clothing，Shoes等的Recommendation，不太感冒，只是对他重复提到
的关于华人做研究的一个问题深表同感，他说华人做研究其实很不错的，能在很多TOP会议期刊发Paper甚至Best Paper，但原创性的问题却很少，我们都在提高别人的Citation，
所以华人还需要在发现问题方面多下功夫，而不是仅仅在解决问题方面。后面两个报告一个是王亦洲的General Purpose Vision，表示没听懂。最后一个报告是王晓刚的Crowd 
Video Surveillance，主要是讲在Video中识别人并跟踪人的移动，或者统计视场中人的数量之类的，只是感性的了解了一下，印象里报告中好像没有提到什么具体的CV技术，
只是举了一个人体位置跟踪的例子，还有一个用在足球视屏中运动员跟踪的例子。</p>




<p>第二天上午第一个Session是两个报告，一个是陈小武的Image/Video/3D Scene Understanding and Editing，主要分以下四个方面：Illumination Learning and Synthesis，
Labeling and Lavering and Editing，Estimating 3D Model from a single Image，Video Event Representation and Inference，总体感觉讲的内容涉及到很多东西，甚至他的
学生不仅懂CV，还要懂美术、剪纸等，而且他们每年都会发CVPR、ICCV、ECCV之类的，感觉还挺NB的。另外一个报告是非常期待的于凯的关于深度学习和大数据的报告，但听了之后，
感觉有些Depressed，因为他的报告中没有涉及Deep Learning的一些细节的东西，诸如RBM的原理及其训练等，基本上只是泛泛而谈，之前对Deep Learning做了深入的调研和学习，
自我感觉Deep Learning也没什么神秘的，虽然对Gibbs采样和CD算法的理论还没有完全理解清楚，但我觉得Deep Learning更多的是一种思想方法，在Deep Architecture中，Knowledge
通过一层一层抽象和提取后，对于Classification、Clustering等任务具有更有效Representation，而且在Training Error非常小的情况下，还是可以再Testing中获得理想的Error Ratio，
相比Shallow Architecture，不存在模型Over Fitting的问题。另外，有人提到Deep Architecture中Layer数目的确定的问题，于凯的回答是，在Neural Networks中加一层后，进行Deep 
Learning的过程，如果相对于没加该层得到的Test Error更小，并且是非常有效的性能提升，那么就加进这一层。然后同样的，再加一层，再进行Deep Learning，以此类推。</p>




<p>上午的后一个Session是关于CV在Industry中的Application，先是来在Industry中的一些研究开发人员对他们目前的工作做一些简短的介绍，感觉某些公司有严重的广告嫌疑，很是讨厌。
然后是讨论阶段，各自就CV在学术界和工业界之间的Gap发表意见，总结起来主要有以下观点：一方面学术界与工业界的Gap是必要的，学术必须要超前，这样工业界才可能将其成熟的应用；
另一方面，学术界与工业界的Gap可以通过在工业界设置研究院（比如MSRA、百度最近在硅谷设置深度学习研究院之类），这样可以加快学术成果应用于工业界的进程，学术最终的目的就是
在工业界中发挥巨大作用，服务广大民众，给社会带来价值。</p>




<p>上午的Panel严重超时了，直到快1点了才结束，去餐馆吃饭，我们跟老板说我们下午要考试，让快点上菜，结果上菜速度果然飞快，而我们吃得也很快，基本上一盘菜一会儿就吃光，
真是高效啊，哈哈！</p>




<p>下午首先是一个Panel，讨论（更确切的说是辩论）了两个主题，一个是关于计算机视觉是否要借鉴吸收生物神经视觉的结果，另一个是Deep Learning是否是CV的终极解决方案，这两个辩论
都非常精彩，笑点不断。Panel开始之前，首先是两位报告者发言，首先上台的是 @老师木（袁进辉），他自我介绍了一番，然后讲了讲生物视觉与计算机视觉的紧密联系，认为计算机视觉要想
取得重大突破，就必须借鉴生物视觉的研究的发现。另外一位是李学龙老师，很有个性，只写了一张PPT，但发言时却如滔滔江水绵绵不绝，可以听得出，他对生物视觉也非常了解，也认为计算机
视觉必须借鉴生物视觉的一些研究成果。后面的讨论非常精彩，将学术娱乐化了。这两个论题本身就很具争议性，正反两方各执其词，要辩论出个是非来，还真需要真才实学。</p>




<p>Panel完之后是两个报告，一个是吴建鑫的Approximating Additive Kernel for Large Scale Vision Tasks，没怎么听懂。另一个是张敏灵的Multi-label Learning，感觉很没趣，主要是
觉得这并不是一个新问题，但在图像标注方面确实是一个很重要的问题。</p>




<p>最后，还可以从一些微博内容中获取更多关于VALSE2013的信息，可以搜索主题\#VALSE\# 或\#VALSE2013\#，或者关注 @潘布衣（会议Chair潘刚）、@张磊MSRA、@余凯_西二旗民工、
@老师木等等。。。</p>




<h3>游玩休闲</h3>


<p>我们周五下午6点多到，下雨了。坐地铁然后走到旅馆，吃晚饭就8点了，但还好雨也听了，我们就去附近的夫子庙、秦淮河逛了逛。
<center><img src="http://ibillxia.github.com/images/2013/IMAG2013042201.jpg"></center>
第二天晚上去阅江楼逛了逛，到哪儿才发现晚上关门，坑爹啊！不过在外面远眺夜晚的阅江楼也不错，然后走了一个把小时到南京长江大桥。
<center><img src="http://ibillxia.github.com/images/2013/IMAG2013042202.jpg"></center>
第三天晚上就待在住处。因为订的第四天下午6点多的票，所以白天就可以尽情的去玩玩了。第一站来到了中山园陵
<center><img src="http://ibillxia.github.com/images/2013/IMAG2013042203.jpg"></center>
只可惜周一不开放，被挡在“天下为公”的门外。原打算接下来要去的雨花台、大屠杀纪念馆也不开放，坑爹啊！
<center><img src="http://ibillxia.github.com/images/2013/IMAG2013042204.jpg"></center>
木办法，就在里面找了一个开放的十朝历史博物馆去了
<center><img src="http://ibillxia.github.com/images/2013/IMAG2013042205.jpg"></center>
然后去总统府了，就在外面看了看
<center><img src="http://ibillxia.github.com/images/2013/IMAG2013042206.jpg"></center>
再然后去玄武门，一到哪儿就又下雨了
<center><img src="http://ibillxia.github.com/images/2013/IMAG2013042207.jpg"></center>
进里面看了看玄武湖，然后就直接回旅馆了
<center><img src="http://ibillxia.github.com/images/2013/IMAG2013042208.jpg"></center>
</p>



]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[深度学习及其在语音方面的应用]]></title>
    <link href="http://ibillxia.github.com/blog/2013/04/17/Deep-Learning-and-its-application-in-audio-and-speech-processing/"/>
    <updated>2013-04-17T22:43:00+08:00</updated>
    <id>http://ibillxia.github.com/blog/2013/04/17/Deep-Learning-and-its-application-in-audio-and-speech-processing</id>
    <content type="html"><![CDATA[<p>以下是今天在组会上讲的内容，与大家分享一下。有些地方我也没有完全理解，欢迎大家一起来讨论。</p>


<p><center>
<embed width="780"
    height="574"
    name="plugin"
    src="http://ibillxia.github.com/upload/Deep Learning - Bill Xia.pdf"
    type="application/pdf"
/>
</center></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[基于能量的模型和波尔兹曼机]]></title>
    <link href="http://ibillxia.github.com/blog/2013/04/12/Energy-Based-Models-and-Boltzmann-Machines/"/>
    <updated>2013-04-12T22:12:00+08:00</updated>
    <id>http://ibillxia.github.com/blog/2013/04/12/Energy-Based-Models-and-Boltzmann-Machines</id>
    <content type="html"><![CDATA[<p>由于深度置信网络（Deep Belief Networks，DBN）是基于限制性玻尔兹曼机（Restricted Boltzmann Machines，RBM）的深层网络结构，
所以本文重点讨论一下玻尔兹曼机（BM），以及它的学习算法——对比散度（Contrastive Divergence，CD）算法。在介绍BM前，我们首先介绍一下
基于能量的模型（Energy Based Model，EBM），因为BM是一种特殊的EBM。</p>




<h2>1. 基于能量的模型(EBM)</h2>


<p>基于能量的模型是一种具有普适意义的模型，可以说它是一种模型框架，在它的框架下囊括传统的判别模型和生成模型，图变换网络(Graph-transformer 
Networks)，条件随机场，最大化边界马尔科夫网络以及一些流形学习的方法等。EBM通过对变量的每个配置施加一个有范围限制的能量来捕获变量之间的依赖
关系。EBM有两个主要的任务，一个是推断(Inference)，它主要是在给定观察变量的情况，找到使能量值最小的那些隐变量的配置；另一个是学习(Learning)，
它主要是寻找一个恰当的能量函数，使得观察变量的能量比隐变量的能量低。</p>




<p>基于能量的概率模型通过一个能量函数来定义概率分布，
<center>$p(x) = \frac{e^{E(x)}}{Z}.$ &#8230; ①</center>
其中Z为规整因子，
<center>$Z = \sum _{x} e^{-E(x)}.$ &#8230; ②</center>
基于能量的模型可以利用使用梯度下降或随机梯度下降的方法来学习，具体而言，就是以训练集的负对数作为损失函数，
<center>$l(\theta,D) = -L(\theta,D) = - \frac{1}{N}\sum_{x^{(i)}\in D} log p(x^{(i)}).$ &#8230; ③</center>
其中$\theta$为模型的参数，将损失函数对$\theta$求偏导，
<center>$\Delta = \frac{\partial l(\theta,D)}{\partial \theta} = - \frac{1}{N} \frac{\partial \sum log p(x^{(i)})}{\partial \theta}.$ &#8230; ④</center>
即得到损失函数下降最快的方向。</p>




<!--more-->




<h3>包含隐单元的EBMs</h3>


<p>在很多情况下，我们无法观察到样本的所有属性，或者我们需要引进一些没有观察到的变量，以增加模型的表达能力，这样得到的就是包含隐含变量的EBM，
<center>$P(x) = \sum _{h} P(x,h) = \sum _{h} \frac{e^{-E(x,h)}}{Z}.$ &#8230; ⑤</center>
其中$h$表示隐含变量。在这种情况下，为了与不包含隐含变量的模型进行统一，我们引入如下的自由能量函数，
<center>$F(x) = - log \sum_{h}e^{-E(x,h)}.$ &#8230; ⑥</center>
这样$P(x)$就可以写成，
<center>$P(x) = \frac{e^{-F(x)}}{Z}, where Z = \sum_{x} e^{-F(x)}.$ &#8230; ⑦</center>
此时，损失函数还是类似的定义，只是在进行梯度下降求解时稍微有些不同，
<center>$\Delta = - \frac{\partial log p(x)}{\partial \theta} 
= - \frac{\partial (-F(x) -log Z)}{\partial \theta} 
= \frac{\partial F(x)}{\partial \theta} - \sum_{\hat{x}} p(\hat{x}) \frac{\partial F(\hat{x})}{\partial \theta}$. &#8230; ⑧</center>
该梯度表达式中包含两项，他们都影响着模型所定义的分布密度：第一项增加训练数据的概率（通过减小对应的自由能量），而第二项则减小模型
生成的样本的概率。</p>




<p>通常，我们很难精确计算这个梯度，因为式中第一项涉及到可见单元与隐含单元的联合分布，由于归一化因子$Z(\theta)$的存在，该分布很难获取[3]。
我们只能通过一些采样方法（如Gibbs采样）获取其近似值，其具体方法将在后文中详述。</p>




<h2>2. 限制性玻尔兹曼机</h2>


<p>玻尔兹曼机（Boltzmann Machine，BM）是一种特殊形式的对数线性的马尔科夫随机场（Markov Random Field，MRF），即能量函数是自由变量的线性函数。
通过引入隐含单元，我们可以提升模型的表达能力，表示非常复杂的概率分布。</p>




<p>限制性玻尔兹曼机（RBM）进一步加一些约束，在RBM中不存在可见单元与可见单元的链接，也不存在隐含单元与隐含单元的链接，如下图所示
<center><img src="http://ibillxia.github.com/images/2013/IMAG2013041201.png"></center>
RBM的能量函数$E(v,h)$定义为，
<center>$E(v,h) = -b&#8217;v - c&#8217;h - h&#8217;Wv$.</center>
其中&#8217;表示转置，$b,c,W$为模型的参数，$b,c$分别为可见层和隐含层的偏置，$W$为可见层与隐含层的链接权重。此时，对应的自由能量为，
<center>$F(v) = -b&#8217;v - \sum_{i}log\sum_{h_{i}}e^{h_{i}(c_{i}+W_{i}v)}.$ &#8230; ⑨</center>
另外，由于RBM的特殊结构，可见层/隐含层内个单元之间是相互独立的，所以我们有，
<center>$p(h|v) = \prod _{i} p(h_{i}|v)$,</center>
<center>$p(v|h) = \prod _{j} p(v_{j}|h).$ &#8230; ⑩</center>
</p>




<h3>使用二值单元的RBM</h3>


<p>如果RBM中的每个单元都是二值的，即有$v_{j},h_{i} \in \{0,1\}$，我们可以得到，
<center>$p(h_{i}=1|v) = sigmoid(c_{i} + W_{i}v)$,</center>
<center>$p(v_{j}=1|h) = sigmoid(b_{j} + W_{j}&#8217;h).$ &#8230; ⑪</center>
而对应的自由能量函数为，
<center>$F(v) = -b&#8217;v - \sum_{i}log(1+e^{c_{i}+W_{i}v}).$ &#8230; ⑫</center>
使用梯度下降法求解模型参数时，各参数的梯度值如下[2]，
<center>$-\frac{\partial logp(v)}{\partial W_{ij}} = E_{v}[p(h_{i}|v) * v_{j}] - v_{j}^{(i)} * sigmoid(W_{i} * v^{(i)}+c_{i}),$</center>
<center>$-\frac{\partial logp(v)}{\partial c_{i}} = E_{v}[p(h_{i}|v) * v_{j}] - sigmoid(W_{i} * v^{(i)}),$</center>
<center>$-\frac{\partial logp(v)}{\partial b_{j}} = E_{v}[p(h_{i}|v) * v_{j}] - v_{j}^{(i)}.$ &#8230; ⑬</center>
</p>




<h2>3. RBM的学习</h2>


<p>前面提到了，RBM是很难学习的，即模型的参数很难确定，下面我们就具体讨论一下基于采样的近似学习方法。学习RBM的任务是求出模型的参数
$\theta = \{c, b, W\}$的值。</p>




<h3>3.1 Gibbs采样</h3>


<p>Gibbs采样是一种基于马尔科夫链蒙特卡罗(Markov Chain Monte Carlo,MCMC)策略的采样方法。对于一个$K$为随机向量$X = (X_{1},X_{2},&#8230;,X_{K})$，
假设我们无法求得关于$X$的联合分布$P(X)$，但我们知道给定$X$的其他分量时，其第$k$个分量$X_{k}$的条件分布，即$P(X_{k}|X_{k^{-}})$，其中$X_{k^{-}} = 
(X_{1},X_{2},&#8230;,X_{k-1},X_{k+1},&#8230;,X_{K})$，那么，我们可以从$X$的一个任意状态(比如[$x_{1}(0),x_{2}(0),&#8230;,x_{K}(0)$])开始，利用上述条件
分布，迭代的对其分量依次采样，随着采样次数的增加，随机变量[$x_{1}(n),x_{2}(n),&#8230;,x_{K}(n)$]的概率分布将以$n$的几何级数的速度收敛于$X$的联合
概率分布$P(X)$。也就是说，我们可以在未知联合概率分布的条件下对其进行采样。</p>




<p>基于RBM的对称结构，以及其中神经元状态的条件独立性，我们可以使用Gibbs采样方法得到服从RBM定义的分布的随机样本。在RBM中进行$k$步Gibbs采样的具体
算法为：用一个训练样本(或可见层的任何随机化状态)初始化可见层的状态$v0$，交替进行如下采样：
<center>$h_{0} \sim P(h|v_{0}), v_{1} \sim P(v|h_{0}),$</center>
<center>$h_{1} \sim P(h|v_{1}), v_{2} \sim P(v|h_{1}),$</center>
<center>$&#8230; &#8230;, v_{k+1} \sim P(v|h_{k})$.</center>
在经过步数$k$足够大的情况下，我们可以得到服从RBM所定义的分布的样本。此外，使用Gibbs采样我们也可以得到式⑧中第一项的近似。</p>




<h3>3.2 对比散度算法</h3>


<p>尽管利用Gibbs采样我们可以得到对数似然函数关于未知参数梯度的近似，但通常情况下需要使用较大的采样步数，这使得RBM的训练效率仍然不高，尤其是当观测数据
的特征维数较高时。2002年，Hinton[4]提出了RBM的一个快速学习算法，即对比散度（Contrastive Divergence，CD）。与Gibbs采样不同，Hinton指出当使用训练数据初
始化$v_{0}$时，我们仅需要使用$k$（通常k=1）步Gibbs采样变可以得到足够好的近似。在CD算法一开始，可见单元的状态被设置成一个训练样本，并利用式⑪第一个式子
来计算所有隐层单元的二值状态，在所有隐层单元的状态确定了之后，根据式⑪第二个式子来确定第$i$个可见单元$v_{i}$取值为1的概率，进而产生可见层的一个重构
(reconstruction)。然后将重构的可见层作为真实的模型代入式⑬各式中第一项，这样就可以进行梯度下降算法了。</p>




<p>在RBM中，可见单元一般等于训练数据的特征维数，而隐层单元数需要事先给定，这里设可见单元数和隐单元数分别为$n$和$m$，令$W$表示可见层与隐层间的链接权重
矩阵(m×n阶)，$a$(n维列向量)和$b$(m维列向量)分别表示可见层与隐层的偏置向量。RBM的基于CD的快速学习算法主要步骤如下：

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>//输入：一个训练样本x0; 隐层单元个数m; 学习率alpha; 最大训练周期T
</span><span class='line'>//输出：链接权重矩阵W, 可见层的偏置向量a, 隐层的偏置向量b
</span><span class='line'>//训练阶段
</span><span class='line'>初始化：令可见层单元的初始状态v1 = x0; W, a, b为随机的较小数值
</span><span class='line'>For t=1,2,...,T
</span><span class='line'>  For j=1,2,...,m //对所有隐单元
</span><span class='line'>      计算P(h1j=1|v1), 即P(h1j=1|v1) = sigmoid(bj+sum_i(v1i*Wij));
</span><span class='line'>      从条件分布P(h1j|v1)中抽取h1j ∈ {0,1}
</span><span class='line'>  EndFor
</span><span class='line'>  
</span><span class='line'>  For i=1,2,...,n //对所有可见单元
</span><span class='line'>      计算P(v2i=1|h1), 即P(v2i=1|h1) = sigmoid(ai+sum_j(Wij*h1j));
</span><span class='line'>      从条件分布P(v2i|h1)中抽取v2i ∈ {0,1}
</span><span class='line'>  EndFor
</span><span class='line'>  
</span><span class='line'>  For j=1,2,...,m //对所有隐单元
</span><span class='line'>      计算P(h2j=1|v2), 即P(h2j=1|v2) = sigmoid(bj+sum_i(v2i*Wij));
</span><span class='line'>  EndFor
</span><span class='line'>  
</span><span class='line'>  //更新RBM的参数
</span><span class='line'>  W = W + alpha *(P(h1=1|v1)v1' - P(h2=1|v2)v2');
</span><span class='line'>  a = a + alpha *(v1-v2);
</span><span class='line'>  b = b + alpha *(P(h1=1|v1) - P(h2=1|v2));
</span><span class='line'>EndFor</span></code></pre></td></tr></table></div></figure>

上述基于CD的学习算法是针对RBM的可见单元和隐层单元均为二值变量的情形，我们可以很容易的推广到这些单元为高斯变量的情形。
</p>




<p>RBM的完整实现参见https://github.com/ibillxia/DeepLearnToolbox/tree/master/DBN的Matlab代码。</p>




<h2>References</h2>


<p>
[1] Learn Deep Architectures for AI, Chapter 5.</br>
[2] Deep Learning Tutorial, Release 0.1, Chapter 9.</br>
[3] 受限波尔兹曼机简介. 张春霞. </br>
[4] Training Products of experts by minimizing contrastive divergence. GE Hinton.
</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[2013IDF声龙语音识别技术演示]]></title>
    <link href="http://ibillxia.github.com/blog/2013/04/10/Intel-Developer-Forum-2013-Nuance-Dragon-Presentation/"/>
    <updated>2013-04-10T12:57:00+08:00</updated>
    <id>http://ibillxia.github.com/blog/2013/04/10/Intel-Developer-Forum-2013-Nuance-Dragon-Presentation</id>
    <content type="html"><![CDATA[<p>2013英特尔信息技术峰会(Intel Developer Forum, IDF)上，来自Nuance的声龙语音合成和识别技术的演示，中文语音识别不给力，
笑点频出啊，哈哈</p>




<p><iframe height=560 width=780 src="http://player.youku.com/embed/XNTQwNjQ0MjUy" frameborder=0 allowfullscreen></iframe></p>



]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[卷积神经网络（CNN）]]></title>
    <link href="http://ibillxia.github.com/blog/2013/04/06/Convolutional-Neural-Networks/"/>
    <updated>2013-04-06T23:34:00+08:00</updated>
    <id>http://ibillxia.github.com/blog/2013/04/06/Convolutional-Neural-Networks</id>
    <content type="html"><![CDATA[<h2>1. 概述</h2>


<p>卷积神经网络是一种特殊的深层的神经网络模型，它的特殊性体现在两个方面，一方面它的神经元间的连接是<strong>非全连接</strong>的，
另一方面同一层中某些神经元之间的连接的<strong>权重是共享的</strong>（即相同的）。它的非全连接和权值共享的网络结构使之更类似于生物
神经网络，降低了网络模型的复杂度（对于很难学习的深层结构来说，这是非常重要的），减少了权值的数量。</p>




<p>卷积网络最初是受视觉神经机制的启发而设计的，是为识别二维形状而设计的一个多层感知器，这种网络结构对平移、比例缩放、倾斜或者共他
形式的变形具有高度不变性。1962年Hubel和Wiesel通过对猫视觉皮层细胞的研究，提出了感受野(receptive field)的概念，1984年日本学者Fukushima
基于感受野概念提出的神经认知机(neocognitron)模型，它可以看作是卷积神经网络的第一个实现网络，也是感受野概念在人工神经网络领域的首次应用。</p>




<p>神经认知机将一个视觉模式分解成许多子模式(特征)，然后进入分层递阶式相连的特征平面进行处理，它试图将视觉系统模型化，使其能够在即使物体有
位移或轻微变形的时候，也能完成识别。神经认知机能够利用位移恒定能力从激励模式中学习，并且可识别这些模式的变化形。在其后的应用研究中，Fukushima
将神经认知机主要用于手写数字的识别。随后，国内外的研究人员提出多种卷积神经网络形式，在邮政编码识别（Y. LeCun etc）、车牌识别和人脸识别等方面
得到了广泛的应用。</p>




<h2>2. CNN的结构</h2>


<p>卷积网络是为识别二维形状而特殊设计的一个多层感知器，这种网络结构对平移、比例缩放、倾斜或者共他形式的变形具有高度不变性。
这些良好的性能是网络在有监督方式下学会的，网络的结构主要有稀疏连接和权值共享两个特点，包括如下形式的约束：</br>
1 特征提取。每一个神经元从上一层的局部接受域得到突触输人，因而迫使它提取<strong>局部特征</strong>。一旦一个特征被提取出来，
只要它相对于其他特征的位置被近似地保留下来，它的精确位置就变得没有那么重要了。</br>
2 特征映射。网络的每一个计算层都是由<strong>多个特征映射组</strong>成的，每个特征映射都是平面形式的。平面中单独的神经元在约束下<strong>共享
相同的突触权值</strong>集，这种结构形式具有如下的有益效果：a.平移不变性。b.自由参数数量的缩减(通过权值共享实现)。</br>
3.子抽样。每个卷积层跟着一个实现局部平均和子抽样的计算层，由此特征映射的分辨率降低。这种操作具有使特征映射的输出对平移和其他
形式的变形的敏感度下降的作用。</p>


<!--more-->




<h3>2.1 稀疏连接(Sparse Connectivity)</h3>


<p>卷积网络通过在相邻两层之间强制使用局部连接模式来利用图像的空间局部特性，在第m层的隐层单元只与第m-1层的输入单元的局部区域有连接，第m-1层的这些局部
区域被称为空间连续的接受域。我们可以将这种结构描述如下：</br>
设第m-1层为视网膜输入层，第m层的接受域的宽度为3，也就是说该层的每个单元与且仅与输入层的3个相邻的神经元相连，第m层与第m+1层具有类似的链接规则，如下图所示。
<center><img src="http://ibillxia.github.com/images/2013/IMAG2013040201.jpg"></center>
可以看到m+1层的神经元相对于第m层的接受域的宽度也为3，但相对于输入层的接受域为5，这种结构将学习到的过滤器（对应于输入信号中被最大激活的单元）限制在局部空间
模式（因为每个单元对它接受域外的variation不做反应）。从上图也可以看出，多个这样的层堆叠起来后，会使得过滤器（不再是线性的）逐渐成为全局的（也就是覆盖到了更
大的视觉区域）。例如上图中第m+1层的神经元可以对宽度为5的输入进行一个非线性的特征编码。
</p>




<h3>2.2 权值共享(Shared Weights)</h3>


<p>在卷积网络中，每个稀疏过滤器<em>$h_{i}$</em>通过共享权值都会覆盖整个可视域，这些共享权值的单元构成一个特征映射，如下图所示。
<center><img src="http://ibillxia.github.com/images/2013/IMAG2013040202.jpg"></center>
在图中，有3个隐层单元，他们属于同一个特征映射。同种颜色的链接的权值是相同的，我们仍然可以使用梯度下降的方法来学习这些权值，只需要对原始算法做一些小的改动，
这里共享权值的梯度是所有共享参数的梯度的总和。我们不禁会问为什么要权重共享呢？一方面，重复单元能够对特征进行识别，而不考虑它在可视域中的位置。另一方面，权值
共享使得我们能更有效的进行特征抽取，因为它极大的减少了需要学习的自由变量的个数。通过控制模型的规模，卷积网络对视觉问题可以具有很好的泛化能力。
</p>




<h3>2.3 The Full Model</h3>


<p>卷积神经网络是一个多层的神经网络，每层由多个二维平面组成，而每个平面由多个独立神经元组成。网络中包含一些简单元和复杂元，分别记为S-元
和C-元。S-元聚合在一起组成S-面，S-面聚合在一起组成S-层，用Us表示。C-元、C-面和C-层(Us)之间存在类似的关系。网络的任一中间级由S-层与C-层
串接而成，而输入级只含一层，它直接接受二维视觉模式，样本特征提取步骤已嵌入到卷积神经网络模型的互联结构中。</p>




<p>一般地，Us为特征提取层，每个神经元的输入与前一层的局部感受野相连，并提取该局部的特征，一旦该局部特征被提取后，它与其他特征间的位置关系
也随之确定下来；Uc是特征映射层，网络的每个计算层由多个特征映射组成，每个特征映射为一个平面，平面上所有神经元的权值相等。特征映射结构采用
影响函数核小的sigmoid函数作为卷积网络的激活函数，使得特征映射具有位移不变性(这一句表示没看懂，那位如果看懂了，请给我讲解一下)。此外，由于
一个映射面上的神经元共享权值，因而减少了网络自由参数的个数，降低了网络参数选择的复杂度。卷积神经网络中的每一个特征提取层(S-层)都紧跟着一个
用来求局部平均与二次提取的计算层(C-层)，这种特有的两次特征提取结构使网络在识别时对输入样本有较高的畸变容忍能力。</p>




<p>下图是一个卷积网络的实例
<center><img src="http://ibillxia.github.com/images/2013/IMAG2013040203.jpg"></center>
图中的卷积网络工作流程如下，输入层由32×32个感知节点组成，接收原始图像。然后，计算流程在卷积和子抽样之间交替进行，如下所
述：第一隐藏层进行卷积，它由8个特征映射组成，每个特征映射由28×28个神经元组成，每个神经元指定一个 5×5 的接受域；第二隐藏层实现子
抽样和局部平均，它同样由 8 个特征映射组成，但其每个特征映射由14×14 个神经元组成。每个神经元具有一个 2×2 的接受域，一个可训练
系数，一个可训练偏置和一个 sigmoid 激活函数。可训练系数和偏置控制神经元的操作点。第三隐藏层进行第二次卷积，它由 20 个特征映射组
成每个特征映射由 10×10 个神经元组成。该隐藏层中的每个神经元可能具有和下一个隐藏层几个特征映射相连的突触连接，它以与第一个卷积
层相似的方式操作。第四个隐藏层进行第二次子抽样和局部平均汁算。它由 20 个特征映射组成，但每个特征映射由 5×5 个神经元组成，它以
与第一次抽样相似的方式操作。第五个隐藏层实现卷积的最后阶段，它由 120 个神经元组成，每个神经元指定一个 5×5 的接受域。最后是个全
连接层，得到输出向量。相继的计算层在卷积和抽样之间的连续交替，我们得到一个“双尖塔”的效果，也就是在每个卷积或抽样层，随着空
间分辨率下降，与相应的前一层相比特征映射的数量增加。卷积之后进行子抽样的思想是受到动物视觉系统中的“简单的”细胞后面跟着“复
杂的”细胞的想法的启发而产生的。</p>




<p>图中所示的多层感知器包含近似 100000 个突触连接，但只有大约2600 个自由参数。自由参数在数量上显著地减少是通过权值共享获得
的，学习机器的能力（以 VC 维的形式度量）因而下降，这又提高它的泛化能力。而且它对自由参数的调整通过反向传播学习的随机形式来实
现。另一个显著的特点是使用权值共享使得以并行形式实现卷积网络变得可能。这是卷积网络对全连接的多层感知器而言的另一个优点。</p>




<h2>3. CNN的学习</h2>


<p>总体而言，前面提到的卷积网络可以简化为下图所示模型：
<center><img src="http://ibillxia.github.com/images/2013/IMAG2013040204.jpg"></center>
其中，input 到C1、S4到C5、C5到output是全连接，C1到S2、C3到S4是一一对应的连接，S2到C3为了消除网络对称性，去掉了一部分连接，
可以让特征映射更具多样性。需要注意的是 C5 卷积核的尺寸要和 S4 的输出相同，只有这样才能保证输出是一维向量。</p>




<h3>3.1 卷积层的学习</h3>


<p>卷积层的典型结构如下图所示。
<center><img src="http://ibillxia.github.com/images/2013/IMAG2013040205.jpg"></center>
</p>




<p>卷积层的前馈运算是通过如下算法实现的：</br>
<center>卷积层的输出= Sigmoid( Sum(卷积) +偏移量) </center>
其中卷积核和偏移量都是可训练的。下面是其核心代码：

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>ConvolutionLayer::fprop(input,output) {
</span><span class='line'>  //取得卷积核的个数
</span><span class='line'>  int n=kernel.GetDim(0);
</span><span class='line'>  for (int i=0;i&lt;n;i++) {
</span><span class='line'>      //第i个卷积核对应输入层第a个特征映射，输出层的第b个特征映射
</span><span class='line'>      //这个卷积核可以形象的看作是从输入层第a个特征映射到输出层的第b个特征映射的一个链接
</span><span class='line'>      int a=table[i][0], b=table[i][1];
</span><span class='line'>      //用第i个卷积核和输入层第a个特征映射做卷积
</span><span class='line'>      convolution = Conv(input[a],kernel[i]);
</span><span class='line'>      //把卷积结果求和
</span><span class='line'>      sum[b] +=convolution;
</span><span class='line'>  }
</span><span class='line'>  for (i=0;i&lt;(int)bias.size();i++) {
</span><span class='line'>      //加上偏移量
</span><span class='line'>      sum[i] += bias[i];
</span><span class='line'>  }
</span><span class='line'>  //调用Sigmoid函数
</span><span class='line'>  output = Sigmoid(sum);
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>

其中，input是 n1×n2×n3 的矩阵，n1是输入层特征映射的个数，n2是输入层特征映射的宽度，n3是输入层特征映射的高度。output, sum, convolution,
bias是n1×(n2-kw+1)×(n3-kh+1)的矩阵，kw,kh是卷积核的宽度高度(图中是5×5)。kernel是卷积核矩阵。table是连接表，即如果第a输入和第b个输出之间
有连接，table里就会有[a,b]这一项，而且每个连接都对应一个卷积核。</p>




<p>卷积层的反馈运算的核心代码如下：

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>ConvolutionLayer::bprop(input,output,in_dx,out_dx) {
</span><span class='line'>  //梯度通过DSigmoid反传
</span><span class='line'>  sum_dx = DSigmoid(out_dx);
</span><span class='line'>  //计算bias的梯度
</span><span class='line'>  for (i=0;i&lt;bias.size();i++)  {
</span><span class='line'>      bias_dx[i] = sum_dx[i];
</span><span class='line'>  }
</span><span class='line'>  //取得卷积核的个数
</span><span class='line'>  int n=kernel.GetDim(0);
</span><span class='line'>  for (int i=0;i&lt;n;i++)
</span><span class='line'>  {
</span><span class='line'>      int a=table[i][0],b=table[i][1];
</span><span class='line'>      //用第i个卷积核和第b个输出层反向卷积（即输出层的一点乘卷积模板返回给输入层），并把结果累加到第a个输入层
</span><span class='line'>      input_dx[a] += DConv(sum_dx[b],kernel[i]);
</span><span class='line'>      //用同样的方法计算卷积模板的梯度
</span><span class='line'>      kernel_dx[i] += DConv(sum_dx[b],input[a]);
</span><span class='line'>  }
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>

其中in_dx,out_dx 的结构和 input,output 相同，代表的是相应点的梯度。
</p>


<p></p>




<h3>3.2 子采样层的学习</h3>


<p>子采样层的典型结构如下图所示。
<center><img src="http://ibillxia.github.com/images/2013/IMAG2013040206.jpg"></center></p>




<p>类似的字采样层的输出的计算式为：</br>
<center>输出= Sigmoid( 采样*权重 +偏移量)</center>
其核心代码如下：

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>SubSamplingLayer::fprop(input,output) {
</span><span class='line'>  int n1= input.GetDim(0);
</span><span class='line'>  int n2= input.GetDim(1);
</span><span class='line'>  int n3= input.GetDim(2);
</span><span class='line'>  for (int i=0;i&lt;n1;i++) {
</span><span class='line'>      for (int j=0;j&lt;n2;j++) {
</span><span class='line'>          for (int k=0;k&lt;n3;k++) {
</span><span class='line'>              //coeff 是可训练的权重，sw 、sh 是采样窗口的尺寸。
</span><span class='line'>              sub[i][j/sw][k/sh] += input[i][j][k]*coeff[i];
</span><span class='line'>          }
</span><span class='line'>      }
</span><span class='line'>  }
</span><span class='line'>  for (i=0;i&lt;n1;i++) {
</span><span class='line'>      //加上偏移量
</span><span class='line'>      sum[i] = sub[i] + bias[i];
</span><span class='line'>  }
</span><span class='line'>  output = Sigmoid(sum);
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>

</p>




<p>子采样层的反馈运算的核心代码如下：

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>SubSamplingLayer::bprop(input,output,in_dx,out_dx) {
</span><span class='line'>  //梯度通过DSigmoid反传
</span><span class='line'>  sum_dx = DSigmoid(out_dx);
</span><span class='line'>  //计算bias和coeff的梯度
</span><span class='line'>  for (i=0;i&lt;n1;i++) {
</span><span class='line'>      coeff_dx[i] = 0;
</span><span class='line'>      bias_dx[i] = 0;
</span><span class='line'>      for (j=0;j&lt;n2/sw;j++)
</span><span class='line'>          for (k=0;k&lt;n3/sh;k++) {
</span><span class='line'>              coeff_dx[i] += sub[j][k]*sum_dx[i][j][k];
</span><span class='line'>              bias_dx[i] += sum_dx[i][j][k]);
</span><span class='line'>          }
</span><span class='line'>  }
</span><span class='line'>  for (i=0;i&lt;n1;i++) {
</span><span class='line'>      for (j=0;j&lt;n2;j++)
</span><span class='line'>          for (k=0;k&lt;n3;k++) {
</span><span class='line'>              in_dx[i][j][k] = coeff[i]*sum_dx[i][j/sw][k/sh];
</span><span class='line'>          }
</span><span class='line'>  }
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>

</p>




<h3>3.3 全连接层的学习</h3>


<p>全连接层的学习与传统的神经网络的学习方法类似，也是使用BP算法，这里就不详述了。</p>




<p>关于CNN的完整代码可以参考https://github.com/ibillxia/DeepLearnToolbox/tree/master/CNN中的Matlab代码。</p>




<h2>References</h2>


<p>[1] Learn Deep Architectures for AI, Chapter 4.5.</br>
[2] Deep Learning Tutorial, Release 0.1, Chapter 6.</br>
[3] Convolutional Networks for Images Speech and Time-Series.</br>
[4] 基于卷积网络的三维模型特征提取. 王添翼.</br>
[5] 卷积神经网络的研究及其在车牌识别系统中的应用. 陆璐.
</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[为什么要进行傅立叶变换 ]]></title>
    <link href="http://ibillxia.github.com/blog/2013/04/04/why-do-Fourier-transformation/"/>
    <updated>2013-04-04T10:42:00+08:00</updated>
    <id>http://ibillxia.github.com/blog/2013/04/04/why-do-Fourier-transformation</id>
    <content type="html"><![CDATA[<h2>一、傅立叶变换的由来</h2>


<p>关于傅立叶变换，无论是书本还是在网上可以很容易找到关于傅立叶变换的描述，但是大都是些故弄玄虚的文章，太过抽象，
尽是一些让人看了就望而生畏的公式的罗列，让人很难能够从感性上得到理解，最近，我偶尔从网上看到一个关于数字信号处理
的电子书籍，是一个叫Steven W. Smith, Ph.D.外国人写的，写得非常浅显，里面有七章由浅入深地专门讲述关于离散信号的傅
立叶变换，虽然是英文文档，我还是硬着头皮看完了有关傅立叶变换的有关内容，看了有茅塞顿开的感觉，在此把我从中得到的
理解拿出来跟大家分享，希望很多被傅立叶变换迷惑的朋友能够得到一点启发，这电子书籍是免费的，有兴趣的朋友也可以从网
上下载下来看一下，URL地址是：http://www.dspguide.com/pdfbook.htm </p>




<p>要理解傅立叶变换，确实需要一定的耐心，别一下子想着傅立叶变换是怎么变换的，当然，也需要一定的高等数学基础，最基本
的是级数变换，其中傅立叶级数变换是傅立叶变换的基础公式。</p>




<h2>二、傅立叶变换的提出</h2>


<p>让我们先看看为什么会有傅立叶变换？傅立叶是一位法国数学家和物理学家的名字，英语原名是Jean Baptiste Joseph Fourier(1768-1830), 
Fourier对热传递很感兴趣，于1807年在法国科学学会上发表了一篇论文，运用正弦曲线来描述温度分布，论文里有个在当时具有争议性的决断：
任何连续周期信号可以由一组适当的正弦曲线组合而成。当时审查这个论文的人，其中有两位是历史上著名的数学家拉格朗日(Joseph Louis 
Lagrange, 1736-1813)和拉普拉斯(Pierre Simon de Laplace, 1749-1827)，当拉普拉斯和其它审查者投票通过并要发表这个论文时，拉格朗日
坚决反对，在近50年的时间里，拉格朗日坚持认为傅立叶的方法无法表示带有棱角的信号，如在方波中出现非连续变化斜率。法国科学学会屈服
于拉格朗日的威望，拒绝了傅立叶的工作，幸运的是，傅立叶还有其它事情可忙，他参加了政治运动，随拿破仑远征埃及，法国大革命后因会被
推上断头台而一直在逃避。直到拉格朗日死后15年这个论文才被发表出来。</p>




<!--more-->




<p>谁是对的呢？拉格朗日是对的：正弦曲线无法组合成一个带有棱角的信号。但是，我们可以用正弦曲线来非常逼近地表示它，逼近到两种表示方法
不存在能量差别，基于此，傅立叶是对的。</p>




<p>为什么我们要用正弦曲线来代替原来的曲线呢？如我们也还可以用方波或三角波来代替呀，分解信号的方法是无穷的，但分解信号的目的是为了
更加简单地处理原来的信号。用正余弦来表示原信号会更加简单，因为正余弦拥有原信号所不具有的性质：正弦曲线保真度。一个正弦曲线信号
输入后，输出的仍是正弦曲线，只有幅度和相位可能发生变化，但是频率和波的形状仍是一样的。且只有正弦曲线才拥有这样的性质，正因如此
我们才不用方波或三角波来表示。</p>




<h2>三、傅立叶变换分类</h2>


<p>根据原信号的不同类型，我们可以把傅立叶变换分为四种类别：</br>
1 非周期性连续信号 傅立叶变换（Fourier Transform）</br>
2 周期性连续信号 傅立叶级数(Fourier Series)</br>
3 非周期性离散信号 离散时域傅立叶变换（Discrete Time Fourier Transform）</br>
4 周期性离散信号 离散傅立叶变换(Discrete Fourier Transform)
</p>




<p>下图是四种原信号图例：
<center><img src="http://ibillxia.github.com/images/2013/IMAG2013040401.jpg"></center>
</p>




<p>这四种傅立叶变换都是针对正无穷大和负无穷大的信号，即信号的的长度是无穷大的，我们知道这对于计算机处理来说是不可能的，那么有没有
针对长度有限的傅立叶变换呢？没有。因为正余弦波被定义成从负无穷小到正无穷大，我们无法把一个长度无限的信号组合成长度有限的信号。面对
这种困难，方法是把长度有限的信号表示成长度无限的信号，可以把信号无限地从左右进行延伸，延伸的部分用零来表示，这样，这个信号就可以被
看成是非周期性离解信号，我们就可以用到离散时域傅立叶变换的方法。还有，也可以把信号用复制的方法进行延伸，这样信号就变成了周期性离解
信号，这时我们就可以用离散傅立叶变换方法进行变换。这里我们要学的是离散信号，对于连续信号我们不作讨论，因为计算机只能处理离散的数值
信号，我们的最终目的是运用计算机来处理信号的。</p>




<p>但是对于非周期性的信号，我们需要用无穷多不同频率的正弦曲线来表示，这对于计算机来说是不可能实现的。所以对于离散信号的变换只有离散傅立叶
变换（DFT）才能被适用，对于计算机来说只有离散的和有限长度的数据才能被处理，对于其它的变换类型只有在数学演算中才能用到，在计算机面前我们
只能用DFT方法，后面我们要理解的也正是DFT方法。这里要理解的是我们使用周期性的信号目的是为了能够用数学方法来解决问题，至于考虑周期性信号
是从哪里得到或怎样得到是无意义的。</p>




<p>每种傅立叶变换都分成实数和复数两种方法，对于实数方法是最好理解的，但是复数方法就相对复杂许多了，需要懂得有关复数的理论知识，不过，如果
理解了实数离散傅立叶变换(real DFT)，再去理解复数傅立叶就更容易了，所以我们先把复数的傅立叶放到一边去，先来理解实数傅立叶变换，在后面
我们会先讲讲关于复数的基本理论，然后在理解了实数傅立叶变换的基础上再来理解复数傅立叶变换。</p>




<p>还有，这里我们所要说的变换(transform)虽然是数学意义上的变换，但跟函数变换是不同的，函数变换是符合一一映射准则的，对于离散数字信号处理（DSP），
有许多的变换：傅立叶变换、拉普拉斯变换、Z变换、希尔伯特变换、离散余弦变换等，这些都扩展了函数变换的定义，允许输入和输出有多种的值，简单地
说变换就是把一堆的数据变成另一堆的数据的方法。</p>




<h2>四、傅立叶变换的物理意义</h2>


<p>傅立叶变换是数字信号处理领域一种很重要的算法。要知道傅立叶变换算法的意义，首先要了解傅立叶原理的意义。傅立叶原理表明：任何连续测量的时序或信号，
都可以表示为不同频率的正弦波信号的无限叠加。而根据该原理创立的傅立叶变换算法利用直接测量到的原始信号，以累加方式来计算该信号中不同正弦波信号的频率、
振幅和相位。</p>




<p>和傅立叶变换算法对应的是反傅立叶变换算法。该反变换从本质上说也是一种累加处理，这样就可以将单独改变的正弦波信号转换成一个信号。因此，可以说，
傅立叶变换将原来难以处理的时域信号转换成了易于分析的频域信号（信号的频谱），可以利用一些工具对这些频域信号进行处理、加工。最后还可以利用傅立叶
反变换将这些频域信号转换成时域信号。</p>




<p>从现代数学的眼光来看，傅里叶变换是一种特殊的积分变换。它能将满足一定条件的某个函数表示成正弦基函数的线性组合或者积分。在不同的研究领域，傅里叶
变换具有多种不同的变体形式，如连续傅里叶变换和离散傅里叶变换。</p>




<p>在数学领域，尽管最初傅立叶分析是作为热过程的解析分析的工具，但是其思想方法仍然具有典型的还原论和分析主义的特征。&#8221;任意&#8221;的函数通过一定的分解，
都能够表示为正弦函数的线性组合的形式，而正弦函数在物理上是被充分研究而相对简单的函数类：1. 傅立叶变换是线性算子,若赋予适当的范数,它还是酉算子;
2. 傅立叶变换的逆变换容易求出,而且形式与正变换非常类似;3. 正弦基函数是微分运算的本征函数,从而使得线性微分方程的求解可以转化为常系数的代数方程的
求解.在线性时不变杂的卷积运算为简单的乘积运算,从而提供了计算卷积的一种简单手段;4. 离散形式的傅立叶的物理系统内,频率是个不变的性质,从而系统对于
复杂激励的响应可以通过组合其对不同频率正弦信号的响应来获取;5. 著名的卷积定理指出:傅立叶变换可以化复变换可以利用数字计算机快速的算出(其算法称为
快速傅立叶变换算法(FFT))。</p>




<p>正是由于上述的良好性质,傅里叶变换在物理学、数论、组合数学、信号处理、概率、统计、密码学、声学、光学等领域都有着广泛的应用。</p>




<h2>五、图像傅立叶变换的物理意义</h2>


<p>图像的频率是表征图像中灰度变化剧烈程度的指标，是灰度在平面空间上的梯度。如：大面积的沙漠在图像中是一片灰度变化缓慢的区域，对应的频率值很低；
而对于地表属性变换剧烈的边缘区域在图像中是一片灰度变化剧烈的区域，对应的频率值较高。傅立叶变换在实际中有非常明显的物理意义，设f是一个能量有限的
模拟信号，则其傅立叶变换就表示f的谱。从纯粹的数学意义上看，傅立叶变换是将一个函数转换为一系列周期函数来处理的。从物理效果看，傅立叶变换是将图像
从空间域转换到频率域，其逆变换是将图像从频率域转换到空间域。换句话说，傅立叶变换的物理意义是将图像的灰度分布函数变换为图像的频率分布函数，傅立叶
逆变换是将图像的频率分布函数变换为灰度分布函数。</p>




<p>傅立叶变换以前，图像（未压缩的位图）是由对在连续空间（现实空间）上的采样得到一系列点的集合，我们习惯用一个二维矩阵表示空间上各点，则图像可由z=f(x,y)
来表示。由于空间是三维的，图像是二维的，因此空间中物体在另一个维度上的关系就由梯度来表示，这样我们可以通过观察图像得知物体在三维空间中的对应关系。为什么
要提梯度？因为实际上对图像进行二维傅立叶变换得到频谱图，就是图像梯度的分布图，当然频谱图上的各点与图像上各点并不存在一一对应的关系，即使在不移频的情况
下也是没有。傅立叶频谱图上我们看到的明暗不一的亮点，实际上图像上某一点与邻域点差异的强弱，即梯度的大小，也即该点的频率的大小（可以这么理解，图像中的低
频部分指低梯度的点，高频部分相反）。一般来讲，梯度大则该点的亮度强，否则该点亮度弱。这样通过观察傅立叶变换后的频谱图，也叫功率图，我们首先就可以看出，
图像的能量分布，如果频谱图中暗的点数更多，那么实际图像是比较柔和的（因为各点与邻域差异都不大，梯度相对较小），反之，如果频谱图中亮的点数多，那么实际图
像一定是尖锐的，边界分明且边界两边像素差异较大的。对频谱移频到原点以后，可以看出图像的频率分布是以原点为圆心，对称分布的。将频谱移频到圆心除了可以清晰
地看出图像频率分布以外，还有一个好处，它可以分离出有周期性规律的干扰信号，比如正弦干扰，一副带有正弦干扰，移频到原点的频谱图上可以看出除了中心以外还存
在以某一点为中心，对称分布的亮点集合，这个集合就是干扰噪音产生的，这时可以很直观的通过在该位置放置带阻滤波器消除干扰。</p>




<p>另外我还想说明以下几点： </br>
1、图像经过二维傅立叶变换后，其变换系数矩阵表明：若变换矩阵Fn原点设在中心，其频谱能量集中分布在变换系数短阵的中心附近(图中阴影区)。若所用的二维傅立叶
变换矩阵Fn的原点设在左上角，那么图像信号能量将集中在系数矩阵的四个角上。这是由二维傅立叶变换本身性质决定的。同时也表明一股图像能量集中低频区域。 </br>
2 、变换之后的图像在原点平移之前四角是低频，最亮，平移之后中间部分是低频，最亮，亮度大说明低频的能量大（幅角比较大）。</p>




<h2>六、一个关于实数离散傅立叶变换(Real DFT)的例子</h2>


<p>先来看一个变换实例，一个原始信号的长度是16，于是可以把这个信号分解9个余弦波和9个正弦波（一个长度为N的信号可以分解成N/2+1个正余弦信号，这是为什么呢？
结合下面的18个正余弦图,我想从计算机处理精度上就不难理解，一个长度为N的信号，最多只能有N/2+1个不同频率，再多的频率就超过了计算机所能所处理的精度范围），
如下图，9个正弦信号：
<center><img src="http://ibillxia.github.com/images/2013/IMAG2013040402.jpg"></center>
9个余弦信号：
<center><img src="http://ibillxia.github.com/images/2013/IMAG2013040403.jpg"></center>
</p>




<p>把以上所有信号相加即可得到原始信号，至于是怎么分别变换出9种不同频率信号的，我们先不急，先看看对于以上的变换结果，在程序中又是该怎么表示的，我们可以看看
下面这个示例图：
<center><img src="http://ibillxia.github.com/images/2013/IMAG2013040404.jpg"></center>
</p>




<p>上图中左边表示时域中的信号，右边是频域信号表示方法，从左向右表示正向转换(Forward DFT)，从右向左表示逆向转换(Inverse DFT)，用小写x[]表示信号在每个时间点上
的幅度值数组, 用大写X[]表示每种频率的副度值数组, 因为有N/2+1种频率，所以该数组长度为N/2+1，X[]数组又分两种，一种是表示余弦波的不同频率幅度值：Re X[]，另
一种是表示正弦波的不同频率幅度值：Im X[]，Re是实数(Real)的意思，Im是虚数(Imagine)的意思，采用复数的表示方法把正余弦波组合起来进行表示，但这里我们不考虑复
数的其它作用，只记住是一种组合方法而已，目的是为了便于表达（在后面我们会知道，复数形式的傅立叶变换长度是N，而不是N/2+1）。</p>




<h2>七、用Matlab实现快速傅立叶变换</h2>


<p>FFT是离散傅立叶变换的快速算法，可以将一个信号变换到频域。有些信号在时域上是很难看出什么特征的，但是如果变换到频域之后，就很容易看出特征了。这就是很多
信号分析采用FFT变换的原因。另外，FFT可以将一个信号的频谱提取出来，这在频谱分析方面也是经常用的。 </p>




<p>虽然很多人都知道FFT是什么，可以用来做什么，怎么去做，但是却不知道FFT之后的结果是什意思、如何决定要使用多少点来做FFT。 </p>




<p>现在就根据实际经验来说说FFT结果的具体物理意义。一个模拟信号，经过ADC采样之后，就变成了数字信号。采样定理告诉我们，采样频率要大于信号频率的两倍，这些我就不在此啰嗦了。 </p>




<p>采样得到的数字信号，就可以做FFT变换了。N个采样点，经过FFT之后，就可以得到N个点的FFT结果。为了方便进行FFT运算，通常N取2的整数次方。 </p>




<p>假设采样频率为Fs，信号频率F，采样点数为N。那么FFT之后结果就是一个为N点的复数。每一个点就对应着一个频率点。这个点的模值，就是该频率值下的幅度特性。
具体跟原始信号的幅度有什么关系呢？假设原始信号的峰值为A，那么FFT的结果的每个点（除了第一个点直流分量之外）的模值就是A的N/2倍。而第一个点就是直流分量，
它的模值就是直流分量的N倍。而每个点的相位呢，就是在该频率下的信号的相位。第一个点表示直流分量（即0Hz），而最后一个点N的再下一个点（实际上这个点是不存在的，
这里是假设的第N+1个点，也可以看做是将第一个点分做两半分，另一半移到最后）则表示采样频率Fs，这中间被N-1个点平均分成N等份，每个点的频率依次增加。例如某点n所
表示的频率为：Fn=(n-1)*Fs/N。由上面的公式可以看出，Fn所能分辨到频率为为Fs/N，如果采样频率Fs为1024Hz，采样点数为1024点，则可以分辨到1Hz。1024Hz的采样率采样
1024点，刚好是1秒，也就是说，采样1秒时间的信号并做FFT，则结果可以分析到1Hz，如果采样2秒时间的信号并做FFT，则结果可以分析到0.5Hz。如果要提高频率分辨力，则必
须增加采样点数，也即采样时间。频率分辨率和采样时间是倒数关系。 </p>




<p>假设FFT之后某点n用复数a+bi表示，那么这个复数的模就是An=根号a*a+b*b，相位就是Pn=atan2(b,a)。根据以上的结果，就可以计算出n点（n≠1，且n<=N/2）对应的信号的
表达式为：An/(N/2)*cos(2*pi*Fn*t+Pn)，即2*An/N*cos(2*pi*Fn*t+Pn)。对于n=1点的信号，是直流分量，幅度即为A1/N。由于FFT结果的对称性，通常我们只使用前半部分的
结果，即小于采样频率一半的结果。 </p>

<p>下面以一个实际的信号来做说明。假设我们有一个信号，它含有2V的直流分量，频率为50Hz、相位为-30度、幅度为3V的交流信号，以及一个频率为75Hz、相位为90度、幅度
为1.5V的交流信号。用数学表达式就是如下：S=2+3*cos(2*pi*50*t-pi*30/180)+1.5*cos(2*pi*75*t+pi*90/180)。式中cos参数为弧度，所以-30度和90度要分别换算成弧度。
我们以256Hz的采样率对这个信号进行采样，总共采样256点。按照我们上面的分析，Fn=(n-1)*Fs/N，我们可以知道，每两个点之间的间距就是1Hz，第n个点的频率就是n-1。
我们的信号有3个频率：0Hz、50Hz、75Hz，应该分别在第1个点、第51个点、第76个点上出现峰值，其它各点应该接近0。实际情况如何呢？我们来看看FFT的结果的模值如图
所示。
<center><img src="http://ibillxia.github.com/images/2013/IMAG2013040405.jpg"></center>
</p>

<p>从图中我们可以看到，在第1点、第51点、和第76点附近有比较大的值。我们分别将这三个点附近的数据拿上来细看： </br>
1点： 512+0i </br>
2点： -2.6195E-14 - 1.4162E-13i </br>
3点： -2.8586E-14 - 1.1898E-13i </br>
50点：-6.2076E-13 - 2.1713E-12i </br>
51点：332.55 - 192i </br>
52点：-1.6707E-12 - 1.5241E-12i </br>
75点：-2.2199E-13 -1.0076E-12i </br>
76点：3.4315E-12 + 192i </br>
77点：-3.0263E-14 +7.5609E-13i </br>
很明显，1点、51点、76点的值都比较大，它附近的点值都很小，可以认为是0，即在那些频率点上的信号幅度为0。接着，我们来计算各点的幅度值。分别计算这三个点的模值，结果如下： </br>
1点： 512 </br>
51点：384 </br>
76点：192 </br>
按照公式，可以计算出直流分量为：512/N=512/256=2；50Hz信号的幅度为：384/(N/2)=384/(256/2)=3；75Hz信号的幅度为192/(N/2)=192/(256/2)=1.5。可见，从频谱分析出来的幅度是正确的。</p>
 
<p>然后再来计算相位信息。直流信号没有相位可言，不用管它。先计算50Hz信号的相位，atan2(-192, 332.55)=-0.5236,结果是弧度，换算为角度就是180*(-0.5236)/pi=-30.0001。
再计算75Hz信号的相位，atan2(192, 3.4315E-12)=1.5708弧度，换算成角度就是180*1.5708/pi=90.0002。可见，相位也是对的。根据FFT结果以及上面的分析计算，我们就可以写出
信号的表达式了，它就是我们开始提供的信号。
<center><img src="http://ibillxia.github.com/images/2013/IMAG2013040406.jpg"></center>
</p>

<p>总结：假设采样频率为Fs，采样点数为N，做FFT之后，某一点n（n从1开始）表示的频率为：Fn=(n-1)*Fs/N；该点的模值除以N/2就是对应该频率下的信号的幅度（对于直流信号是除以N）；
该点的相位即是对应该频率下的信号的相位。相位的计算可用函数atan2(b,a)计算。atan2(b,a)是求坐标为(a,b)点的角度值，范围从-pi到pi。要精确到xHz，则需要采样长度为1/x秒的信号，
并做FFT。要提高频率分辨率，就需要增加采样点数，这在一些实际的应用中是不现实的，需要在较短的时间内完成分析。解决这个问题的方法有频率细分法，比较简单的方法是采样比较短
时间的信号，然后在后面补充一定数量的0，使其长度达到需要的点数，再做FFT，这在一定程度上能够提高频率分辨力。具体的频率细分法可参考相关文献。</p>

<h2>八、 让傅立叶变换从理性蜕变到感性，从抽象升华到具体</h2>
<p>（应不少网友反应说以上7部分还是不够浅显而另加的一部分，希望对大家有所启发）</p>

<p>1、我们都知道，LTI系统对谐波函数的响应也是相同频率的谐波函数，只是幅度和相位可能不同罢了，因此我们用谐波函数来表示信号正是为了导出频域的概念。
那你就会问为什么我们要在频域来分析信号，它比时域分析究竟好在哪里呢？这个问题非常好，我来回答你，第一，在频域观察和分析信号有助于揭示系统的本质属性，
更重要的是对于某些系统可以极大地简化其设计和分析过程。这一点想必大家都知道，我不再啰嗦！第二，从数学上来看，系统从时域到频域的转换就意味着系统的微分
或差分方程将转变为代数方程，而系统的分析也将采用描述系统的复系数代数方程而不是微分或差分方程。既然如此，那么请问？童鞋，你是喜欢跟微分差分方程玩儿呢
还是喜欢跟代数方程玩儿呢？假若你说你更喜欢跟微分差分方程玩儿。那我也无话可说啦！</p>

<p>可能你还是觉得以上所述只是一个很理性的认识，那么接下来，满足你的感性需求。其实，在生活中，我们无时无刻不在进行着傅立叶变换。（什么？我没有听错吧？！）
对的，请相信你的耳朵，你完全没有听错。我们来看人类听觉系统的处理过程：当我们听到一个声音，大脑的实际反应是什么？事实上耳朵感觉到一个时变的空气压力，
这种变化也许是一个类似于口哨声的单音。当我们听到一个口哨声时，我们所关心的并不是气压随时间的振动（它非常非常快！），而是声音的三个特征：基音、声强以及
音长。基音可以理解为频率的同义词，声强不是别的，它就是幅度。我们的耳朵—大脑系统能有效地将信号表示成三个简单的特征参数：基音、声强以及音长，并不理会气压
的快速变化过程（一个重复的变化过程）。这样耳朵—大脑系统就提取了信号的本质信息。傅立叶变换的分析过程与此类似，只不过我们从数学意义把它更加精确化和专业话罢了。</p>

<p>2、不要把傅立叶变换想得那么高深莫测，其实它就是对傅立叶级数的一种拓展。我们知道，傅立叶级数能描述无限时间的周期信号。那么，傅立叶级数能不能描述某些特殊的
无限时间的非周期信号呢？答案是，不能。但我们经常要分析处理这样的信号啊！于是傅立叶变换这个家伙现身啦！傅立叶变换就是为了使傅立叶级数能够描述所有（没错！
就是所有！）周期和非周期的无限时间信号而导出的，因而傅立叶变换是对傅立叶级数的一种拓展。</p>

<p>可能你还是觉得以上所述只是一个很抽象的认识，那么接下来，满足你的具体需求。我们先不管是怎么进行拓展的。我们先关注另外两个概念：周期信号和非周期信号。
他们的显著区别就在于：周期信号每隔一个有限的时间即基波周期To重复一次。它自始至终都将以这个基波周期To重复。而非周期信号则没有一个确定的或固定的周期，
可能在一段时间内他将重复某一段波形很多次，但不会在整个无限长时间范围都如此。我们找到一个周期信号的傅立叶级数，然后让这个信号的基波周期趋于无限，就
完成了从傅立叶级数到傅立叶变换的演变过程。因为当周期信号的基波周期趋于无限时，它的波形在有限长时间内都不会重复，这时它就不具有周期性啦！也就是说，
说一个信号具有无限长的周期和说它是一个非周期信号实际上是一回事！</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[反向传播(BP)神经网络]]></title>
    <link href="http://ibillxia.github.com/blog/2013/03/30/back-propagation-neural-networks/"/>
    <updated>2013-03-30T21:37:00+08:00</updated>
    <id>http://ibillxia.github.com/blog/2013/03/30/back-propagation-neural-networks</id>
    <content type="html"><![CDATA[<p>前面几篇文章中对神经网络和深度学习进行一些简介，包括神经网络的发展历史、基本概念和常见的几种神经网络以及神经网络的学习方法等，
本文具体来介绍一下一种非常常见的神经网络模型——反向传播(Back Propagation)神经网络。</p>




<h2>1.概述</h2>


<p>BP（Back Propagation）神经网络是1986年由Rumelhart和McCelland为首的科研小组提出，参见他们发表在Nature上的论文
<em><a href="http://www.cs.toronto.edu/~hinton/absps/naturebp.pdf">Learning representations by back-propagating errors</a></em>
值得一提的是，该文的第三作者Geoffrey E. Hinton就是在深度学习邻域率先取得突破的神犇。
</p>




<p>BP神经网络是一种按误差逆传播算法训练的多层前馈网络，是目前应用最广泛的神经网络模型之一。BP网络能学习和存贮大量的
输入-输出模式映射关系，而无需事前揭示描述这种映射关系的数学方程。它的学习规则是使用最速下降法，通过反向传播来不断
调整网络的权值和阈值，使网络的误差平方和最小。</p>




<!-- more -->




<h2>2.BP网络模型</h2>


<p>一个典型的BP神经网络模型如图1所示。</br>
<center><img src="http://ibillxia.github.com/images/2013/IMAG2013033001.jpg"></center>
<center>图1 典型的BP神经网络模型</center></p>




<p>BP神经网络与其他神经网络模型类似，不同的是，BP神经元的传输函数为非线性函数(而在感知机中为阶跃函数，在线性神经网络中为线性函数)，最常用的
是log-sigmoid函数或tan-sigmoid函数。BP神经网络(BPNN)一般为多层神经网络，图1中所示的BP神经网络的隐层的传输函数即为非线性函数，隐层可以有多层，
而输出层的传输函数为线性函数，当然也可以是非线性函数，只不过线性函数的输出结果取值范围较大，而非线性函数则限制在较小范围（如logsig函数输出
取值在(0,1)区间）。图1所示的神经网络的输入输出关系如下：</br>
1)输入层与隐层的关系：</br>
<center>$\boldsymbol{h} = \mathit{f_{1}} (\boldsymbol{W^{(1)}x}+\boldsymbol{b^{(1)}})$.</center>
其中$\boldsymbol{x}$为$m$维特征向量(列向量)，$\boldsymbol{W^{(1)}}$为$n × m$维权值矩阵，$\boldsymbol{b^{(1)}}$为$n$维的偏置(bias)向量(列向量)。</br>
2)隐层与输出层的关系：</br>
<center>$\boldsymbol{y} = \mathit{f_{2}} (\boldsymbol{W^{(2)}h}+\boldsymbol{b^{(2)}})$.</center>
</p>




<h2>3.BP网络的学习方法</h2>


<p>神经网络的关键之一是权值的确定，也即神经网络的学习，下面主要讨论一下BP神经网络的学习方法，它是一种监督学习的方法。</br>
假定我们有$q$个带label的样本(即输入)$p_{1},p_{2},&#8230;,p_{q}$，对应的label(即期望输出Target)为$T_{1},T_{2},&#8230;,T_{q}$，神经网络的实际输出
为$a2_{1},a2_{2},&#8230;,a2_{q}$，隐层的输出为$a1[.]$那么可以定义误差函数：</br>
<center>$\boldsymbol{E(W,B)} = \frac{1}{2}\sum_{k=1}^{n}(t_{k} - a2_{k})^{2} $.</center>
BP算法的目标是使得实际输出approximate期望输出，即使得训练误差最小化。BP算法利用梯度下降(Gradient Descent)法来求权值的变化及
误差的反向传播。对于图1中的BP神经网络，我们首先计算输出层的权值的变化量，从第$i$个输入到第$k$个输出的权值改变为：</br>
<center>$\Delta w2_{ki} = - \eta \frac{\partial E}{\partial w2_{ki}} &#92;
= - \eta \frac{\partial E}{\partial a2_{k}} \frac{\partial a2_{k}}{\partial w2_{ki}} &#92;
= \eta (t_{k}-a2_{k})f_{2}&#8217;a1_{i} = \eta \delta_{ki}a1_{i}$.</center>
其中$\eta$为学习速率。同理可得：</br>
<center>$\Delta b2_{ki} = - \eta \frac{\partial E}{\partial b2_{ki}} 
= - \eta \frac{\partial E}{\partial a2_{k}} \frac{\partial a2_{k}}{\partial b2_{ki}}
= \eta (t_{k}-a2_{k})f_{2}&#8217; = \eta \delta_{ki}$.</center>
而隐层的权值变化为：</br>
<center>$\Delta w1_{ij} = - \eta \frac{\partial E}{\partial w1_{ij}} 
= - \eta \frac{\partial E}{\partial a2_{k}} \frac{\partial a2_{k}}{\partial a1_{i}} \frac{\partial a1_{i}}{\partial w1_{ij}}
= \eta \sum_{k=1}^{n}(t_{k}-a2_{k})f_{2}&#8217;w2_{ki}f_{1}&#8217;p_{j} = \eta \delta_{ij}p_{j}$.</center>
其中，$\delta_{ij} = e_{i}f_{1}&#8217;, e_{i} = \sum_{k=1}^{n}\delta_{ki}w2_{ki}$</br>
同理可得，$\Delta b1_{i} = \eta \delta_{ij}$。</br>
这里我们注意到，输出层的误差为$e_{j},j=1..n$，隐层的误差为$e_{i},i=1..m$，其中$e_{i}$可以认为是$e_{j}$的加权组合，由于作用函数的
存在，$e_{j}$的等效作用为$\delta_{ji} = e_{j}f&#8217;()$。
</p>




<h2>4.BP网络的设计</h2>


<p>在进行BP网络的设计是，一般应从网络的层数、每层中的神经元个数和激活函数、初始值以及学习速率等几个方面来进行考虑，下面是一些选取的原则。</p>




<p><strong>1.网络的层数</strong></br>
理论已经证明，具有偏差和至少一个S型隐层加上一个线性输出层的网络，能够逼近任何有理函数，增加层数可以进一步降低误差，提高精度，但同时也是网络
复杂化。另外不能用仅具有非线性激活函数的单层网络来解决问题，因为能用单层网络解决的问题，用自适应线性网络也一定能解决，而且自适应线性网络的
运算速度更快，而对于只能用非线性函数解决的问题，单层精度又不够高，也只有增加层数才能达到期望的结果。
</p>




<p><strong>2.隐层神经元的个数</strong></br>
网络训练精度的提高，可以通过采用一个隐含层，而增加其神经元个数的方法来获得，这在结构实现上要比增加网络层数简单得多。一般而言，我们用精度和
训练网络的时间来恒量一个神经网络设计的好坏：</br>
（1）神经元数太少时，网络不能很好的学习，训练迭代的次数也比较多，训练精度也不高。</br>
（2）神经元数太多时，网络的功能越强大，精确度也更高，训练迭代的次数也大，可能会出现过拟合(over fitting)现象。</br>
由此，我们得到神经网络隐层神经元个数的选取原则是：在能够解决问题的前提下，再加上一两个神经元，以加快误差下降速度即可。
</p>




<p><strong>3.初始权值的选取</strong></br>
一般初始权值是取值在$(-1,1)$之间的随机数。另外威得罗等人在分析了两层网络是如何对一个函数进行训练后，提出选择初始权值量级为$\sqrt[r]{s}$的策略，
其中$r$为输入个数，$s$为第一层神经元个数。
</p>




<p><strong>4.学习速率</strong></br>
学习速率一般选取为$0.01 - 0.8$，大的学习速率可能导致系统的不稳定，但小的学习速率导致收敛太慢，需要较长的训练时间。对于较复杂的网络，
在误差曲面的不同位置可能需要不同的学习速率，为了减少寻找学习速率的训练次数及时间，比较合适的方法是采用变化的自适应学习速率，使网络在
不同的阶段设置不同大小的学习速率。
</p>




<p><strong>5.期望误差的选取</strong></br>
在设计网络的过程中，期望误差值也应当通过对比训练后确定一个合适的值，这个合适的值是相对于所需要的隐层节点数来确定的。一般情况下，可以同时对两个不同
的期望误差值的网络进行训练，最后通过综合因素来确定其中一个网络。
</p>




<h2>5.BP网络的局限性</h2>


<p>BP网络具有以下的几个问题：</br>
<strong>(1)需要较长的训练时间</strong>：这主要是由于学习速率太小所造成的，可采用变化的或自适应的学习速率来加以改进。</br>
<strong>(2)完全不能训练</strong>：这主要表现在网络的麻痹上，通常为了避免这种情况的产生，一是选取较小的初始权值，而是采用较小的学习速率。</br>
<strong>(3)局部最小值</strong>：这里采用的梯度下降法可能收敛到局部最小值，采用多层网络或较多的神经元，有可能得到更好的结果。
</p>




<h2>6.BP网络的改进</h2>


<p>BP算法改进的主要目标是加快训练速度，避免陷入局部极小值等，常见的改进方法有带动量因子算法、自适应学习速率、变化的学习速率以及作用函数后缩法等。
动量因子法的基本思想是在反向传播的基础上，在每一个权值的变化上加上一项正比于前次权值变化的值，并根据反向传播法来产生新的权值变化。而自适应学习
速率的方法则是针对一些特定的问题的。改变学习速率的方法的原则是，若连续几次迭代中，若目标函数对某个权倒数的符号相同，则这个权的学习速率增加，
反之若符号相反则减小它的学习速率。而作用函数后缩法则是将作用函数进行平移，即加上一个常数。</p>




<h2>7.BP网络实现异或</h2>


<p>见参考文献[7]或Andrew Ng. 的ML公开课的第8讲。</p>


<p>另外BP算法的讲解及C++实现参见[4]。</p>




<h2>参考文献</h2>


<p>[1]An Introduction to Back-Propagation Neural Networks: http://www.seattlerobotics.org/encoder/nov98/neural.html</br>
[2]Wiki - Backpropagation: http://en.wikipedia.org/wiki/Backpropagation</br>
[3]Chapter 7 The backpropagation algorithm of Neural Networks - A Systematic Introduction by Raúl Rojas: http://page.mi.fu-berlin.de/rojas/neural/chapter/K7.pdf</br>
[4]Back-propagation Neural Net - C++ 实现: http://www.codeproject.com/Articles/13582/Back-propagation-Neural-Net</br>
[5]《Visual C++数字图像模式识别技术及工程实践》(第3章)，求实科技 张宏林</br>
[6]《Matlab神经网络设置及应用》(第5章)，周品，清华大学出版社
</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[神经网络的学习方法概述]]></title>
    <link href="http://ibillxia.github.com/blog/2013/03/27/learning-process-of-neural-networks/"/>
    <updated>2013-03-27T23:51:00+08:00</updated>
    <id>http://ibillxia.github.com/blog/2013/03/27/learning-process-of-neural-networks</id>
    <content type="html"><![CDATA[<p>本文主要讨论一下神经网络的一般学习方法，主要有error-correction learning，memory-based learning， Hebbian learning，competitive learning，
Boltzmann learning等。然后介绍一些学习的方式，如监督学习、非监督学习、强化学习等。最后是一些具体的应用领域和实际问题。</p>




<h2>1.Knowledge Representation</h2>


<p>好的学习方法固然重要，但知识的表示，直接影响到feature的表示，也是非常重要的，因此在正式讨论学习方法之前，我们首先谈谈知识的表示。
首先一个问题是，什么是知识？在PRML中我们有如下定义：</br>
<blockquote><p>Knowledge refers to stored information or models used by a person or machine to interpret, predict, and appropriately respond to the outside world.</p><footer><strong>Fischler and Firschein</strong> <cite>Intelligence: The Eye，the Brain and the Computer</cite></footer></blockquote>
</p>




<!-- more -->


<p>在知识的表示中需要考虑的两个核心问题是：</br>
1). What information is actually made explicit(明确的；清楚的；直率的；详述的);</br>
2). How the information is physically encoded for subsequent(后来的，随后的) use.</br>
很显然，这里知识的表示是目标驱动的(goal directed)，在现实世界的智能设备中，一些好的解决问题的办法依赖于好的知识表示方法。
</p>




<p>一个精心设计的神经网络应该能够很恰当的表示出现实世界的知识，这是一个极大的挑战，因为知识的表示方式是多种多样的，而现实世界的知识也是丰富多彩的，
也就意味着我们的神经网络的输入是千变万化的。一般而言，在PRML中我们将现实世界的知识分为以下两种：</br>
1). Prior information = the known facts.</br>
2). Observation (measurements). Usually noisy, but give examples(prototypes) for training the neural network.</br>
其中Observation中的examples包含两种类型的，一种是labeled，另一种是unlabeled。</p>




<p>在神经网络中，那些自由变量(包括weights和biases)是知识表示的关键。知识的表示一般应该满足一下几个规则：</br>
<strong>Rule 1</strong>. Similar inputs from similar classes should produce similar representations inside the network, and they should be classiﬁed to the same category.</br>
<strong>Rule 2</strong>. Items to be categorized as separate classes should be given widely diﬀerent representations in the network.</br>
<strong>Rule 3</strong>. If a particular feature is important, there should be a large number of neurons involved in representing it in the network.</br>
<strong>Rule 4</strong>. Prior information and invariances should be built into the design of a neural network.</p>




<p>一方面，我们应该如何将先验知识运用到我们的神经网络(简记为NN)的设计中呢？没有通用的方法，但能产生更好结果的专用方法到是有两种：</br>
1). Restricting the network architecture through the use of local connections known as receptive ﬁelds(接受域).</br>
2). Constraining the choice of synaptic weights through the use of weight-sharing.</br>
这两种方法都是通过减少需要学习的自由变量来达到目的的。当然我们也可以利用Bayes公式来将先验知识应用到我们的NN的设计中。</p>




<p>另一方面，我们应该如何将Invariances(不变性；恒定性)融入到我们的NN的设计中呢？我们有三种思路可以考虑：</br>
<strong>1). Invariance by Structure</strong> - Synaptic connections between the neurons are created so that transformed versions of the 
same input are forced to produce the same output. Drawback: the number of synaptic connections tends to grow very large.</br>
<strong>2). Invariance by Training</strong> - The network is trained using diﬀerent examples of the same object corresponding to 
diﬀerent transformations (for example rotations). Drawbacks: computational load, generalization ability for other objects.</br>
<strong>3). Invariant feature space</strong> - Try to extract features of the data invariant to transformations. Use these instead of the 
original input data. Probably the most suitable technique to be used for neural classiﬁers. Requires prior knowledge on the problem.</br>
然而，要优化一个NN的结构是非常困难的，通常需要一些先验的知识。
</p>




<h2>2.Basic Learning Rules</h2>


<p>首先我们对NN的学习做如下的定义：</br>
<blockquote><p>Learning is a process by which the free parameters of a neural network are adapted through a process of stimulation by the environment in which the network is embedded. The type of learning is determined by the manner in which the parameters changes take place.</p><footer><strong>Simon Haykin</strong> <cite>Neural Networks: A Comprehensive Foundation</cite></footer></blockquote>
这个定义蕴含以下几个意思：</br>
1). The neural network is <em>stimulated</em> by an envirnment.</br>
2). The neural network <em>undergoes changes</em> in its free parameters as a result of this stimulation.</br>
3). The neural network <em>responds in a new way</em> to the envirnment because of the changes that have 
occurred in its internal structure.</br>
</p>




<h4>2.1 Error-Correction Learning</h4>


<p>Error-Correction的学习方法的核心思想是：对于给定输入，优化权值(weights)使得输出(设为$y_{k}(n)$)与真实值(设为$d_{k}(n)$)
的偏差最小。我们先定义一个error signal如下：</br>
<center>$e_{k}(n) = d_{k}(n) - y_{k}(n)$.</center></br>
那么，我们需要优化的目标函数为：</br>
<center>$\mathscr{E}(n) = \frac{1}{2}e^{2}_{k}(n)$.</center></br>
其中$\mathscr{E}(n)$称为error energy，也是我们要优化(最小化)的目标函数。</p>




<p>那么如何来求解这个优化问题呢？这里我们有一个所谓的delta-rule，也称为Widrow-Hoff rule</br>
<center>$\Delta w_{kj}(n) = \eta e_{k}(n)x_{j}(n)$.</center></br>
这里$w_{kj}(n)$是第$k$个输出神经元的第$j$个输入的权重，$\Delta w_{kj}(n)$为第$n$步迭代过程中，权值$w_{kj}(n)$的改变量，$\eta$称为学习速率，
是一个$(0,1]$之间的常数。关于该方法的详细内容会在后续文章中深入讨论。</p>




<h4>2.2 Memory-Based Learning</h4>


<p>Memory-Based Learning，顾名思义，是一种将past experiences全部保存起来的策略。假设我们的经验数据集为：</br>
<center>{$(x_{1},d_{1}),(x_{2},d_{2}),&#8230;,(x_{N},d_{N})$}.</center></br>
那么对于新来的测试数据$\mathbf{x} _{test}$，我们需要分析它与经验数据的关系，主要就是需要找出与它最相近的经验数据，即它的local neighborhood。
在Memory-Based Learning方法中，主要涉及两个问题，一个是定义local neighborhood的标准，另一个是训练样本集上的学习规则。一个最简单的学习规则是
最近邻规则(nearest-neighbor rule)。另外我们可以构造Memory-Based Classifier，如k-nearest-neighbor classifier，radial-basis function networks 
classifier等。</p>




<h4>2.3 Hebbian Learning</h4>


<p>Hebb规则是最古老也是最流行的NN学习规则，现在一般都是它的扩展版的规则，其基本思想是根据联接的神经元的活化
水平改变权，即两种神经元间联接权的变化与两神经元的活化值（激活值）相关，若突触(connection)两端的两神经元同时
兴奋，则联接加强；若不同时兴奋，则联接减弱甚至忽略。</p>




<p>Hebbian规则有以下几个特点：</br>
Time-dependent: 权值修正仅发生于突触前(如输入$x_{i}$)和突触后(如输出$y_{j}$)同时存在信号的时候；</br>
Local: 仅使用神经元能够取得的局部的信息；</br>
Interactive: 权值修正同时依赖于突触前和突触后，信号间的交互可以是确定性的或随机的；</br>
Conjunctional or Correlational: 突触前与突触后的信号产生时间与权值修正是密切相关的。</p>




<p>权值修正可以分为Hebbian, anti-Hebbian, 和non-Hebbian等三种情况。Hebbian方式会增强正相关的突触前和突触后的信号，而减弱负相关的
突触前和突触后的信号。anti-Hebbian方式则与Hebbian相反。而non-Hebbian则不使用Hebbian方式。Hebbian的权值修正方式的一般形式为：</br>
<center>$\Delta w_{kj}(n) = F(y_{k}(n),x_{j}(n))$.</center></br>
其中$F(y,x)$是关于突触后($y$)和突触前($x$)的函数，对于标准的Hebbian学习规则，该函数为$\eta y_{k}(n)x_{j}(n)$；而对于协方差的Hebbian学习
规则，该函数为$\eta [x_{j}(n)-m_{x}][y_{k}(n)-m_{y}]$。</p>




<h4>2.4 Competitive Learning</h4>


<p>竞争型学习规则是指网络的某神经元群体中所有神经元相互竞争对外界刺激模式响应的能力，竞争取胜的神经元的联接权变化向着对这一
刺激模式竞争更为有力的方向进行。具体而言，就是任何时候输出层的神经元有且仅有一个(即输出最大的那个神经元)是激活的，这种学习
规则比较适合于寻找分类任务的相关feature。</p>




<h4>2.5 Boltzmann Learning</h4>


<p>Boltzmann的学习方法是一种随机化的学习方法，它结合随机过程、概率和能量等概念来调整网络的变量，从而使网络的能量函数最小（或最大）。
在学习过程中，网络变量的随机变化不是完全随机的，而是据能量函数的改变有指导的进行。网络的变量可以是联接权，也可以是神经元的状
态。能量函数可定义为问题的目标函数或者网络输出的均方差函数。 基于Boltzmann的学习方法的NN称为Boltzmann机，关于Boltzmann机的更多
详细内容将会在后续文章中深入讨论。</p>




<h2>3.Learning Methodologies</h2>


<h4>3.1 Credit-Assignment Problem</h4>


<p>Credit Assignment(CA) Problem是指，一个learning machine的输出结果应该归功于或归咎于哪些内部或中间decision。在很多情况下，输出结果是由一些列的
actions来决定的，也就是说，中间决策过程影响需要采取的特定的action，然后这些action而不是那些decision直接影响最终的输出的。在这种情况下，我们
可以将这个CA问题分解为两个子问题：</br>
1). The assignment of credit for outcomes to actions. This is called the <em>Temporal Credit-Assignment problem</em> in that it involves the 
instants of time when the actions that deserve credit were actually taken.</br>
2). The assignment of credit for actions to internal decisions. This is called the <em>Structural Credit-Assignment problem</em> in that it involves 
assigning credit to the <em>internal strucures</em> of actions generated by the system.</br>
结构型CA问题在多组件的learning machine中比较常见，我们需要知道哪些组件需要修改，以及修改后能够对最终结果有多大的改善。而时间型CA问题中，我们需要
知道在某一时刻采取的多个action中，哪些action主要决定了最终的输出。</p>




<p>当我们使用Error-Correction来训练一个多层的前向反馈神经网络时，就会出现CA问题。很显然，最终输出与隐层和输出层的神经元都是相关的，而权值的修正
是通过当前输出自适应目标输出来实现的。</p>




<p>PS：这一节看的云里雾里的，似懂非懂，感觉有点脱离NN的样子，但这ms是一个general的问题，所以其中的一些术语也是general的，比如decision，action，credit等，
导致理解起来比较困难，:-( </p>




<h4>3.2 Learning with a Teacher</h4>


<p>Learning with a Teacher也就是supervised learning(监督学习)，Error-Correction的学习方法就属于这种。在监督学习中，对于分类或识别问题，输入数据
不仅包含输入的feature，还包含它对应的label，即它所属的类别(也就是teacher提供的answer)。Error-Correction的学习方法的目标函数就是使NN的输出与Teacher的
answer的差异最小，即均方误差最小。经过监督学习之后，NN应该能够在不需要Teacher的情况下对新数据进行处理(分类或识别等)。</p>




<h4>3.3 Learning without a Teacher</h4>


<p>Learning without a Teacher包含两种学习方法：非监督学习(Unsupervised Learning)和增强学习(Reinforcement Learning)。在非监督学习中，
没有Teacher指导学习过程，也没有可用的critic，此时NN只能尝试着学习出数据中隐含的统计规律，例如用一个适合的线性模型来区分输入数据。
Competitive Learning和Hebbian Learning都算是非监督型学习。经过非监督学习之后，NN可以对输入数据进行特征编码。</p>




<p>而在增强学习中，用到了critic，它将从环境中获取的原始信号转换为更高质量的启发式的增强信号。系统从延迟的reinforcement中学习，
这意味着系统观察到的是时序的状态向量，这最终将会产生启发式的增强信号。增强学习的目标是为了最小化一个cost-to-go-function，它的
另一个任务是discover the actions determining the best overall behavior of the system。增强学习的过程与动态规划算法非常相似。</p>




<h2>4.Learning Tasks</h2>


<p>前文中主要讨论了一些Learning Algorithm和Learning Paradigm，在这一节主要介绍一些Learning的Task，对于一个特定的Learning Task，需要使用对应的
学习算法。</p>




<h4>4.1 Pattern Association</h4>


<p><em>Associative Memory</em>是一种像大脑一样分布式的、learns by association的memory。Association是人类记忆的主要特点，它可以分为
<em>autoassociation</em>和<em>heteroassociation</em>。在autoassociation中，NN需要通过不断的将patterns(vectors)呈现给NN来保存一个pattern集合，最后
NN会呈现原始pattern的部分描述或包含噪声的version，而我们的任务就是要恢复这个特定的pattern。而在heteroassociation中，任意一个输入的pattern集与
另外人一个输出的pattern集是成对的。Autoassociation使用非监督的学习方法，而heteroassociation使用监督学习的方法。</br>
PS:这一段表示看不太懂，有些概念无法理解！</p>




<h4>4.2 Pattern Recognition</h4>


<p>模式识别如语音识别、人脸识别、物体识别等，在模式识别中，NN首先通过学习训练出网络的链接权重，然后对测试数据进行分类。一般输入数据为高维的
特征向量(feature vector)，经过训练后，数据的决策空间根据特征的pattern被分割成了若干区域。在模式识别中NN起到两种角色：一方面在非监督NN中进行特征
提取，另一方面在随后的监督学习中用于分类决策。在多层前向反馈网络中，隐层就可以看做是特征提取(往往是对特征进行了降维，即输入的维数大于隐层的
神经元个数)的角色。</p>




<h4>4.3 Function Approximation</h4>


<p>设有一个非线性的输入输出映射$d = f(x)$，其中$x$为输入，$d$为输出，而映射函数$f(.)$是未知的，但我们知道的是一系列带label的样本</br>
<center>$\mathscr{F} = $ { $(x_{1},d_{1}),(x_{2},d_{2}),&#8230;,(x_{N},d_{N})$ }.</center></br>
那么NN的目标就是找到一个映射$F(.)$最大可能的接近$f(.)$。如果有足够的训练样本和free parameters，那个这个目标是可以实现的。</p>




<h4>4.4 Control</h4>


<p>NN也可以用于控制系统，例如用在误差反馈控制系统中。</p>




<h4>4.5 Filtering</h4>


<p>一个Filter可以从包含噪声的观察样本中获取一些有趣的性质，它可以用于Filtering、Smoothing以及Prediction等，例如它可以解决cocktail party problem(鸡尾
酒会问题)，这是一个blind signal separation的问题，这可以通过independent的假设来解决。</p>




<h2>5. Adaptation</h2>


<p>在一个稳定的环境中，一个NN经过学习之后，就可以保持weight不变了，并将之应用在新数据上。但在实际应用中，环境是会随着时间而改变的，这就需要我们不断
更新我们的NN模型，也就是要根据环境变化(输入数据的变化)来改变weight，这个过程称为Adaptation。在Adaptation中，线性的adaptation方法是最简单的，然而更多的
可能是使用非线性的filter。在实际中，我们也可以在适当的时机重新训练NN。</p>




<h2>推荐资料</h2>


<p>Machine Learning Lecture by Andrew Ng, Stanford University</br>
Lecture VIII: Neural Network - Representation</br>
Lecture IX: Neural Network - Learning</br>
Video courses on Coursera: https://class.coursera.org/ml-2012-002/lecture/index</br>
Lecture homepage in Standford: http://cs229.stanford.edu/</p>




<h2>参考文献</h2>


<p>[1] Simon Haykin, “Neural Networks: a Comprehensive Foundation”, 2009 (3rd edition)</br>
[2]<a href="http://www.cis.hut.fi/Opinnot/T-61.3030/schedule2007.shtml">T-61.3030 PRINCIPLES OF NEURAL COMPUTING (5 CP)</a></br>
</p>



]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[神经网络模型分类]]></title>
    <link href="http://ibillxia.github.com/blog/2013/03/24/classes-of-neural-networks/"/>
    <updated>2013-03-24T23:07:00+08:00</updated>
    <id>http://ibillxia.github.com/blog/2013/03/24/classes-of-neural-networks</id>
    <content type="html"><![CDATA[<p>本文主要介绍一下几种不同类型的神经网络模型，主要有前馈神经网络，反馈神经网络，自组织神经网络，随机神经网络</p>




<h2>1.前馈神经网络</h2>


<h4>1)自适应线性神经网络(Adaline)</h4>


<p>自适应线性神经网络（Adaptive Linear，简称Adaline) 是由威德罗（Widrow）和霍夫（Hoff）首先提出的。它与感知器的主要不同之处在于
其神经元有一个线性激活函数，这允许输出可以是任意值，而不仅仅只是像感知器中那样只能取0或1。它采用的是W—H学习法则，也称最小均方差(LMS)
规则对权值进行训练。自适应线性元件的主要用途是线性逼近一个函数式而进行模式联想。</p>




<h4>2)单层感知器</h4>


<p>单层感知器（Perceptron）是由美国计算机科学家罗森布拉特（F.Roseblatt）于1957年提出的。它是一个具有单层神经元的网络，由线性阈值
逻辑单元所组成。它的输入可以是非离散量，而且可以通过学习而得到，这使单层感知器在神经网络研究中有着重要的意义和地位：它提出了自组织、
自学习的思想，对能够解决的问题，有一个收敛的算法，并从数学上给出了严格的证明。</p>


<!-- more -->




<h4>3)多层感知器</h4>


<p>单层感知器由于只有一个神经元，功能单一，只能完成线性决策或实现“与”、“或”、“非”等单一逻辑函数。多层感知器（Multilayer Perceptron）
是在单层感知器的基础上发展起来的，它是一种在输入层与输出层之间含有一层或多层隐含结点的具有正向传播机制的神经网络模型。多层感知器克服了
单层感知器的许多局限，它的性能主要来源于它的每层结点的非线性特性（节点输出函数的非线性特性）。如果每个结点是线性的，那么多层感知器的
功能就和单层感知器一样。</p>




<p>在人工神经网络中，应用最普遍的是多层前馈网络模型。在1986年，Rumelhant和McClelland提出了多层前馈网络的误差反向传播（Error Back Propagation）
学习算法，简称BP算法，这是一种多层网络的逆推学习算法。由此采用BP算法的多层前馈网络也广泛被称为BP网络。</p>




<h2>2.反馈神经网络</h2>


<p>反馈神经网络模型可用一完备的无向图表示。从系统的观点看，反馈神经网络模型是一反馈动力学系统，它具有极复杂的动力学特性。在反馈神经网络模型中，
我们关心的是其稳定性，稳定性是神经网络相联存储性质的体现，可以说稳定就意味着完成回忆。从计算的角度讲，反馈神经网络模型具有比前馈神经网络模型
更强的计算能力，它包括Hopfield神经网络、海明神经网络和双向联想存储器。</p>




<h4>1)Hopfield神经网络</h4>


<p>1982年，美国神经网络学者霍普菲尔德（J.J.Hopfield）提出了反馈型的全连接神经网络，是一种对记忆功能的较好模拟。Hopfield神经网络的结构特点是：
每一个神经元的输出信号通过其它神经元后，反馈到自己的输入端。这种反馈方式有利于通过联想记忆实现最优化，经过分析比较与判断确定最优解决问题的方法。
网络状态的演变是一种非线性动力学系统的行为描述过程，作为一种非线性动力学系统，系统从初始化出发后，系统状态经过演变可能发生如下结果： </br>
a) 渐进稳定形成稳定点，又称为吸引子。</br>
b) 极限环状态。</br>
c) 混沌状态。</br>
d) 发散状态。</br>
发散状态是不希望看到的。对于人工神经网络而言，由于选取网络的变换函数为一个有界函数，因此系统状态不会演变成发散。</p>




<h4>2)海明神经网络(Hamming)</h4>


<p>海明（Hamming）网络由匹配子网和竞争子网组成。匹配子网在学习阶段将若干类别的样本记忆存储在网络的连接权值中；在工作阶段（回忆阶段），
该子网计算输入模式和各个样本模式的匹配程度，并将结果送入竞争子网中，由竞争子网选择出匹配子网中最大的输出。从而，实现了对离散输入模式
进行在海明距离最小意义下的识别和分类。</p>




<h4>3)双向联想存储器(BAM)</h4>


<p>双向联想存储器（BAM）是由日本的Kosko提出的一种神经网络模型，它是ART网络模型的一种简化形式， 是一种异联想存储器。它能存储成对的
模式$(A1，B1)， (A2 ,B2), ⋯,( AN, BN)$。Ai和Bi是不同向量空间中的向量。如果模式A输入到BAM，输出是模式B，且若A与iA最为接近，B就是在BAM所
存储的向量iB。 BAM网络模型中的神经元为非线性单元，每个神经元的作用相当于一个非线性函数，这个函数一般取为S型函数：$y = \frac{1}{1+exp^{-x}}$.
</p>




<h2>3.自组织神经网络</h2>


<h4>1)自适应谐振理论(ART)</h4>


<p>自适应谐振理论（adaptive resonance theory，简称ART）的目的是为人类的心理和认知活动建立一个统一的数学理论。1976年，美国学者Carpenter
和Grossberg提出了ART神经网络模型。它是利用生物神经细胞的自兴奋与侧抑制的原理来指导学习，让输入模式通过网络的双向连接权的作用来进行比较
与识别，最后使网络对输入模式产生所谓的谐振，因此来完成对输入模式的记忆，并以同样的方式实现网络的回想。当网络已经存储了一定的内容之后，
则可用它来进行识别。在识别过程中，如果输入是已记忆的或与已记忆的模式十分相似，则网络会把它回想出来。如果是没有记忆的新模式，则在不影响
原有记忆的前提下，把它记忆下来，并用一个没用过的输出层神经元作为这一新模式的分类标志。<p>

<p>ART网络主要有三种形式：ART1是处理双极型或二进制数据，即观察向量的每个分量是二值的，只能取0或1；ART2是用于处理连续型模拟信号，
即观察向量的每个分量可取任意实数值，也可用于二进制输入；ART3是分级搜索模型，它兼容前两种结构的功能并将两层神经元网络扩大为任意
多层神经元网络，并在神经元的运行模型中纳入人类神经元生物电—化学反应机制，因而具备了相当强的功能和扩展能力。</p>

<h4>2)自组织映射神经网络模型(SOM)</h4>
<p>在人的感觉通道上一个很重要的组织原理是神经元有序地排列着，并且往往可以反映出所感觉到外界刺激的某些物理特性。如在听觉通道的每一个层次上，
其神经元与神经纤维在结构上的排列与外界刺激的频率关系十分密切，对于某个频率，相应的神经元具有最大的响应，这种听觉通道上的有序排列一直延续到
听觉皮层，尽管许多低层次上的组织是预先排好的，但高层次上的神经组织则是通过学习自组织而形成的。由此生物背景，提出了自组织映射神经网络模型（SOM）。</p>

<h4>3)对流神经网络模型(CPN)</h4>
<p>CPN是由SOM模型和Grossberg外星网络组合而形成的一种神经网络模型。是由美国Hecht-Nielsen和Robert-Nielsen于1987年首先提出来的。一般认为，
这种由两种或多种网络组合而成的新型网络往往具有比原网络模型更强的能力，它能够克服单个网络的缺陷，而且学习时间较短。</p>

<h2>4.随机神经网络</h2>
<h4>1) 模拟退火算法</h4>
<p>在物理学中，对固体物质进行退火处理时，通常先将它加温溶化，使其中的粒子可自由地运动，然后随着物质温度的下降，粒子也形成了低能态的晶格。
若在凝结点附近的温度下降速度足够慢，则固体物质一定会形成最低能量的基态。对于组合优化问题来说，它也有类似的过程，也就是说物理中固体物质的
退火过程与组合优化问题具有相似性。组合优化问题也是在解空间寻求花费函数最小（或最大）的解。</p>

<h4>2) Boltzmann机</h4>
<p>Boltzmann机是由Hinton和Sejnowski提出来的一种统计神经网络模型，是在Hopfield网络基础之上引入了随机性机制而形成的。与Hopfield神经网络不同
的是Boltzmann机具有学习能力，即其权值通过学习来调整，而不是预先设置。Boltzmann机是一种约束满足神经网络模型。</p>

<p>基于模拟退火算法的波尔兹曼机训练的基本思想为：当神经网络中某个与温度对应的参数发生变化时，神经网络的兴奋模式也会如同物理上的热运动那
样发生变化：当温度逐渐下降时，由决定函数判断神经元是否处于兴奋状态。在从高温到低温的退火(annealing)中，能量并不会停留在局部极小值上，
而以最大的概率到达全局最小值。</p>

<h3>推荐资料</h3>
<p>Machine Learning Lecture by Andrew Ng, Stanford University</br>
Lecture VIII: Neural Network - Representation</br>
Lecture IX: Neural Network - Learning</br>
Video courses on Coursera: https://class.coursera.org/ml-2012-002/lecture/index</br>
Lecture homepage in Standford: http://cs229.stanford.edu/</p>

<h3>参考文献</h3>
<p>[1] Simon Haykin, “Neural Networks: a Comprehensive Foundation”, 2009 (3rd edition)</br>
[2]T-61.3030 <a href="http://www.cis.hut.fi/Opinnot/T-61.3030/schedule2007.shtml">PRINCIPLES OF NEURAL COMPUTING</a> (5 CP)</br>
[3]人工神经网络综述：<a href="http://ishare.iask.sina.com.cn/f/36537774.html">http://ishare.iask.sina.com.cn/f/36537774.html</a></br>
</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[神经网络简介]]></title>
    <link href="http://ibillxia.github.com/blog/2013/03/20/basics-of-neural-networks/"/>
    <updated>2013-03-20T23:21:00+08:00</updated>
    <id>http://ibillxia.github.com/blog/2013/03/20/basics-of-neural-networks</id>
    <content type="html"><![CDATA[<p>Deep Learning的本质是多层的神经网络，因此在深入学习Deep Learning之前，有必要了解一些神经网络的基本知识。
本文首先对神经网络的发展历史进行简要的介绍，然后给出神经元模型的形式化描述，接着是神经网络模型的定义、特性，
最后是一些最新的进展等。关于神经网络的分类、学习方法、应用场景等将在后续文章中介绍。</p>




<h2>1.发展简史</h2>


<p>1943年，心理学家W.S.McCulloch和数理逻辑学家W.Pitts建立了神经网络和数学模型，称为MP模型。他们通过MP模型提出
了神经元的形式化数学描述和网络结构方法，证明了单个神经元能执行逻辑功能，从而开创了人工神经网络研究的时代。</br>
1945年，Von Neumann在成功的试制了存储程序式电子计算机后，他也对人脑的结构与存储式计算机进行的根本区别的比较，还提出了以简单神经元构成的自再生自动机网络结构。</br>
1949年，心理学家D.O.Heb提出了突触联系强度可变的设想，并据此提出神经元的学习准则——Hebb规则，为神经网络的学习算法奠定了基础。</br>
1958年，F.Rosenblatt提出了感知模型，该模型是由阈值神经元组成的，它试图模拟动物和人的感知和学习能力。</br>
1962年Widrow提出了自适应线性元件，这是一种连续的取值的线性网络，主要用于自适应信号处理和自适应控制。</p>




<!-- more -->


<p>Minkey和Papert从数学上对感知机的功能及其局限性做了深入的分析，于1969年出版了《Perceptron（感知机）》一书，
提出感知机不可能实现复杂的逻辑函数，他们认为感知机的功能是有限的，不能解决如XOR这样的基本问题，而且多层的
网络还不能找到有效的计算方法，进而否定了这一模型。（其实在后来发现，加入隐藏层就可以解决XOR问题。）</br>
虽然冯诺依曼结构和感知机结构大概是一个历史阶段的产物，由于《Perceptron》一书的悲观情绪和冯诺依曼机的快速发展，
神经网络进入了低潮。直到1982年Hopfield提出了HNN模型，他引入了“计算能量函数”的概念，给出了网络稳定性的判据，
推动了人工神经网络技术得以发展。特别是他提出的电子电路的实现为神经计算机的研究奠定的基础。</br>
1986年，Rumelhart及LeCun等学者提出了多层感知器的反向传播算法，克服了当初阻碍感知机继续发展的重要障碍。
与此同时，冯诺依曼机在处理视觉、听觉、联想记忆都方面都体现出了局限性，促使人们开始寻找更加接近人脑的
计算模型，于是又产生了对神经网络研究的热潮。</br>
目前，神经网络的发展非常迅速，从理论上对它的计算能力、对任意连续函数的逼近能力、学习理论以及动态网络的
稳定性分析上都取得了丰硕的成果。特别是在应用上已迅速扩展到许多重要领域，如模式识别与图像处理中的手写体
字符识别，语音识别，人脸识别，基因序列分析，控制及优化，；金融中的股票市场预测，借贷风险管理，信用卡欺骗检测等。
</p>




<h2>2 神经元模型</h2>


<h3>2.1概述</h3>


<p>神经网络的基本组成单元是神经元，在数学上的神经元模型是和在生物学上的神经细胞对应的，也就是说，人工神经网络理论是
用神经元这种抽象的数学模型来描述客观世界的生物细胞的。因此，生物的神经细胞是神经网络理论诞生和形成的物质基础和源泉。
这样，神经元的数学描述就必须以生物神经细胞的客观行为特性为依据。本节在介绍了生物神经元的基本结构的基础上，给出了神经元
的数学模型和形式化表示。</p>




<h3>2.2生物神经元</h3>


<p>生物神经元是大脑处理信息的基本单元，人脑大约由1011个神经元组成，神经元互相连接成神经网络。神经元以细胞体为主体，
由许多向周围延伸的不规则树枝状纤维构成的神经细胞，其形状很像一棵枯树的枝干。主要由细胞体、树突、轴突和突触(Synapse，
又称神经键)组成，如图1所示。</br>
<center><img src="http://ibillxia.github.com/images/2013/IMAG2013032001.png"></center>
<center>图1. 生物神经元结构图</center></br>
更多关于神经元的生物学解剖，信息的处理与传递方式以及工作特点等内容请参见[4]。</p>




<h3>2.3人工神经元模型</h3>


<p>神经元是神经网络中最基本的信息处理单元，其形式化表示如图2所示。</br>
<center><img src="http://ibillxia.github.com/images/2013/IMAG2013032002.png"></center>
<center>图2 神经元模型的形式化表示</center></p>




<p>一个典型的神经元模型由以下3个部分组成：</br>
1）突触集（a set ofsynapses）：用权重（weights）来表示。</br>
2）加法器（adder）：将加权后的输入进行求和，即$\sum_{j=1}^{n} w_{kj} x_{j}$</br>
3）激活函数（activation function）：也称为压缩函数（squashing function），作用于神经元的输出，一般是一个非线性函数，常见的有。
另外，很多时候一个神经元还包含一个偏置项bk。
可用用如下的数学表达式来刻画一个神经元：</p>


<center>$u_{k} = \sum_{j=1}^{m} w_{kj} x_{j}$，</center>


<center>$y_{k} = \varphi (u_{k} + b_{k})$.</center>


<p>其中，$u_{k}$表示输入的线性加和，$\varphi (·)$表示激活函数，$y_{k}$表示神经元的输出，$x_{i}$表示输入信号，$w_{ki}$表示权重。
很多时候，为了表示的简单，在输入中加入$x_{0}$项、权重中加入$w_{k0}$项，将偏置$b_{k}$表示为$x_{0} * w_{k0}$。</p>




<h2>3. 神经网络模型及其特性</h2>


<h3>3.1概念</h3>


<p>神经网络（Neural Networks，NN）是由大量的、简单的处理单元（称为神经元）广泛地互相连接而形成的复杂网络系统，它反映了人脑功能的
许多基本特征，是一个高度复杂的非线性动力学习系统。一个典型的神经网络结构如图3所示。这是一个多层的（包含两个隐层L2、L3）、包含两个
输出单元的神经网络拓扑结构。</br>
<center><img src="http://ibillxia.github.com/images/2013/IMAG2013032003.png"></center>
<center>图3 一个典型的神经网络结构</center></p>




<p>人工神经网络（Artificial Neural Network，ANN）结构和工作机理基本上是以人脑的组织结构（大脑神经元网络）和活动规律为背景的，
它反映了人脑的某些基本特征，但并不是要对人脑部分的真实再现，可以说它是某种抽象、简化或模仿。</p>




<h3>3.2基本特性</h3>


<p>神经网络具有四个基本特征：</p>


<h5>（1）非线性</h5>


<p>非线性关系是自然界的普遍特性。大脑的智慧就是一种非线性现象。人工神经元处于激活或抑制二种不同的状态，这种行为在数学上表现
为一种非线性关系。具有阈值的神经元构成的网络具有更好的性能，可以提高容错性和存储容量。</p>


<h5>（2）非局限性</h5>


<p>一个神经网络通常由多个神经元广泛连接而成。一个系统的整体行为不仅取决于单个神经元的特征，而且可能主要由单元之间的相互作用、
相互连接所决定。通过单元之间的大量连接模拟大脑的非局限性。联想记忆是非局限性的典型例子。</p>


<h5>（3）非常定性</h5>


<p>人工神经网络具有自适应、自组织、自学习能力。神经网络不但处理的信息可以有各种变化，而且在处理信息的同时，非线性动力系统本身
也在不断变化。经常采用迭代过程描写动力系统的演化过程。</p>


<h5>（4）非凸性</h5>


<p>一个系统的演化方向，在一定条件下将取决于某个特定的状态函数。例如能量函数，它的极值相应于系统比较稳定的状态。非凸性是指这种
函数有多个极值，故系统具有多个较稳定的平衡态，这将导致系统演化的多样性。</p>




<h2>4.近期进展</h2>


<p>最近几年神经网络模型又成了研究热点，这是因为深层神经网络的学习方法取得了突破性的成就。2006年，以Hinton为首的研究人员在深度置信
网络（Deep Belief Networks，DBNs）方面的划时代性的工作，极大的减小了深层神经网络的训练和测试误差，从此深度学习的方法一路所向披靡，
在交通路标识别、字符识别、人脸识别、语音识别等方面的顶级Contest中都取得了最佳效果。</p>




<p>值得一提的是，谷歌的“Google Brain”项目，使用16000个PC机，从YouTube视频中找到的1000万张数字照片作为训练数据集，
用非监督学习的方法建立了一个拥有10亿多条连接的深层神经网络，最后成功的从中识别出猫咪的图片。</p>




<h3>参考文献</h3>


<p>[1] Simon Haykin, “Neural Networks: a Comprehensive Foundation”, 2009 (3rd edition)</br>
[2]T-61.3030 <a href="http://www.cis.hut.fi/Opinnot/T-61.3030/schedule2007.shtml">PRINCIPLES OF NEURAL COMPUTING</a> (5 CP)</br>
[3] Wiki - Neural network: http://en.wikipedia.org/wiki/Neural_network</br>
[4] 百度百科-神经网络模型：http://baike.baidu.com/view/3406239.htm </br>
[5] 人工神经网络综述：http://ishare.iask.sina.com.cn/f/36537774.html </br>
[6] How bio-inspired deep learning keeps winning competitions：http://www.kurzweilai.net/how-bio-inspired-deep-learning-keeps-winning-competitions </br>
[7] Google&#8217;s &#8216;brain simulator&#8217;: 16,000 computers to identify a cat：http://www.smh.com.au/technology/sci-tech/googles-brain-simulator-16000-computers-to-identify-a-cat-20120626-20zmd.html 
</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[深度学习简介]]></title>
    <link href="http://ibillxia.github.com/blog/2013/03/16/introduction-to-deep-learning/"/>
    <updated>2013-03-16T21:36:00+08:00</updated>
    <id>http://ibillxia.github.com/blog/2013/03/16/introduction-to-deep-learning</id>
    <content type="html"><![CDATA[<h2>0.概述</h2>

<p>
以下是Wiki上对深度学习的下的定义：</br>
Deep learning refers to a sub-field of machine learning that is based on learning several levels of representations, 
corresponding to a hierarchy of features or factors or concepts, where higher-level concepts are defined from lower-level ones, 
and the same lower-level concepts can help to define many higher-level concepts.
</p>


<p>
深度学习就是学习多个级别的表示和抽象，帮助理解数据，如图像、声音和文本。深度学习的概念源于人工神经网络的研究，
含多隐层的多层感知器就是一种深度学习结构。那些涉及从输入产生输出的计算,我们可以用流程图来表示，
流程图的一个特殊的概念就是它的深度: 从输入到输出的路径的最长长度。传统的前馈神经网络可以理解为
深度等于层数(隐层数+1)的网络。深度学习通过组合低层特征形成更加抽象的高层表示（属性类别或特征），
以发现数据的分布式特征表示。
</p>


<h2>1.深度学习产生的背景</h2>

<h3>1.1深度不够的缺陷</h3>

<p>
在很多情况下，深度为2就已足以在给定精度范围内表示任何函数了，例如逻辑门、正常
神经元、sigmoid-神经元、SVM中的RBF(Radial Basis Function)等，但这样也有一个代价：
那就是图中需要的节点数会很多，这也就意味着当我们学习目标函数时，需要更多的计算
单元和更多的参数。理论结果显示，对于某一类函数，需要的参数的个数与输入的大小是
成指数关系的，逻辑门、正常神经元、RBF单元就属于这类。后来Hastad发现，当深度为d时，
这类函数可以用O(n)个节点（输入为n个）的神经网络有效表示，但当深度被限制为d-1时，
则需要有O(n2)个节点来表示。
</p>




<!-- more -->


<p>
我们可以讲深层结构看做是因子分解。大多数随机选择的函数，无论是用深层的结构还是用浅层结构，
都是无法有效的表示的。但很多可以用深层结构有效表示的却无法用浅层的来有效表示（参见推荐
阅读[5]中的polynomials example）。这种深层表示的现象表明，对于一些需要表示的函数，
其中存在一些结构化的特性。如果其中没有结构化的东西，那么它将无法很好的泛化。
</p>


<h3>1.2大脑具有深层的结构</h3>

<p>
例如，被深入研究的视觉皮层（如下图（a）所示）包含一系列的区域，每个区域都有输入，
信号流从一个区域到下一个区域（也有跳过连接或在某种程度上的并行路径，所以情况更复杂）。
在这种功能层次结构中，每个层次上的输入代表了不同层次的抽象特征，越上层的特征，
又越底层的信息表征，抽象性越高，如下图（c）所示。</br>
<center><img src="http://ibillxia.github.com/images/2013/IMAG2013031601.png"></center>
</p>


<p>
值得注意的是，大脑中的表示是介于密集分布和纯局部之间，也就意味着它们是稀疏的：大脑中大约1%的神经元是同时活动的。
</p>


<h3>1.3认知过程是深层次的</h3>

<p>
• 人们是使用层次化的方式来组织它们的想法和观念的；</br>
• 人们首先是学习简单的概念，然后将它们组合起来以表示更加抽象的概念；</br>
• 工程师们习惯于将解决问题的方案分解为多个层次的抽象和处理过程。
</p>


<p>如果能够像人一样学习到这些概念，那将会是非常棒的。知识工程（Knowledge Engineering）在这方面是失败的，
但语言表达概念的内省的方法也表明了稀疏表示：对于一个特定的输入（就像一幅视觉的图像），
仅仅只有一小部分的单词或概念是相关的。
</p></p>

<h2>2.深度学习的巨大突破</h2>

<h3>2.1学术上的突破</h3>

<p>
在2006年以前，尝试训练一个深层的、监督的前馈神经网络往往会比浅层的（1~2个隐层）网络产生更糟糕的结果（无论是训练误差，还是测试误差）。
但在2006年，以Hinton为首的研究人员在深度置信网络（Deep Belief Networks，DBNs）方面的划时代性的工作，将此终结。其代表性的论文是：</br>
• Hinton, G. E., Osindero, S. and Teh, Y., <a href="http://www.cs.toronto.edu/~hinton/absps/fastnc.pdf" target="_blank">A fast learning algorithm for deep belief nets</a>. Neural Computation. 18:1527-1554, 2006</br>
• Yoshua Bengio, Pascal Lamblin, Dan Popovici and Hugo Larochelle, <a href="http://www.iro.umontreal.ca/~lisa/publications2/index.php/publications/show/190" target="_blank">Greedy Layer-Wise Training of Deep Networks</a>, in J. Platt et al. (Eds), Advances in Neural Information Processing Systems 19 (NIPS 2006), pp. 153-160, MIT Press, 2007</br>
• Marc’Aurelio Ranzato, Christopher Poultney, Sumit Chopra and Yann LeCun. <a href="http://yann.lecun.com/exdb/publis/pdf/ranzato-06.pdf" target="_blank">Efficient Learning of Sparse Representations with an Energy-Based Model</a>, in J. Platt et al. (Eds), Advances in Neural Information Processing Systems (NIPS 2006), MIT Press, 2007
</p>


<p>
在这些论文中提出了以下几个非常关键的原则：</br>
• 非监督学习被用来（预）训练各个层；</br>
• 非监督学习在之前学习到的层次之上，一次只学习一个层次，每个层次学习到的结果将作为下一个层次的输入；</br>
• 除了一些专门用于预测的层次外，用监督学习来调整层与层之间的权重。</br>
这些DBNs用RBMs（Restricted Boltzmann Machines）来作为每个层的非监督学习，
Bengio的paper研究并比较了RBMs和auto-encoders（通过瓶颈内部层的表示来预测它的输入的神经网络）。
Ranzato的paper将稀疏的auto-encoder（与sparse coding相似）用在传统的神经网络结构中。
关于auto-encoders和传统的神经网络结构将在后续的文章中讲解。
</p>


<h3>2.2学术中的研究和应用</h3>

<h4>(1)计算机视觉</h4>

<p>
·ImageNet Classification with Deep Convolutional Neural Networks, Alex Krizhevsky, Ilya Sutskever, Geoffrey E Hinton, NIPS 2012.
</br>
·Learning Hierarchical Features for Scene Labeling, Clement Farabet, Camille Couprie, Laurent Najman and Yann LeCun, 
IEEE Transactions on Pattern Analysis and Machine Intelligence, 2013.
</br>
·Learning Convolutional Feature Hierachies for Visual Recognition, Koray Kavukcuoglu, Pierre Sermanet, Y-Lan Boureau, 
Karol Gregor, Micha&euml;l Mathieu and Yann LeCun, Advances in Neural Information Processing Systems (NIPS 2010), 23, 2010.
</br>……</p>


<h4>(2)语音识别</h4>

<p>
微软研究人员通过与hintion合作，首先将RBM和DBN引入到语音识别声学模型训练中，并且在大词汇量语音识别系统中获得巨大成功，使得语音识别的错误率相对减低30%。
但是，DNN还没有有效的并行快速算法，目前，很多研究机构都是在利用大规模数据语料通过GPU平台提高DNN声学模型的训练效率。
</br>
在国际上，IBM、google等公司都快速进行了DNN语音识别的研究，并且速度飞快。
</br>
国内方面，科大讯飞、百度、中科院自动化所等公司或研究单位，也在进行深度学习在语音识别上的研究。
</p>


<h4>(3)自然语言处理等其他领域</h4>

<p>
很多机构在开展研究，但目前深度学习在自然语言处理方面还没有产生系统性的突破。
</p>


<h3>2.3工程中的应用</h3>

<p>
• 微软：2009年，首先将深度学习应用到语音识别；如今，已将深度学习融入到实际的产品当中，如Xbox。
</br>
• 谷歌：“Google Brain”项目，用1.6万台机器，从1000万张图像中识别出猫，这是完全的非监督学习
（We never told it during the training, &#8216;This is a cat,&#8217; &#8230; It basically invented the concept of a cat.）。
</br>
• 百度：2012年夏开始从事深度学习方面的工作，在语音识别和图像识别中取得了巨大成功，
目前也已初步融入到百度的产品当中，如百度语音助手、百度寻人等产品。
</p>


<h2>3构建深度学习的方法</h2>

<p>
深度学习的概念和思想很简单，然而如果构建一个合理的深度网络拓扑结构，如何学习网络中的信号传递权值，
都是非常困难的问题。下面介绍几种非常perfect的方法。
</p>


<h3>3.1 Autoencoder[4]</h3>

<p>
最简单的一种方法是利用人工神经网络的特点，人工神经网络（ANN）本身就是具有层次结构的系统，
如果给定一个神经网络，我们假设其输出与输入是相同的，然后训练调整其参数，得到每一层中的权重，
自然地，我们就得到了输入I的几种不同表示（每一层代表一种表示），这些表示就是特征，在研究中可以发现，
如果在原有的特征中加入这些自动学习得到的特征可以大大提高精确度，
甚至在分类问题中比目前最好的分类算法效果还要好！这种方法称为AutoEncoder，如下图所示。</br>
<center><img src="http://ibillxia.github.com/images/2013/IMAG2013031602.png"></center>
</p>


<p>
当然，我们还可以继续加上一些约束条件得到新的Deep Learning方法，例如在AutoEncoder的基础上
加上L1的Regularity限制（L1主要是约束每一层中的节点中大部分都要为0，只有少数不为0，
这就是Sparse名字的来源），我们就可以得到Sparse AutoEncoder方法。
</p>


<h3>3.2 Sparse Coding[5]</h3>

<p>
Sparse Coding是一种利用非监督的方法来学习表示数据的过完备基的方法，它的目的就是为了找到一组基向量$\phi _{i}$，
进而将输入向量$\mathbf{x}$表示为这组基向量的线性组合:</br>
<center>$\mathbf{x} = \sum_{i=1}^{k}a _{i} \phi _{i}$.</center>
</p>


<p>
主成分分析（Principal Component Analysis，PCA）是一种有效的学习一组完备的基向量的方法，
而Sparse Coding则希望学习一组过完备的基向量。这样做的好处在于，过完备的基向量能够更好
的捕获到输入数据当中的结构和模式。然而，使用过完备的基向量带来的一个问题是，
该组基向量表示输入向量的结果不唯一，也就是说系数$a _{i}$是不唯一的。因此，在Sparse Coding当中，
我们引入一些额外的准则，即稀疏性（sparsity），来解决这个问题。具体而言，这里的稀疏性是指，
系数$a _{i}$中大多数都为零或接近为零，从优化问题的角度来讲，就是要使得系数中尽可能少的系数是
尽可能的比零大，这样就可以得到输入的唯一标示。
</p>


<h3>3.3 Restricted Boltzmann Machine（RBM）</h3>

<p>
Boltzmann Machine其实是一种无向图，里面的节点是互相连接的，但不一定是全连接，也即不是每个节点都两两相连，
连接着的两个节点之间就有一个权值。为理解方便就假设节点只能取值为0或者1，有些节点值是已知的，有些是未知的，
把已知的节点集合记为V，未知的节点集合记为H，这样就把所有节点分成两个集合，其实集合V就可以认为是visible层，
集合H就可以认为是hidden层。如果hidden层中的节点都不互相连接，visible层中的节点也都不互相连接，那么就成为了RBM模型。
</p>


<p>
在神经网络中，两层神经网络(即一个hidden层和一个output层，visible层不算在内)的建模能力是很强的，
但要求hidden层的节点数够多，但节点数太多就会导致计算量的复杂，矩阵的维护会相当大。
一个很好想到的方法就是将层数加大，通过层数的增多来缓解单层中节点数过多的负担，
比如设置两个hidden层，每层100个节点，就相当于单个hidden层100×100个节点的建模能力，
同理三个hidden层，每层分别是100、200、300个节点，就相当于单层的100×200×300个节点的建模能力。
然而这样做的问题在于，当层数大于2时，经典的训练方法效果会较差，因为参数的局部极小值太多，
容易收敛到一个不好的极值。Hinton把RBM(Restricted Boltzmann Machine)层叠在一起，训练出权值，
然后把这个权值当成是下一个RBM层的输入作为权值的初始值，利用传统的梯度下降法去训练网络，
得到了更好的结果，也即在每个RBM层通过筛选得到较好的参数初始值，使得最后的结果更好。
</p>


<h2>4.小结</h2>

<p>
当前多数分类、回归等学习方法为浅层结构算法，其局限性在于有限样本和计算单元情况下对复杂函数的表示能力有限，
针对复杂分类问题其泛化能力受到一定制约。深度学习可通过学习一种深层非线性网络结构，实现复杂函数逼近，
表征输入数据分布式表示，并展现了强大的从少数样本集中学习数据集本质特征的能力。
</p>


<p>
虽然距离深度学习的突破已经有六年多了，但它仍处于发展初期，大量工作还需要研究。
模型方面是否有其他更为有效且有理论依据的深度模型学习算法，探索新的特征提取模型是值得深入研究的内容。
此外有效的可并行训练算法也是值得研究的一个方向。当前基于最小批处理的随机梯度优化算法很难在多计算机中
进行并行训练。通常办法是利用图形处理单元加速学习过程，然而单个机器GPU对大规模数据识别或相似任务数据集
并不适用。在深度学习应用拓展方面， 如何充分合理地利用深度学习在增强传统学习算法的
性能仍是目前各领域的研究重点。
</p>


<h2>参考文献</h2>

<p>
[1]<a href="http://www.iro.umontreal.ca/~pift6266/H10/notes/deepintro.html#introduction-to-deep-learning-algorithms" target="_blank">Introduction to Deep Learning Algorithms.</a></br>
[2]<a href="http://bigeye.au.tsinghua.edu.cn/MLA12/program_files/MLA2012_%E4%BD%99%E5%87%AF.pdf" target="_blank">A tutorial on deep learning</a>，
<a href="http://www.infoq.com/cn/presentations/deep-learning-and-application-to-multimedia#" target="_blank">Video</a>.</br>
[3]<a href="http://wenku.baidu.com/view/6dcd1e3b5727a5e9856a6180.html " target="_blank">A Brief Introduction to Deep Learning</a></br>
[4]<a href="http://deeplearning.stanford.edu/wiki/index.php/Autoencoders_and_Sparsity" target="_blank">Autoencoders and Sparsity</a></br>
[5]<a href="http://deeplearning.stanford.edu/wiki/index.php/Sparse_Coding" target="_blank">Sparse Coding</a></br>
[6]<a href="http://blog.sina.com.cn/s/blog_70a384770101f58p.html" target="_blank">关于深度学习(deep learning)</a></br>
[7] 百度百科-<a href="http://baike.baidu.com/view/9964678.htm" target="_blank">深度学习</a></br>
[8] 孙志军等，深度学习研究综述.</p>


<h2>推荐阅读</h2>

<p>
[1] Chris Bishop, “<a href="http://research.microsoft.com/en-us/um/people/cmbishop/prml/" target="_blank">Pattern Recognition and Machine Learning</a>”, 2007</br>
[2] Simon Haykin, “<a href="http://www.amazon.com/Neural-Networks-A-Comprehensive-Foundation/dp/B000O8QMAU" target="_blank">Neural Networks: a Comprehensive Foundation</a>”, 2009 (3rd edition)</br>
[3] Richard O. Duda, Peter E. Hart and David G. Stork, “<a href="http://www.rii.ricoh.com/~stork/DHS.html" target="_blank">Pattern Classification</a>”, 2001 (2nd edition)</br>
[4]Deep Learning Tutorial：<a href="http://deeplearning.net/tutorial/ " target="_blank">http://deeplearning.net/tutorial/</a></br>
[5]Yoshua Bengio, <a href="http://www.iro.umontreal.ca/~lisa/publications2/index.php/publications/show/239" target="_blank">Learning Deep Architectures for AI</a>, Foundations & Trends in ML, 2(1), 2009</br>
[6] Unsupervised Feature Learning and Deep Learning：<a href="http://deeplearning.stanford.edu/wiki/index.php/UFLDL_Tutorial" target="_blank">UFLDL Tutorial</a></br>
[7] <a href="http://deeplearning.net/" target="_blank">http://deeplearning.net/</a></br>
[8] 深度学习相关软件包（Matlab）: <a href="https://github.com/rasmusbergpalm/DeepLearnToolbox" target="_blank">https://github.com/rasmusbergpalm/DeepLearnToolbox</a>
</p>

]]></content>
  </entry>
  
</feed>
