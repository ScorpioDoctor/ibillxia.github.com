<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Bill's Blog]]></title>
  <link href="http://ibillxia.github.com/atom.xml" rel="self"/>
  <link href="http://ibillxia.github.com/"/>
  <updated>2013-06-30T11:09:25+08:00</updated>
  <id>http://ibillxia.github.com/</id>
  <author>
    <name><![CDATA[Bill Xia]]></name>
    <email><![CDATA[ibillxia@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[给Octpress博客添加返回顶部按钮]]></title>
    <link href="http://ibillxia.github.com/blog/2013/06/30/add-a-back-to-top-button-on-ur-octpress-blog/"/>
    <updated>2013-06-30T10:47:00+08:00</updated>
    <id>http://ibillxia.github.com/blog/2013/06/30/add-a-back-to-top-button-on-ur-octpress-blog</id>
    <content type="html"><![CDATA[<p>有时候，博客文章太长，需要返回顶部时，需要用鼠标拖着滚动条向上好半天，这里提供一个用jQuery来实现的动态上滚的示例。
这个示例完全参考和翻译自webdesignerwall的blog：<a href="http://webdesignerwall.com/tutorials/animated-scroll-to-top">http://webdesignerwall.com/tutorials/animated-scroll-to-top</a>，
其中有部分删改，并在本人的blog上实现。</p>




<p>主要包含HTML和CSS的设计，基于jQuery的JS的设计。另外还有一点小trick</p>




<h2>Design & CSS</h2>


<p>相关的HTML代码很简单，在source/_include/custom/footer.html中添加如下代码：

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&lt;p id = "back-top">
</span><span class='line'>      &lt;a href="#top">&lt;span>&lt;/span>Back to Top&lt;/a>
</span><span class='line'>  &lt;/p></span></code></pre></td></tr></table></div></figure>

</p>




<!--more-->




<p>对应的CSS样式的设置如下：（这段代码同样的放在source/_include/custom/footer.html文件中）

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&lt;style type="text/css">
</span><span class='line'>#back-top {
</span><span class='line'>  position: fixed;
</span><span class='line'>  bottom: 50px;
</span><span class='line'>  right: 100px;
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>#back-top a {
</span><span class='line'>  width: 80px;
</span><span class='line'>  display: block;
</span><span class='line'>  text-align: center;
</span><span class='line'>  font: 11px/100% Arial, Helvetica, sans-serif;
</span><span class='line'>  text-transform: uppercase;
</span><span class='line'>  text-decoration: none;
</span><span class='line'>  color: #bbb;
</span><span class='line'>
</span><span class='line'>  /* transition */
</span><span class='line'>  -webkit-transition: 1s;
</span><span class='line'>  -moz-transition: 1s;
</span><span class='line'>  transition: 1s;
</span><span class='line'>}
</span><span class='line'>#back-top a:hover {
</span><span class='line'>  color: #000;
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>/* arrow icon (span tag) */
</span><span class='line'>#back-top span {
</span><span class='line'>  width: 80px;
</span><span class='line'>  height: 80px;
</span><span class='line'>  display: block;
</span><span class='line'>  margin-bottom: 7px;
</span><span class='line'>  background: #ddd url(/images/up-arrow.png) no-repeat center center;
</span><span class='line'>
</span><span class='line'>  /* rounded corners */
</span><span class='line'>  -webkit-border-radius: 15px;
</span><span class='line'>  -moz-border-radius: 15px;
</span><span class='line'>  border-radius: 15px;
</span><span class='line'>
</span><span class='line'>  /* transition */
</span><span class='line'>  -webkit-transition: 1s;
</span><span class='line'>  -moz-transition: 1s;
</span><span class='line'>  transition: 1s;
</span><span class='line'>}
</span><span class='line'>/*
</span><span class='line'>#back-top a:hover span {
</span><span class='line'>  background-color: #888;
</span><span class='line'>}
</span><span class='line'>*/
</span><span class='line'>&lt;/style></span></code></pre></td></tr></table></div></figure>

</p>




<p>上面的css中用到了一张图片up-arrow.png，放在source/images/目录下，图片如下：
<center><img src="http://ibillxia.github.com/images/up-arrow.png"></center>
这是从google image里面随便找的一个，你也可以找一个自己喜欢的图片。
</p>




<h2>jQuery部分</h2>


<p>HTML和CSS样式设置好了之后，最后就是添加JavaScript事件响应代码了，这里是基于jQuery实现的。代码如下：（这段代码还是放在source/_include/custom/footer.html文件中）

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&lt;script type="text/javascript" src="http://ajax.googleapis.com/ajax/libs/jquery/1.4.3/jquery.min.js">&lt;/script>
</span><span class='line'>&lt;script type="text/javascript">
</span><span class='line'>$(document).ready(function(){
</span><span class='line'>
</span><span class='line'>  // hide #back-top first
</span><span class='line'>  $("#back-top").hide();
</span><span class='line'>  
</span><span class='line'>  // fade in #back-top
</span><span class='line'>  $(function () {
</span><span class='line'>      $(window).scroll(function () {
</span><span class='line'>          if ($(this).scrollTop() > 100) {
</span><span class='line'>              $('#back-top').fadeIn();
</span><span class='line'>          } else {
</span><span class='line'>              $('#back-top').fadeOut();
</span><span class='line'>          }
</span><span class='line'>      });
</span><span class='line'>
</span><span class='line'>      // scroll body to 0px on click
</span><span class='line'>      $('#back-top a').click(function () {
</span><span class='line'>          $('body,html').animate({
</span><span class='line'>              scrollTop: 0
</span><span class='line'>          }, 800);
</span><span class='line'>          return false;
</span><span class='line'>      });
</span><span class='line'>  });
</span><span class='line'>
</span><span class='line'>});
</span><span class='line'>&lt;/script></span></code></pre></td></tr></table></div></figure>

</p>




<h2>一个Trick</h2>


<p>
在上面的HTML代码中，我们将一个链接添加到了ID为#top的里面，这个#top标签是<body>标签的ID，这样即使浏览器不支持相关的JS，
通过这个link也实现了返回顶部的功能。
</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[蛙泳、自由泳、仰泳、蝶泳图解动画教你游泳]]></title>
    <link href="http://ibillxia.github.com/blog/2013/06/28/swimming-tutorial-with-gif-images/"/>
    <updated>2013-06-28T21:37:00+08:00</updated>
    <id>http://ibillxia.github.com/blog/2013/06/28/swimming-tutorial-with-gif-images</id>
    <content type="html"><![CDATA[<p>最近在室友的带领下，开始去cjl游泳馆学游泳，这里转载一篇游泳教程，分享给初学游泳的网友们</p>




<h2>蛙泳</h2>


<p>蛙泳配合有一个顺口溜，在讲解蛙泳动作要领之前先介绍给大家：“划手腿不动，收手再收腿，先伸胳膊后蹬腿，并拢伸直漂一会儿。”
从顺口溜中可以看到，手的动作是先于腿的动作。一定要在收手后再收腿，伸手后再蹬腿。</p>




<center><img src="http://ibillxia.github.com/images/2013/IMAG2013062801.gif"></center>




<p>臂部动作：</br>
1、外划。双手前伸，手掌倾斜大约45度(小拇指朝上)。双手同时向外、后方划，继而屈臂向后、向下方划。</br>
2、内划。掌心由外转向内，手带动小臂加速内划，手由下向上并在胸前并拢(手高肘低、肘在肩下)，前伸。</br>
3、前伸。双手向前伸(肘关节伸直)。要提醒大家注意的是：外划是放松的，内划是用力的、加速完成的、前伸是积极的。
</p>




<!--more-->




<center><img src="http://ibillxia.github.com/images/2013/IMAG2013062802.gif"></center>




<p>腿部动作：</br>
1、收腿：屈膝收腿，脚跟向臀部靠拢，小腿要躲在大腿后面慢收腿，这样可以减少阻力。收腿结束时，两膝与肩同宽，小腿与水面垂直，脚牚在水面附近。</br>
2、翻脚：两脚距离大于两膝距离，两脚外翻，脚尖朝外，脚牚朝天，小腿和脚内侧对准水，像英文字母“W”。</br>
3、夹蹬水：实际上是腿伸直的过程(屈髋、伸膝)，由腰腹和大腿同时发力，以小腿和脚内侧同时蹬夹水，先是向外、向后、向下，然后是向内、向上方蹬水，
就像画半个圆圈。向外蹬水和向内夹水是连续完成的，也就是连蹬带夹。蹬夹水完成时双腿并拢伸直，双脚内转，脚尖相对。蹬水的速度不要过猛，要由
慢到快地加速蹬水，两条腿将近伸直并拢的时候蹬水速度最快。</br>
4、停：双腿并拢伸直后在一个短暂的滑行(1-2秒)。
</p>




<center><img src="http://ibillxia.github.com/images/2013/IMAG2013062803.gif"></center>




<p>蛙泳的完整配合动作：双手外划时抬头换气，双手内划时收腿低头稍憋气，双手前伸过头时蹬腿吐气。</p>




<h2>自由泳</h2>


<p>游泳是全身运动，任何一个部位的活动都离不开全身的协调配合。从表面上看，自由泳依靠划水和打腿产生推进力，实际上，躯干的作用也不能忽视。首先，
躯干应保持一定的紧张度，腰部如果松软，整个人就像一摊泥一样。其次，身体的转动能够有效地发挥躯干部大肌肉群的力量，减少阻力，提高工作效果。</p>




<p>自由泳的完整配合有多种形式。一般常见的是每划水2次，打水6次，呼吸1次。</p>




<center><img src="http://ibillxia.github.com/images/2013/IMAG2013062804.gif"></center>




<p>1、手的入水点在肩的延长线和身体中线之间，以大拇指领先，斜插入水。</br>
2、入水后，手、肘、肩继续前伸，使手臂伸展。随着身体的转动，屈腕、屈肘，手臂向外、后方抓水;手下划到最低点后，旋转手臂向内、上、后方划水，
保持高肘屈臂的划水姿势。</br>
3、手臂与水平面垂直时，经手领先，加速推水，手臂转为向外、向上和身后划水直到大腿侧，提肘出水。</br>
4、出水后，手臂自然、放松地经空中向前移臂，保持高肘姿势。然后手在肩前领先入水，开始下一个动作。
</p>




<center><img src="http://ibillxia.github.com/images/2013/IMAG2013062805.gif"></center>




<p>1、手臂在水下成曲线划水路线，从侧面看，手相对于身体的划水轨迹为“S”形。</br>
2、自由泳两臂配合有前交叉配合、中交叉配合、和后交叉配合3种基本形式。本图为前交叉形式，为初学者比较容易掌握的方式。
</p>


<center><img src="http://ibillxia.github.com/images/2013/IMAG2013062806.gif"></center>




<p>单臂打水划臂动作是初学者应该重点练习的动作。如此图，左臂划水，那么可以右臂扶板。一般腿打水10次左右，手臂划水一次。
掌握到一定程度的时候可以加上呼吸练习。</p>




<p>腿部鞭状打水：</br>
1、打腿动作从髋部开始发力，大腿带动小腿，做鞭状打水动作。</br>
2、向上打水腿从直到弯。以直腿开始向上打，脚接近水面时屈膝，小腿上抬，使脚牚露出水面后向下打水。开始可直腿打水，但腿略放松，不要僵硬，
在水的压力下腿会自然弯曲。向下打水前膝关节弯曲角度约130-160度，打水幅度约为30-40厘米。打水时要绷脚(芭蕾脚)，不要勾脚。
</p>




<h2>仰泳</h2>


<p>1、臂划水时，出水以大拇指领先，移臂时手臂与水面垂直，上臂贴近耳朵。移臂过程中手臂旋转，入水时小拇指领先插入水中。</br>
2、如果以头的位置为钟表12点，两手的入水点在11点和1点的位置。手入水后先直臂下划。</br>
3、两臂划水应与身体转动协调配合，两肩不断形成位置差。</br>
4、两臂划水配合采用中交叉方式，即两臂始终处于相反的位置，一臂划水时，另一臂移臂。</br>
5、头部保持稳定没有左右摆动。
</p>




<center><img src="http://ibillxia.github.com/images/2013/IMAG2013062807.gif"></center>




<p>1、呼吸虽然不受限制，但最好采用有节奏的呼吸方式，或以固定在一臂移臂时吸气。毕竟划水以及身体在水中行进时会有波浪及水花。随意呼吸易呛水。</br>
2、保持水平的身体姿势，躯干和肩随手臂动作围绕纵轴转动，始终有一肩不露出水面。</br>
3、一般每划水2次，腿打水6次，呼吸1次。</br>
4、两腿交替做鞭状上下打水。向上打水要快而有力，脚略内旋并绷直，向下打水时腿和脚自然放松。
</p>




<center><img src="http://ibillxia.github.com/images/2013/IMAG2013062808.gif"></center>




<p>1、移臂时手臂紧贴身体不能太宽。</br>
2、移臂时如果手臂易弯曲，则可暂时用小拇指领先出水，养成直臂出水的习惯以后再用大拇指领先出水。</br>
3、身体始终保持伸展、正直、几乎水平地仰卧于水面，好像平躺在床上，头下有一只矮枕头。
</p>




<center><img src="http://ibillxia.github.com/images/2013/IMAG2013062809.gif"></center>




<p>建议：仰泳腿要体会大腿用力，上抬与下压都要有，体会大腿带动小腿的感觉。</p>




<h2>蝶泳</h2>


<center><img src="http://ibillxia.github.com/images/2013/IMAG2013062810.gif"></center>


<p>1、蝶泳的手入水点在两肩的延长线上，以大拇指领先，斜插入水。</br>
2、入水后，肩、肘前伸，两手沿曲线向外、后、下方抓水。两手分开到肩宽时，屈肘，加速划水。</br>
3、两手分开到达到最大宽度后，手臂转为向内、向上和向后划水，手臂上抬时保持高肘屈臂。两手在胸下或腹下时，手之间的距离最近。</br>
4、呼吸与划水的配合也是蝶泳技术的关键。手臂结束向内划水时，头露出水面吸气，移臂时头还原入水。记住两个“之前”，即头在手出水前出水，在手入水前入水。
</p>




<center><img src="http://ibillxia.github.com/images/2013/IMAG2013062811.gif"></center>




<p>1、蝶泳双手划水两手距离接近最近时，手臂划水的方向再一次改变，转为向外、向上和向后划水，直至出水。</br>
2、划水出水后，手臂在肩的带动下经空中向前移臂，准备入水、移臂一般以低、平、放松的姿势从两侧前移。</br>
3、蝶泳的身体姿势掌握比较难，同时鞭状打水也不易掌握。在蝶泳学习的时候，我们会有专门的分解练习让您逐步掌握运作。
</p>




<center><img src="http://ibillxia.github.com/images/2013/IMAG2013062812.gif"></center>




<p>1、蝶泳的划水路线一般为“钥匙孔”形，指两手在胸下或腹下时的距离最近，这种前后划水路线比较均匀。</br>
2、注意蝶泳的四肢动作是双臂、双腿同时协调发力。
</p>



]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[用Windows API实现一个简单的录音程序]]></title>
    <link href="http://ibillxia.github.com/blog/2013/06/04/a-simple-code-for-wave-recording-using-windows-api/"/>
    <updated>2013-06-04T23:59:00+08:00</updated>
    <id>http://ibillxia.github.com/blog/2013/06/04/a-simple-code-for-wave-recording-using-windows-api</id>
    <content type="html"><![CDATA[<p>本文介绍如何使用Windows API来录制语音信号兵保存到wave文件中，主要用到三个结构体和几个wave开头的API函数（在Winmm.lib文件中）。其中三个结构体是WAVEFORMATEX、WAVEHDR、MMTIME，其详细定义都在MMSystem.h中定义，
可以转到定义看其详细内容及每一项的英文注释。用到的API函数的详细用法可以参见MSDN： http://msdn.microsoft.com/en-us/library/windows/desktop/dd743847(v=vs.85).aspx
详细的使用过程请看下文的源代码，这是一个Win32 Application，需要手动添加Winmm.lib的依赖。</p>




<!--more-->




<p>实例程序</p>




<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
<span class='line-number'>64</span>
<span class='line-number'>65</span>
<span class='line-number'>66</span>
<span class='line-number'>67</span>
<span class='line-number'>68</span>
<span class='line-number'>69</span>
<span class='line-number'>70</span>
<span class='line-number'>71</span>
<span class='line-number'>72</span>
<span class='line-number'>73</span>
<span class='line-number'>74</span>
<span class='line-number'>75</span>
<span class='line-number'>76</span>
<span class='line-number'>77</span>
<span class='line-number'>78</span>
<span class='line-number'>79</span>
<span class='line-number'>80</span>
<span class='line-number'>81</span>
<span class='line-number'>82</span>
<span class='line-number'>83</span>
<span class='line-number'>84</span>
<span class='line-number'>85</span>
<span class='line-number'>86</span>
<span class='line-number'>87</span>
<span class='line-number'>88</span>
<span class='line-number'>89</span>
<span class='line-number'>90</span>
<span class='line-number'>91</span>
<span class='line-number'>92</span>
<span class='line-number'>93</span>
<span class='line-number'>94</span>
<span class='line-number'>95</span>
<span class='line-number'>96</span>
<span class='line-number'>97</span>
<span class='line-number'>98</span>
<span class='line-number'>99</span>
<span class='line-number'>100</span>
<span class='line-number'>101</span>
<span class='line-number'>102</span>
<span class='line-number'>103</span>
<span class='line-number'>104</span>
<span class='line-number'>105</span>
<span class='line-number'>106</span>
<span class='line-number'>107</span>
<span class='line-number'>108</span>
<span class='line-number'>109</span>
<span class='line-number'>110</span>
<span class='line-number'>111</span>
<span class='line-number'>112</span>
<span class='line-number'>113</span>
<span class='line-number'>114</span>
<span class='line-number'>115</span>
<span class='line-number'>116</span>
<span class='line-number'>117</span>
<span class='line-number'>118</span>
<span class='line-number'>119</span>
<span class='line-number'>120</span>
<span class='line-number'>121</span>
<span class='line-number'>122</span>
<span class='line-number'>123</span>
<span class='line-number'>124</span>
<span class='line-number'>125</span>
<span class='line-number'>126</span>
<span class='line-number'>127</span>
<span class='line-number'>128</span>
<span class='line-number'>129</span>
<span class='line-number'>130</span>
<span class='line-number'>131</span>
<span class='line-number'>132</span>
<span class='line-number'>133</span>
<span class='line-number'>134</span>
<span class='line-number'>135</span>
<span class='line-number'>136</span>
<span class='line-number'>137</span>
<span class='line-number'>138</span>
<span class='line-number'>139</span>
<span class='line-number'>140</span>
<span class='line-number'>141</span>
<span class='line-number'>142</span>
<span class='line-number'>143</span>
<span class='line-number'>144</span>
<span class='line-number'>145</span>
<span class='line-number'>146</span>
<span class='line-number'>147</span>
<span class='line-number'>148</span>
<span class='line-number'>149</span>
<span class='line-number'>150</span>
<span class='line-number'>151</span>
<span class='line-number'>152</span>
<span class='line-number'>153</span>
<span class='line-number'>154</span>
<span class='line-number'>155</span>
<span class='line-number'>156</span>
<span class='line-number'>157</span>
<span class='line-number'>158</span>
<span class='line-number'>159</span>
<span class='line-number'>160</span>
<span class='line-number'>161</span>
<span class='line-number'>162</span>
<span class='line-number'>163</span>
<span class='line-number'>164</span>
<span class='line-number'>165</span>
<span class='line-number'>166</span>
<span class='line-number'>167</span>
<span class='line-number'>168</span>
<span class='line-number'>169</span>
<span class='line-number'>170</span>
<span class='line-number'>171</span>
<span class='line-number'>172</span>
<span class='line-number'>173</span>
<span class='line-number'>174</span>
<span class='line-number'>175</span>
<span class='line-number'>176</span>
<span class='line-number'>177</span>
<span class='line-number'>178</span>
<span class='line-number'>179</span>
<span class='line-number'>180</span>
<span class='line-number'>181</span>
<span class='line-number'>182</span>
<span class='line-number'>183</span>
<span class='line-number'>184</span>
<span class='line-number'>185</span>
<span class='line-number'>186</span>
<span class='line-number'>187</span>
<span class='line-number'>188</span>
<span class='line-number'>189</span>
<span class='line-number'>190</span>
<span class='line-number'>191</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>// ******************* FileName: WinMain.cpp *****************************
</span><span class='line'>// 该源程序需要加入到 VC6 的 Win32 Application 的 empty Project 中
</span><span class='line'>// 对于工程的 Link 选项，至少要包含以下库: msvcrt.lib Winmm.lib
</span><span class='line'>
</span><span class='line'>#include &lt;stdio.h>
</span><span class='line'>#include &lt;atlstr.h>
</span><span class='line'>#include &lt;windows.h>
</span><span class='line'>#include &lt;Mmsystem.h>
</span><span class='line'>
</span><span class='line'>#pragma comment(lib,"Winmm.lib")
</span><span class='line'>
</span><span class='line'>char lpTemp[256];
</span><span class='line'>
</span><span class='line'>DWORD FCC(LPSTR lpStr)
</span><span class='line'>{
</span><span class='line'>  DWORD Number = lpStr[0] + lpStr[1] *0x100 + lpStr[2] *0x10000 + lpStr[3] *0x1000000 ;
</span><span class='line'>  return Number;
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>int WINAPI WinMain( HINSTANCE hInstance, HINSTANCE hPrevInstance, LPSTR lpCmdLine, int nCmdShow )
</span><span class='line'>{
</span><span class='line'>  DWORD datasize = 48000;
</span><span class='line'>    
</span><span class='line'>  // 设置录音采样参数
</span><span class='line'>  WAVEFORMATEX waveformat;
</span><span class='line'>  waveformat.wFormatTag=WAVE_FORMAT_PCM; // 指定录音格式
</span><span class='line'>  waveformat.nChannels=1;
</span><span class='line'>  waveformat.nSamplesPerSec=8000;
</span><span class='line'>  waveformat.nBlockAlign=1;
</span><span class='line'>  waveformat.wBitsPerSample=8;
</span><span class='line'>  waveformat.cbSize=0;
</span><span class='line'>  waveformat.nAvgBytesPerSec=waveformat.nSamplesPerSec*waveformat.wBitsPerSample/8;
</span><span class='line'>  
</span><span class='line'>  sprintf(lpTemp,"WAVEFORMATEX size = %lu", sizeof(WAVEFORMATEX));
</span><span class='line'>  MessageBox(NULL,CString(lpTemp),CString("提示"),MB_OK);
</span><span class='line'>
</span><span class='line'>  HWAVEIN m_hWaveIn;
</span><span class='line'>  if ( !waveInGetNumDevs() )
</span><span class='line'>  {
</span><span class='line'>      MessageBox(NULL,CString("没有可以使用的 WaveIn 通道"),CString("提示"),MB_OK);
</span><span class='line'>      return 0;
</span><span class='line'>  }
</span><span class='line'>
</span><span class='line'>  // 打开录音设备
</span><span class='line'>  int res = waveInOpen(&m_hWaveIn,WAVE_MAPPER, &waveformat, (DWORD)NULL,0L,CALLBACK_WINDOW); 
</span><span class='line'>  if ( res != MMSYSERR_NOERROR )
</span><span class='line'>  {
</span><span class='line'>     sprintf(lpTemp, "打开 waveIn 通道失败，Error_Code = 0x%x", res );
</span><span class='line'>     MessageBox(NULL,CString(lpTemp),CString("提示"),MB_OK);
</span><span class='line'>     return 0;
</span><span class='line'>  }
</span><span class='line'>  
</span><span class='line'>  WAVEHDR m_pWaveHdr;
</span><span class='line'>  m_pWaveHdr.lpData = (char *)GlobalLock( GlobalAlloc(GMEM_MOVEABLE|GMEM_SHARE, datasize) );
</span><span class='line'>  memset(m_pWaveHdr.lpData, 0, datasize );
</span><span class='line'>  m_pWaveHdr.dwBufferLength = datasize;
</span><span class='line'>  m_pWaveHdr.dwBytesRecorded = 0;
</span><span class='line'>  m_pWaveHdr.dwUser = 0;
</span><span class='line'>  m_pWaveHdr.dwFlags = 0;
</span><span class='line'>  m_pWaveHdr.dwLoops = 0;
</span><span class='line'>  sprintf( lpTemp, "WAVEHDR size = %lu", sizeof(WAVEHDR) );
</span><span class='line'>  MessageBox(NULL,CString(lpTemp),CString("提示"),MB_OK);
</span><span class='line'>
</span><span class='line'>  // 准备内存块录音
</span><span class='line'>  int resPrepare = waveInPrepareHeader( m_hWaveIn, &m_pWaveHdr, sizeof(WAVEHDR) ); 
</span><span class='line'>  if ( resPrepare != MMSYSERR_NOERROR) 
</span><span class='line'>  {
</span><span class='line'>      sprintf(lpTemp, "不能开辟录音头文件，Error_Code = 0x%03X", resPrepare );
</span><span class='line'>      MessageBox(NULL,CString(lpTemp),CString("提示"),MB_OK);
</span><span class='line'>      return 0;
</span><span class='line'>  }
</span><span class='line'>
</span><span class='line'>  resPrepare = waveInAddBuffer( m_hWaveIn, &m_pWaveHdr, sizeof(WAVEHDR) );
</span><span class='line'>  if ( resPrepare != MMSYSERR_NOERROR) 
</span><span class='line'>  {
</span><span class='line'>      sprintf(lpTemp, "不能开辟录音用缓冲，Error_Code = 0x%03X", resPrepare );
</span><span class='line'>      MessageBox(NULL,CString(lpTemp),CString("提示"),MB_OK);
</span><span class='line'>      return 0;
</span><span class='line'>  }
</span><span class='line'> 
</span><span class='line'>  if (! waveInStart(m_hWaveIn) ) 
</span><span class='line'>  {
</span><span class='line'>      MessageBox(NULL,CString("开始录音"),CString("提示"),MB_OK);
</span><span class='line'>  }
</span><span class='line'>  else 
</span><span class='line'>  {
</span><span class='line'>      MessageBox(NULL,CString("开始录音失败"),CString("提示"),MB_OK);
</span><span class='line'>      return 0;
</span><span class='line'>  }
</span><span class='line'>  Sleep(30000);
</span><span class='line'>
</span><span class='line'>  MMTIME mmt;
</span><span class='line'>  mmt.wType = TIME_BYTES;
</span><span class='line'>  sprintf( lpTemp, "sizeof(MMTIME) = %d, sizeof(UINT) = %d", sizeof(MMTIME), sizeof(UINT) );
</span><span class='line'>  MessageBox(NULL,CString(lpTemp),CString("提示"),MB_OK);
</span><span class='line'>
</span><span class='line'>  if (! waveInGetPosition(m_hWaveIn, &mmt, sizeof(MMTIME)) )
</span><span class='line'>  {
</span><span class='line'>      MessageBox(NULL,CString("取得现在音频位置"),CString("提示"),MB_OK);
</span><span class='line'>  }
</span><span class='line'>  else 
</span><span class='line'>  {
</span><span class='line'>      MessageBox(NULL,CString("不能取得音频长度"),CString("提示"),MB_OK);
</span><span class='line'>      return 0;
</span><span class='line'>  }
</span><span class='line'>
</span><span class='line'>  if (mmt.wType != TIME_BYTES) 
</span><span class='line'>  {
</span><span class='line'>      MessageBox(NULL,CString("指定的 TIME_BYTES 格式音频长度不支持"),CString("提示"),MB_OK);
</span><span class='line'>      return 0;
</span><span class='line'>  }
</span><span class='line'>
</span><span class='line'>  if (! waveInStop(m_hWaveIn) ) 
</span><span class='line'>  {
</span><span class='line'>      MessageBox(NULL,CString("停止录音"),CString("提示"),MB_OK);
</span><span class='line'>  }
</span><span class='line'>  else  
</span><span class='line'>  {
</span><span class='line'>      MessageBox(NULL,CString("停止录音失败"),CString("提示"),MB_OK);
</span><span class='line'>  }
</span><span class='line'>  
</span><span class='line'>  if ( waveInReset(m_hWaveIn) ) 
</span><span class='line'>  {
</span><span class='line'>      MessageBox(NULL,CString("重置内存区失败"),CString("提示"),MB_OK);
</span><span class='line'>      return 0;
</span><span class='line'>  }
</span><span class='line'>
</span><span class='line'>  m_pWaveHdr.dwBytesRecorded = mmt.u.cb;
</span><span class='line'>  DWORD NumToWrite=0;
</span><span class='line'>  DWORD dwNumber = 0;
</span><span class='line'>  HANDLE FileHandle = CreateFile( CString("myTest.wav"), GENERIC_WRITE, 
</span><span class='line'>      FILE_SHARE_READ, NULL, CREATE_ALWAYS, FILE_ATTRIBUTE_NORMAL, NULL);
</span><span class='line'>
</span><span class='line'>  // memset(m_pWaveHdr.lpData, 0, datasize);
</span><span class='line'>  dwNumber = FCC("RIFF");
</span><span class='line'>  WriteFile(FileHandle, &dwNumber, 4, &NumToWrite, NULL);
</span><span class='line'>  dwNumber = m_pWaveHdr.dwBytesRecorded + 18 + 20;
</span><span class='line'>  WriteFile(FileHandle, &dwNumber, 4, &NumToWrite, NULL);
</span><span class='line'>  dwNumber = FCC("WAVE");
</span><span class='line'>  WriteFile(FileHandle, &dwNumber, 4, &NumToWrite, NULL);
</span><span class='line'>  dwNumber = FCC("fmt ");
</span><span class='line'>  WriteFile(FileHandle, &dwNumber, 4, &NumToWrite, NULL);
</span><span class='line'>  dwNumber = 18L;
</span><span class='line'>  WriteFile(FileHandle, &dwNumber, 4, &NumToWrite, NULL);
</span><span class='line'>  WriteFile(FileHandle, &waveformat, sizeof(WAVEFORMATEX), &NumToWrite, NULL);
</span><span class='line'>  dwNumber = FCC("data");
</span><span class='line'>  WriteFile(FileHandle, &dwNumber, 4, &NumToWrite, NULL);
</span><span class='line'>  dwNumber = m_pWaveHdr.dwBytesRecorded;
</span><span class='line'>  WriteFile(FileHandle, &dwNumber, 4, &NumToWrite, NULL);
</span><span class='line'>  WriteFile(FileHandle, m_pWaveHdr.lpData, m_pWaveHdr.dwBytesRecorded, &NumToWrite, NULL);
</span><span class='line'>  SetEndOfFile(FileHandle);
</span><span class='line'>  CloseHandle( FileHandle );  
</span><span class='line'>  FileHandle = INVALID_HANDLE_VALUE; // 收尾关闭句柄
</span><span class='line'>  MessageBox(NULL,CString("应该已生成 myTest.wav 文件"),CString("提示"),MB_OK);
</span><span class='line'>
</span><span class='line'>  if ( waveInUnprepareHeader(m_hWaveIn, &m_pWaveHdr, sizeof(WAVEHDR)) ) 
</span><span class='line'>  {
</span><span class='line'>      MessageBox(NULL,CString("Un_Prepare Header 失败"),CString("提示"),MB_OK);
</span><span class='line'>  }
</span><span class='line'>  else 
</span><span class='line'>  {
</span><span class='line'>      MessageBox(NULL,CString("Un_Prepare Header 成功"),CString("提示"),MB_OK);
</span><span class='line'>      return 0;
</span><span class='line'>  }
</span><span class='line'>
</span><span class='line'>  if ( GlobalFree(GlobalHandle( m_pWaveHdr.lpData )) ) 
</span><span class='line'>  {
</span><span class='line'>      MessageBox(NULL,CString("Global Free 失败"),CString("提示"),MB_OK);
</span><span class='line'>  }
</span><span class='line'>  else 
</span><span class='line'>  {
</span><span class='line'>      MessageBox(NULL,CString("Global Free 成功"),CString("提示"),MB_OK);
</span><span class='line'>      return 0;
</span><span class='line'>  }
</span><span class='line'>
</span><span class='line'>  if (res == MMSYSERR_NOERROR ) // 关闭录音设备
</span><span class='line'>  {
</span><span class='line'>      if (waveInClose(m_hWaveIn)==MMSYSERR_NOERROR)
</span><span class='line'>      {
</span><span class='line'>          MessageBox(NULL,CString("正常关闭录音设备"),CString("提示"),MB_OK);
</span><span class='line'>      }
</span><span class='line'>      else
</span><span class='line'>      {
</span><span class='line'>          MessageBox(NULL,CString("非正常关闭录音设备"),CString("提示"),MB_OK);
</span><span class='line'>          return 0;
</span><span class='line'>      }
</span><span class='line'>  }
</span><span class='line'>
</span><span class='line'>  return 0;
</span><span class='line'>}
</span><span class='line'>// ******************* End of File ************************</span></code></pre></td></tr></table></div></figure>




<p>这里提供的代码有点杂乱，现已整理成一个小的接口，并提供了一个简单的示例，放在GitHub上：https://github.com/ibillxia/Demo/tree/master/DemoSpeechRecord</p>




<p>参考：</br>
[1]MSDN: http://msdn.microsoft.com/en-us/library/windows/desktop/dd743586(v=vs.85).aspx</br>
[2]基于API的录音机程序: http://www.vckbase.com/index.php/wv/664
</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[语音信号处理之时域分析-音高追踪及其Python实现]]></title>
    <link href="http://ibillxia.github.com/blog/2013/05/29/audio-signal-processing-time-domain-pitch-tracking-and-python-realization/"/>
    <updated>2013-05-29T21:37:00+08:00</updated>
    <id>http://ibillxia.github.com/blog/2013/05/29/audio-signal-processing-time-domain-pitch-tracking-and-python-realization</id>
    <content type="html"><![CDATA[<h2>1.概述</h2>


<p>在<a href="http://ibillxia.github.io/blog/2013/05/16/audio-signal-processing-time-domain-pitch-python-realization/">音高及其Python实现</a>一文
中，我们使用了简单的“观察法”来计算音高，这并不太难，但这并不有好而且费时费力，那么我们就想，如何通过分析和计算，使用算法来自动计算音高呢？
用算法让计算机自动抓取音高的过程，称为<b>音高追踪</b>(Pitch Tracking)。所得到的音高信息有如下一些应用：</br>
·旋律识别(Melody Recognition)：或称为“哼唱选歌”，也就是如何由使用者的哼唱，找出音乐资料库中间对应的歌。</br>
·汉语声调识别(Tone Recognition)：辨识使用者讲一句话时，每一个字的声调（一声、二声、三声、四声等）。</br>
·语音合成韵律分析(Prosody Analysis)中的音高分析：如何在合成语音时，使用最自然的音高曲线。</br>
·语音评分中的音调评分(Intonation Assessment)：如何评估使用者说话的语音，其音高曲线是否标准。</br>
·语音识别(Speech Recognition)：我们可以使用语句的音高来提高语音辨识的正确率。</br>
总而言之，音高追踪是语音信号处理中最基本也最重要的一个环节之一。
</p>




<h2>2.音高追踪的基本流程</h2>


<p>音高追踪的基本流程如下：</br>
(1)将整段音讯讯号切成音框（Frames），相邻音框之间可以重叠。</br>
(2)算出每个音框所对应的音高。</br>
(3)排除不稳定的音高值。（可由音量来筛选，或由音高值的范围来过滤。）</br>
(4)对整段音高进行平滑化，通常是使用「中位数滤波器」（Median Filters）。</br>
</p>




<!--more-->




<p>在切音框的过程中，我们允许左右音框的重叠，因此，我们定义「音框率」（Frame Rate）是每秒钟所出现的音框个数，如果取样频率是11025，音框长度是256 点，
重叠点数是84，那么音框率就是11025/(256-84) = 64，换句话说，我们的电脑要能够每秒钟处理64 个音框，才能达到实时处理的要求。如下图所示：
<center><img src="http://ibillxia.github.com/images/2013/IMAG2013052901.png"></center>
</p>




<p>我们让音框重叠的目地，只是希望相邻音框之间的变化不会太大，使抓出来的音高曲线更具有连续性。但是在实际应用时，音框的重叠也不能太大，
否则会造成计算量的过大。一般有以下考虑：</br>
·音框长度至少必须包含2 个基本周期以上，才能显示语音的特性。已知人声的音高范围大约在50 Hz 至1000 Hz 之间，因此对于一个的取样频率，我们就可以计算出
音框长度的最小值。例如，若取样频率fs = 8000 Hz，那么当音高f = 50 Hz（例如男低音的歌声）时，每个基本周期的点数是fs/f = 8000/50 = 160，因此音框必须
至少是320 点；若音高是1000 Hz（例如女高音的歌声）时，每个基本周期的点数是8000/1000 = 8，因此音框必须至少是16 点。</br>
·音框长度也不能太大，太长的音框无法抓到音讯的特性随时间而变化的细微现象，同时计算量也会变大。</br>
·音框之间的重叠完全是看计算机的运算能力来决定，若重叠多，音框率就会变大，计算量就跟着变大。若重叠少（甚至可以不重叠或跳点），音框率就会变小，
计算量也跟着变小。</p>




<h2>3.音高追踪算法</h2>


<h4>3.1概述</h4>


<p>由一个音框计算出音高的方法很多，可以分为时域和频域两大类：</br>
<b>时域（Time Domain）</b></br>
·ACF: Autocorrelation function，自相关函数</br>
·AMDF: Average magnitude difference function，平均强度差分函数</br>
·SIFT: Simple inverse filter tracking</br>
<b>频域（Frequency Domain）</b></br>
·Harmonic product spectrum method</br>
·Cepstrum method</p>




<h4>3.2 ACF自相关函数</h4>


<p>首先，我们来看看ACF(Auto-Correlation Function，自相关函数)的概念。要计算音高，就得找出波形中的周期性，自相关函数的目的就是估算语音信号当前
帧与它的下一帧的相似性，其定义如下：
<center>$acf(\tau) = \sum_{i=0}^{n-1-\tau}s(i)s(i+\tau)$</center>
其中$\tau$是一个延迟的时间间隔。在某个区间使得$acf(\tau)$取得最大值的那个$\tau$值就选为pitch的起止点，如下图所示：
<center><img src="http://ibillxia.github.com/images/2013/IMAG2013052902.png"></center>
也就是说，我们将原始语音信号与其平移延迟信号的重叠（时间上重叠）部分进行内积运算，从而得到ACF。下面看一个具体的实例，其代码如下：

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>import wave
</span><span class='line'>import numpy as np
</span><span class='line'>import pylab as pl
</span><span class='line'>
</span><span class='line'>def ACF(frame):
</span><span class='line'>    flen = len(frame)
</span><span class='line'>    acf = np.zeros(flen)
</span><span class='line'>    for i in range(flen):
</span><span class='line'>        acf[i] = np.sum(frame[i:flen]*frame[0:flen-i])
</span><span class='line'>    return acf
</span><span class='line'>
</span><span class='line'># ============ test the algorithm =============
</span><span class='line'># read wave file and get parameters.
</span><span class='line'>fw = wave.open('a.wav','rb')
</span><span class='line'>params = fw.getparams()
</span><span class='line'>print(params)
</span><span class='line'>nchannels, sampwidth, framerate, nframes = params[:4]
</span><span class='line'>strData = fw.readframes(nframes)
</span><span class='line'>waveData = np.fromstring(strData, dtype=np.int16)
</span><span class='line'>waveData = waveData*1.0/max(abs(waveData))  # normalization
</span><span class='line'>fw.close()
</span><span class='line'>
</span><span class='line'># plot the wave
</span><span class='line'>time = np.arange(0, len(waveData)) * (1.0 / framerate)
</span><span class='line'>
</span><span class='line'>frameSize = 512
</span><span class='line'>idx1 = 10000
</span><span class='line'>idx2 = idx1+frameSize
</span><span class='line'>index1 = idx1*1.0 / framerate
</span><span class='line'>index2 = idx2*1.0 / framerate
</span><span class='line'>acf = ACF(waveData[idx1:idx2])
</span><span class='line'>#acf[0:10] = -acf[0]
</span><span class='line'>#acfmax = np.argmax(acf)
</span><span class='line'>#print(acfmax)
</span><span class='line'>#print(framerate*1.0/acfmax)
</span><span class='line'>
</span><span class='line'>pl.subplot(311)
</span><span class='line'>pl.plot(time, waveData)
</span><span class='line'>pl.plot([index1,index1],[-1,1],'r')
</span><span class='line'>pl.plot([index2,index2],[-1,1],'r')
</span><span class='line'>pl.xlabel("time (seconds)")
</span><span class='line'>pl.ylabel("Amplitude")
</span><span class='line'>
</span><span class='line'>pl.subplot(312)
</span><span class='line'>pl.plot(np.arange(frameSize),waveData[idx1:idx2],'r')
</span><span class='line'>pl.xlabel("index in 1 frame")
</span><span class='line'>pl.ylabel("Amplitude")
</span><span class='line'>
</span><span class='line'>pl.subplot(313)
</span><span class='line'>pl.plot(np.arange(frameSize),acf,'g')
</span><span class='line'>pl.xlabel("index in 1 frame")
</span><span class='line'>pl.ylabel("ACF")
</span><span class='line'>pl.show()</span></code></pre></td></tr></table></div></figure>

程序运行结果如下图所示：
<center><img src="http://ibillxia.github.com/images/2013/IMAG2013052903.png"></center>
</p>




<p>很显然，ACF的最大值出现在第一点，这一点作为起点在任何情况下都是已知的，我们需要知道是第二个波峰。我们可以将开始的一些点的ACF值设为负无穷（这里我
设为-acf[0]），这样可以找到第二个波峰的index为110（这一点称为pitch point，简称pp），那么对应的pitch为framerate/110 = 16000/110 = 145.455Hz.这个过程取消
程序中第32行起的4行注释即可。这样，我们就初步自动计算出了pitch了。</p>




<p>但是，细心的读者会发现，这里还有一个问题，那就是ACF曲线中前多少个点应该被置为负无穷？为简单起见，设人的pitch的范围为[40,1000](Hz)，那么pp的值应满足为：
$40 < \frac{fs}{pp} < 1000$，从而得到pp的范围：$\frac{fs}{1000} < pp < \frac{fs}{40}$。这样可以部分解决问题，对于某些情况可能结果并不一定正确。</p>

<p>另外还有一些对ACF的改进。一个主要的改进原因是，当$\tau$值变大是，两端信号的重叠部分逐渐变小，这样计算出来的ACF当然越来越小。一种改进是增加一个权值：
<center>$acf(\tau) = \sum_{i=0}^{n-1-\tau} \frac{s(i)s(i+\tau)}{n-\tau}$.</center>
这种方法虽然解决了上面提到的问题，但又引入了一个新的问题，那就是，在$\tau$值较大时，计算出来的acf和pitch的差异可能很大，也即出现了不稳定。另一种改进是，
将$\tau$限制在半帧内，也即：
<center>$acf(\tau) = \sum_{i=0}^{n/2}s(i)s(i+\tau)$.</center>
但这样得到的acf只有一帧的一半，对于音高较低的信号就不利了，这时我们就得增大帧的长度，于是计算量也相应的增加了。</p>

<h4>3.3 NSDF</h4>
<p>ACF的范围是未知的，NSDF(normalized squared difference function)将ACF规整到[-1,1]之间，其定义的表达式如下：
<center>$nsdf(\tau) = \frac{2\sum s(i)s(i+\tau)}{\sum s^{2}(i) + \sum s^{2}(i+\tau)}$.</center>
</p>

<h4>3.4 AMDF</h4>
<p>AMDF (average magnitude difference function) 的定义如下：
<center>$amdf(\tau) = \sum_{i=0}^{n-1-\tau}|s(i)-s(i+\tau)|$.</center>
与ACF相反，这里用距离而不是相似度来计算，所以这里选取pitch point(简称pp)的标准是选最小值对应的index(实际代码中，为了与ACF进行统一，我对AMDF取了相反数)。
相应的，也有一些对AMDF这个函数的修正，如加权值、只是用前半帧等，另外还可以将AMDF与ACF结合，将ACF除以AMDF，得到的结果可以更容易找到pitch point。
</p>

<h4>3.5 Pitch Tracking</h4>
<p>能够正确计算pitch了，我们就可以对一段时序的信号进行pitch tracking了。</p>

<p>PS：另外，还有一些频域的音高追踪方法，将在后续文章中介绍。</p>

<h2>4.参考资料</h2>
<p>[1]Audio Signal Processing and Recognition, Chap 7: http://neural.cs.nthu.edu.tw/jang/books/audiosignalprocessing/index.asp
</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[大白鼠听人话]]></title>
    <link href="http://ibillxia.github.com/blog/2013/05/24/cctv-news-rat-understand-what-human-says/"/>
    <updated>2013-05-24T22:59:00+08:00</updated>
    <id>http://ibillxia.github.com/blog/2013/05/24/cctv-news-rat-understand-what-human-says</id>
    <content type="html"><![CDATA[<p>最近一直忙着准备给媒体展示的音控大鼠机器人一不小心上了CCTV了，虽然自己感觉没什么了不起的，也不知道网络上是什么评论。
但既然上了CCTV，还是发博纪念一下吧</p>




<p>央视新闻视频链接：<a href="http://tv.cntv.cn/vodplay/e58c8785e00a4ead9b83dfae5b53f12a/860010-1102010100">浙江杭州最新科研成果：大白鼠听人话</a>
真没想到自己居然正面出境这么长时间。</p>




<p>杭州日报的记者写的新闻还挺生动的：<a href="http://hzdaily.hangzhou.com.cn/hzrb/html/2013-05/24/content_1501396.htm">“嫁接”了机器视觉的大白鼠在沙盘迷宫中寻觅阿汤哥的照片</a></p>




<p>PS：感谢CCTV，感谢杭州日报，感谢ZJU，感谢CCNT，感谢各位老师和同学，感谢生仪的给大鼠做开颅手术的两位mm，感谢各位在微博帮忙宣传和转发的各位同学！</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[语音信号处理之时域分析-端点检测及Python实现]]></title>
    <link href="http://ibillxia.github.com/blog/2013/05/22/audio-signal-processing-time-domain-Voice-Activity-Detection/"/>
    <updated>2013-05-22T22:22:00+08:00</updated>
    <id>http://ibillxia.github.com/blog/2013/05/22/audio-signal-processing-time-domain-Voice-Activity-Detection</id>
    <content type="html"><![CDATA[<h2>端点检测</h2>


<p>端点检测（End-Point Detection，EPD）的目标是要决定信号的语音开始和结束的位置，所以又可以称为Speech Detection或Voice Activity Detection（VAD）。
端点检测在语音预处理中扮演着一个非常重要的角色。</p>




<p>常见的端点检测方法大致可以分为如下两类：</br>
（1）时域（Time Domain）的方法：计算量比较小，因此比较容易移植到计算能力较差的嵌入式平台</br>
（a）音量：只使用音量来进行端检，是最简单的方法，但是容易对清音造成误判。另外，不同的音量计算方法得到的结果也不尽相同，至于那种方法更好也没有定论。</br>
（b）音量和过零率：以音量为主，过零率为辅，可以对清音进行较精密的检测。</br>
（2）频域（Frequency Domain）的方法：计算量相对较大。</br>
（a）频谱的变化性（Variance）：有声音的频谱变化较规律，可以作为一个判断标准。</br>
（b）频谱的Entropy：有规律的频谱的Entropy一般较小，这也可以作为一个端检的判断标准。
</p>




<p>下面我们分别从这两个方面来探讨端检的具体方法和过程。</p>




<!--more-->




<h2>时域的端检方法</h2>


<p>时域的端检方法分为只用音量的方法和同时使用音量和过零率的方法。只使用音量的方法最简单计算量也最小，我们只需要设定一个音量阈值，任何音量小于该阈值的帧
被认为是静音（silence）。这种方法的关键在于如何选取这个阈值，一种常用的方法是使用一些带标签的数据来训练得到一个阈值，使得误差最小。</p>




<p>下面我们来看看最简单的、不需要训练的方法，其代码如下：

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>import wave
</span><span class='line'>import numpy as np
</span><span class='line'>import matplotlib.pyplot as plt
</span><span class='line'>import Volume as vp
</span><span class='line'>
</span><span class='line'>def findIndex(vol,thres):
</span><span class='line'>    l = len(vol)
</span><span class='line'>    ii = 0
</span><span class='line'>    index = np.zeros(4,dtype=np.int16)
</span><span class='line'>    for i in range(l-1):
</span><span class='line'>        if((vol[i]-thres)*(vol[i+1]-thres)&lt;0):
</span><span class='line'>            index[ii]=i
</span><span class='line'>            ii = ii+1
</span><span class='line'>    return index[[0,-1]]
</span><span class='line'>
</span><span class='line'>fw = wave.open('sunday.wav','r')
</span><span class='line'>params = fw.getparams()
</span><span class='line'>nchannels, sampwidth, framerate, nframes = params[:4]
</span><span class='line'>strData = fw.readframes(nframes)
</span><span class='line'>waveData = np.fromstring(strData, dtype=np.int16)
</span><span class='line'>waveData = waveData*1.0/max(abs(waveData))  # normalization
</span><span class='line'>fw.close()
</span><span class='line'>
</span><span class='line'>frameSize = 256
</span><span class='line'>overLap = 128
</span><span class='line'>vol = vp.calVolume(waveData,frameSize,overLap)
</span><span class='line'>threshold1 = max(vol)*0.10
</span><span class='line'>threshold2 = min(vol)*10.0
</span><span class='line'>threshold3 = max(vol)*0.05+min(vol)*5.0
</span><span class='line'>
</span><span class='line'>time = np.arange(0,nframes) * (1.0/framerate)
</span><span class='line'>frame = np.arange(0,len(vol)) * (nframes*1.0/len(vol)/framerate)
</span><span class='line'>index1 = findIndex(vol,threshold1)*(nframes*1.0/len(vol)/framerate)
</span><span class='line'>index2 = findIndex(vol,threshold2)*(nframes*1.0/len(vol)/framerate)
</span><span class='line'>index3 = findIndex(vol,threshold3)*(nframes*1.0/len(vol)/framerate)
</span><span class='line'>end = nframes * (1.0/framerate)
</span><span class='line'>
</span><span class='line'>plt.subplot(211)
</span><span class='line'>plt.plot(time,waveData,color="black")
</span><span class='line'>plt.plot([index1,index1],[-1,1],'-r')
</span><span class='line'>plt.plot([index2,index2],[-1,1],'-g')
</span><span class='line'>plt.plot([index3,index3],[-1,1],'-b')
</span><span class='line'>plt.ylabel('Amplitude')
</span><span class='line'>
</span><span class='line'>plt.subplot(212)
</span><span class='line'>plt.plot(frame,vol,color="black")
</span><span class='line'>plt.plot([0,end],[threshold1,threshold1],'-r', label="threshold 1")
</span><span class='line'>plt.plot([0,end],[threshold2,threshold2],'-g', label="threshold 2")
</span><span class='line'>plt.plot([0,end],[threshold3,threshold3],'-b', label="threshold 3")
</span><span class='line'>plt.legend()
</span><span class='line'>plt.ylabel('Volume(absSum)')
</span><span class='line'>plt.xlabel('time(seconds)')
</span><span class='line'>plt.show()</span></code></pre></td></tr></table></div></figure>

其中计算音量的函数calVolume参见<a href="http://ibillxia.github.io/blog/2013/05/15/audio-signal-process-time-domain-volume-python-realization/">
音量及其Python实现</a>一文。程序的运行结果如下图：
<center><img src="http://ibillxia.github.com/images/2013/IMAG2013052201.png"></center>
</p>




<p>这里采用了三种设置阈值的方法，但这几种设置方法对所有的输入都是相同的，对于一些特定的语音数据可能得不到很好的结果，比如杂音较强、清音较多或音量
变化较大等语音信号，此时单一阈值的方法的效果就不太好了，虽然我们可以通过增加帧与帧之间的重叠部分，但相对而言计算量会比较大。下面我们利用一些更多的
特征来进行端点加测，例如使用过零率等信息，其过程如下：</br>
（1）以较高音量阈值($\tau _{u}$)为标准，找到初步的端点；</br>
（2）将端点前后延伸到低音量阈值($\tau _{l}$)处；</br>
（3）再将端点前后延伸到过零率阈值($\tau _{zc}$)处，以包含语音中清音的部分。</br>
这种方法需要确定三个阈值($\tau _{u}$,$\tau _{l}$,$\tau _{zc}$)，可以用各种搜寻方法来调整这三个参数。其示意图(参考[1])如下：
<center><img src="http://ibillxia.github.com/images/2013/IMAG2013052202.png"></center>
我们在同一个图中绘制出音量和过零率的阈值图如下：
<center><img src="http://ibillxia.github.com/images/2013/IMAG2013052203.png"></center>
可以看到我们可以通过过零率的阈值来把错分的清音加入到语音部分来。上图使用到的阈值还是和音量的阈值选取方法相同，比较简单直接。
</p>




<p>另外，我们还可以连续对波形进行微分，再计算音量，这样就可以凸显清音的部分，从而将其正确划分出来，详见参考[1]。</p>




<h2>频域的端检方法</h2>


<p>有声音的信号在频谱上会有重复的谐波结构，因此我们也可以使用频谱的变化性（Variation）或Entropy来进行端点检测，可以参见如下链接：
http://neural.cs.nthu.edu.tw/jang/books/audiosignalprocessing/paper/endPointDetection/</p>




<p>总之，端点检测是语音预处理的重头戏，其实现方法也是五花八门，本文只给出了最简单最原始也最好理解的几种方法，这些方法要真正做到实用，还需要针对一些
特殊的情况在做一些精细的设置和处理，但对于一般的应用场景应该还是基本够用的。</p>




<h2>参考（References）</h2>


<p>
[1]EPD in Time Domain: http://neural.cs.nthu.edu.tw/jang/books/audiosignalprocessing/epdTimeDomain.asp?title=6-2%20EPD%20in%20Time%20Domain</br>
[2]EPD in Frequency Domain: http://neural.cs.nthu.edu.tw/jang/books/audiosignalprocessing/epdFreqDomain.asp?title=6-3%20EPD%20in%20Frequency%20Domain
</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[语音信号处理之时域分析-音色及其Python实现]]></title>
    <link href="http://ibillxia.github.com/blog/2013/05/18/audio-signal-processing-time-domain-timbre-python-realization/"/>
    <updated>2013-05-18T21:57:00+08:00</updated>
    <id>http://ibillxia.github.com/blog/2013/05/18/audio-signal-processing-time-domain-timbre-python-realization</id>
    <content type="html"><![CDATA[<h2>音色（Timbre）</h2>


<p>音色是一个很模糊的概念，它泛指语音的内容，例如“天书”这两个字的发音，虽然都是一声（即他们的音高应该是相同或接近的），
但由于音色不同，我们可以分辨这两个音。直觉而言，音色的不同，意味着基本波形的不同，因此我们可以用基本周期的波形来代表音色。
</p>




<p>若要从基本周期的波形来直接分析音色是一件很困难的事情。通常我们的做法是将每一个帧进行频谱分析（Spectral Analysis），算出一个
帧如何分解为不同频率的分量，然后才能进行对比或分析。在频谱分析中，最常用的方法就是快速傅里叶变换（Fast Fourier Transform，FFT），
这是一个相当常用的方法，可以讲在时域（Time Domain）的信号转换成频域（Frequency Domain）的信号，并进而知道每个频率的信号强度。</p>




<p>语谱图（Spectrogram）就是语音频谱图，一般是通过处理接收的时域信号得到频谱图，因此只要有足够时间长度的时域信号就可以(时间长度
为保证频率分辨率)。专业点讲，语谱图就是频谱分析视图，如果针对语音数据的话，叫语谱图。语谱图的横坐标是时间，纵坐标是频率，坐标点
值为语音数据能量，因而语谱图很好的表达了语音的音色随时间变化的趋势。有些经验丰富的人能够通过看语谱图而知道对应的语音信号的内容，
这种技术成为Spectrogram Reading。</p>




<!--more-->




<h2>Python绘制语谱图</h2>


<p>如果是用Matlab，绘制语谱图并不难，网上资料也一堆一堆的。但是，如果要想用Python来绘制呢？网上相关资料很少很少，万幸中找到了参考[4]，
但是，[4]中提供的程序是不能运行的，还需要安装几个库，特别是Audiolab这个，折腾了我好半天，最终安装了，但运行时发现这个audiolab根本无法
import进来，因为ms与numpy的版本有冲突，出现了什么“numpy.dtype does not appear to be the correct type object”，弄了好半天也没有解决，
后来才发现其实不需要audiolab也可以的，因为其实audiolab只是读取不同格式（扩展名）的语音文件的一个lib而已，并不涉及到绘制语谱图的东西。</p>




<p>
闲话少说了，上代码吧，其实看看这代码也挺简单的，就调一个matplotlib.pyplot.specgram()就可以了。

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>import wave
</span><span class='line'>import numpy as np
</span><span class='line'>import matplotlib.pyplot as plt
</span><span class='line'>
</span><span class='line'>fw = wave.open('aeiou.wav','r')
</span><span class='line'>soundInfo = fw.readframes(-1)
</span><span class='line'>soundInfo = np.fromstring(soundInfo,np.int16)
</span><span class='line'>f = fw.getframerate()
</span><span class='line'>fw.close()
</span><span class='line'>
</span><span class='line'>plt.subplot(211)
</span><span class='line'>plt.plot(soundInfo)
</span><span class='line'>plt.ylabel('Amplitude')
</span><span class='line'>plt.title('Wave from and spectrogram of aeiou.wav')
</span><span class='line'>
</span><span class='line'>plt.subplot(212)
</span><span class='line'>plt.specgram(soundInfo,Fs = f, scale_by_freq = True, sides = 'default')
</span><span class='line'>plt.ylabel('Frequency')
</span><span class='line'>plt.xlabel('time(seconds)')
</span><span class='line'>plt.show()</span></code></pre></td></tr></table></div></figure>

</p>




<p>程序运行的效果如下图：
<center><img src="http://ibillxia.github.com/images/2013/IMAG2013051801.png"></center>
虽然程序简单，但还有一些小bug，比如subplot(212)的xlabel和ylabel无法显示，这个问题暂时还没有解决。（更新：这个问题已解决，把mpp.show()放到
最后一行就可以了，顺便图也更新了）</p>




<p>另外，就是关于这个语谱图具体是如何绘制的，这一点涉及到FFT和短时能量的计算，短时能量在<a href="">前文中</a>
已经讲过了，这里不再赘述。关于FFT将在后续文章中讨论。</p>




<h2>参考（References）</h2>


<p>
[1]Timbre (音色): http://neural.cs.nthu.edu.tw/jang/books/audiosignalprocessing/basicFeatureTimber.asp?title=5-5</br>
[2]Wiki - 音色: http://zh.wikipedia.org/wiki/音色</br>
[3]语谱图： http://blog.csdn.net/wuxiaoer717/article/details/6941339</br>
[4]How to plot spectrogram with Python：http://jaganadhg.freeflux.net/blog/archive/2009/07/23/how-to-plot-spectrogram-with-python.html
</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[语音信号处理之时域分析-音高及其Python实现]]></title>
    <link href="http://ibillxia.github.com/blog/2013/05/16/audio-signal-processing-time-domain-pitch-python-realization/"/>
    <updated>2013-05-16T23:10:00+08:00</updated>
    <id>http://ibillxia.github.com/blog/2013/05/16/audio-signal-processing-time-domain-pitch-python-realization</id>
    <content type="html"><![CDATA[<h2>音高（Pitch）</h2>


<p>概念：音高（Pitch）是语音信号的一个很重要的特征，直觉上而言它表示声音频率的高低，这个频率是指基本频率（基频），也即基本周期的倒数。
若直接观察语音的波形，只要语音信号稳定，我们可以很容易的看出基本周期的存在。例如我们取一个包含256个采样点的帧，单独绘制波形图，就可以明显的
看到它的基本周期。如下图所示：
<center><img src="http://ibillxia.github.com/images/2013/IMAG2013051601.png"></center>
其中最上面的波形为|a|的发音，中间的为上图中红色双竖线（位于语音区）所对应的帧的具体波形，而最下面的是上图中绿色双竖线（位于静音区）所
对应的帧的具体波形。很容易看到中间的波形具有明显的周期性。
</p>


<!--more-->


<p>其代码如下：

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>import wave
</span><span class='line'>import numpy as np
</span><span class='line'>import pylab as pl
</span><span class='line'>
</span><span class='line'># ============ test the algorithm =============
</span><span class='line'># read wave file and get parameters.
</span><span class='line'>fw = wave.open('a.wav','rb')
</span><span class='line'>params = fw.getparams()
</span><span class='line'>print(params)
</span><span class='line'>nchannels, sampwidth, framerate, nframes = params[:4]
</span><span class='line'>strData = fw.readframes(nframes)
</span><span class='line'>waveData = np.fromstring(strData, dtype=np.int16)
</span><span class='line'>waveData = waveData*1.0/max(abs(waveData))  # normalization
</span><span class='line'>fw.close()
</span><span class='line'>
</span><span class='line'># plot the wave
</span><span class='line'>time = np.arange(0, len(waveData)) * (1.0 / framerate)
</span><span class='line'>
</span><span class='line'>index1 = 10000.0 / framerate
</span><span class='line'>index2 = 10512.0 / framerate
</span><span class='line'>index3 = 15000.0 / framerate
</span><span class='line'>index4 = 15512.0 / framerate
</span><span class='line'>
</span><span class='line'>pl.subplot(311)
</span><span class='line'>pl.plot(time, waveData)
</span><span class='line'>pl.plot([index1,index1],[-1,1],'r')
</span><span class='line'>pl.plot([index2,index2],[-1,1],'r')
</span><span class='line'>pl.plot([index3,index3],[-1,1],'g')
</span><span class='line'>pl.plot([index4,index4],[-1,1],'g')
</span><span class='line'>pl.xlabel("time (seconds)")
</span><span class='line'>pl.ylabel("Amplitude")
</span><span class='line'>
</span><span class='line'>pl.subplot(312)
</span><span class='line'>pl.plot(np.arange(512),waveData[10000:10512],'r')
</span><span class='line'>pl.plot([59,59],[-1,1],'b')
</span><span class='line'>pl.plot([169,169],[-1,1],'b')
</span><span class='line'>print(1/( (169-59)*1.0/framerate ))
</span><span class='line'>pl.xlabel("index in 1 frame")
</span><span class='line'>pl.ylabel("Amplitude")
</span><span class='line'>
</span><span class='line'>pl.subplot(313)
</span><span class='line'>pl.plot(np.arange(512),waveData[15000:15512],'g')
</span><span class='line'>pl.xlabel("index in 1 frame")
</span><span class='line'>pl.ylabel("Amplitude")
</span><span class='line'>pl.show()</span></code></pre></td></tr></table></div></figure>

</p>




<p>根据参考[1]，可以通过观察一帧的波形图来计算基音频率（感觉这种方法有点奇葩，不过很直观。例如这里的基频为：1/( (169-59)*1.0/framerate )=145.45Hz），
然后还可以计算半音（semitone，可以参见[2]），进而得到pitch与semitone的关系。[1]中还提到了钢琴的半音差，DS表示完全看不懂啊，有木有！！！</p>




<p>参考[2]中还简单介绍了如何改变音高、扩展音域，以及如何改变乐器的振动的弦的音高（通过改变弦长、张力、密度等），感兴趣的可以看看。</p>




<p>另外，由于生理结构的差异，男女性的音高范围不尽相同，一般而言：</br>
·男性的音高范围是35~72半音，对应的频率范围是62~523Hz；</br>
·女性的音高范围是45~83半音，对应的频率范围是110~1000Hz。</br>
然而，我们分辨男女的声音并不是只根据音高，还要根据音色（也即共振峰，下一篇文章中将详细介绍）。
</p>




<p>关于音高的计算，目前有很多种算法，具体将会在后续文章中详细介绍。</p>




<h2>参考（References）</h2>


<p>
[1]Pitch (音高): http://neural.cs.nthu.edu.tw/jang/books/audiosignalprocessing/basicFeaturePitch.asp</br>
[2]Wiki： http://zh.wikipedia.org/wiki/音高
</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[语音信号处理之时域分析-过零率及其Python实现]]></title>
    <link href="http://ibillxia.github.com/blog/2013/05/15/audio-signal-processing-time-domain-ZeroCR-python-realization/"/>
    <updated>2013-05-15T21:44:00+08:00</updated>
    <id>http://ibillxia.github.com/blog/2013/05/15/audio-signal-processing-time-domain-ZeroCR-python-realization</id>
    <content type="html"><![CDATA[<h2>过零率（Zero Crossing Rate）</h2>


<p>概念：过零率（Zero Crossing Rate，ZCR）是指在每帧中，语音信号通过零点（从正变为负或从负变为正）的次数。
这个特征已在语音识别和音乐信息检索领域得到广泛使用，是对敲击的声音的分类的关键特征。</p>




<p>ZCR的数学形式化定义为：
<center>$zcr = \frac{1}{T-1}\sum_{t=1}^{T-1}\pi\{s_{t}s_{t-1}<0\}$.</center>
其中$s$是采样点的值，$T$为帧长，函数$\pi\{A\}$在A为真是值为1，否则为0.
</p>




<p>特性：</br>
(1).一般而言，清音（unvoiced sound）和环境噪音的ZCR都大于浊音（voiced sound）；</br>
(2).由于清音和环境噪音的ZCR大小相近，因而不能够通过ZCR来区分它们；</br>
(3).在实际当中，过零率经常与短时能量特性相结合来进行端点检测，尤其是ZCR用来检测清音的起止点；</br>
(4).有时也可以用ZCR来进行粗略的基频估算，但这是非常不可靠的，除非有后续的修正（refine）处理过程。
</p>




<!--more-->




<h2>ZCR的Python实现</h2>


<p>ZCR的Python实现如下：

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>import math
</span><span class='line'>import numpy as np
</span><span class='line'>
</span><span class='line'>def ZeroCR(waveData,frameSize,overLap):
</span><span class='line'>    wlen = len(waveData)
</span><span class='line'>    step = frameSize - overLap
</span><span class='line'>    frameNum = math.ceil(wlen/step)
</span><span class='line'>    zcr = np.zeros((frameNum,1))
</span><span class='line'>    for i in range(frameNum):
</span><span class='line'>        curFrame = waveData[np.arange(i*step,min(i*step+frameSize,wlen))]
</span><span class='line'>        #To avoid DC bias, usually we need to perform mean subtraction on each frame
</span><span class='line'>        #ref: http://neural.cs.nthu.edu.tw/jang/books/audiosignalprocessing/basicFeatureZeroCrossingRate.asp
</span><span class='line'>        curFrame = curFrame - np.mean(curFrame) # zero-justified
</span><span class='line'>        zcr[i] = sum(curFrame[0:-1]*curFrame[1::]&lt;=0)
</span><span class='line'>    return zcr</span></code></pre></td></tr></table></div></figure>

</p>




<p>对于给定语音文件aeiou.wav，利用上面的函数计算ZCR的代码如下：

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>import math
</span><span class='line'>import wave
</span><span class='line'>import numpy as np
</span><span class='line'>import pylab as pl
</span><span class='line'>
</span><span class='line'># ============ test the algorithm =============
</span><span class='line'># read wave file and get parameters.
</span><span class='line'>fw = wave.open('aeiou.wav','rb')
</span><span class='line'>params = fw.getparams()
</span><span class='line'>print(params)
</span><span class='line'>nchannels, sampwidth, framerate, nframes = params[:4]
</span><span class='line'>str_data = fw.readframes(nframes)
</span><span class='line'>wave_data = np.fromstring(str_data, dtype=np.short)
</span><span class='line'>wave_data.shape = -1, 1
</span><span class='line'>#wave_data = wave_data.T
</span><span class='line'>fw.close()
</span><span class='line'>
</span><span class='line'># calculate Zero Cross Rate
</span><span class='line'>frameSize = 256
</span><span class='line'>overLap = 0
</span><span class='line'>zcr = ZeroCR(wave_data,frameSize,overLap)
</span><span class='line'>
</span><span class='line'># plot the wave
</span><span class='line'>time = np.arange(0, len(wave_data)) * (1.0 / framerate)
</span><span class='line'>time2 = np.arange(0, len(zcr)) * (len(wave_data)/len(zcr) / framerate)
</span><span class='line'>pl.subplot(211)
</span><span class='line'>pl.plot(time, wave_data)
</span><span class='line'>pl.ylabel("Amplitude")
</span><span class='line'>pl.subplot(212)
</span><span class='line'>pl.plot(time2, zcr)
</span><span class='line'>pl.ylabel("ZCR")
</span><span class='line'>pl.xlabel("time (seconds)")
</span><span class='line'>pl.show()</span></code></pre></td></tr></table></div></figure>

</p>




<p>运行以上程序得到下图：
<center><img src="http://ibillxia.github.com/images/2013/IMAG2013051502.png"></center>
</p>




<h2>参考（References）</h2>


<p>
[1]Zero Crossing Rate (過零率): http://neural.cs.nthu.edu.tw/jang/books/audiosignalprocessing/basicFeatureZeroCrossingRate.asp?title=5-3%20Zero%20Crossing%20Rate%20(%B9L%B9s%B2v)&language=english</br>
[2]Wiki: http://zh.wikipedia.org/zh/过零率
</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[语音信号处理之时域分析-音量及其Python实现]]></title>
    <link href="http://ibillxia.github.com/blog/2013/05/15/audio-signal-process-time-domain-volume-python-realization/"/>
    <updated>2013-05-15T19:36:00+08:00</updated>
    <id>http://ibillxia.github.com/blog/2013/05/15/audio-signal-process-time-domain-volume-python-realization</id>
    <content type="html"><![CDATA[<h2>1.概述（Introduction）</h2>


<p>本系列文主要介绍语音信号时域的4个基本特征及其Python实现，这4个基本特征是：</br>
(1)音量（Volume）；</br>
(2)过零率（Zero-Crossing-Rate）；</br>
(3)音高（Pitch）；</br>
(4)音色（Timbre）。
</p>




<h2>2.音量（Volume）</h2>


<p>音量代表声音的强度，可由一个窗口或一帧内信号振幅的大小来衡量，一般有两种度量方法：</br>
（1）每个帧的振幅的绝对值的总和：
<center>$volume = \sum_{i=1}^{n}|s_{i}|$.</center>
其中$s_{i}$为第该帧的$i$个采样点，$n$为该帧总的采样点数。这种度量方法的计算量小，但不太符合人的听觉感受。</br>
（2）幅值平方和的常数对数的10倍：
<center>$volume = 10 * log_{10}\sum_{i=1}^{n}s_{i}^{2}$.</center>
它的单位是分贝（Decibels），是一个对数强度值，比较符合人耳对声音大小的感觉，但计算量稍复杂。
</p>


<!--more-->


<p>音量计算的Python实现如下：</p>




<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>import math
</span><span class='line'>import numpy as np
</span><span class='line'>
</span><span class='line'># method 1: absSum
</span><span class='line'>def calVolume(waveData, frameSize, overLap):
</span><span class='line'>    wlen = len(waveData)
</span><span class='line'>    step = frameSize - overLap
</span><span class='line'>    frameNum = int(math.ceil(wlen*1.0/step))
</span><span class='line'>    volume = np.zeros((frameNum,1))
</span><span class='line'>    for i in range(frameNum):
</span><span class='line'>        curFrame = waveData[np.arange(i*step,min(i*step+frameSize,wlen))]
</span><span class='line'>        curFrame = curFrame - np.median(curFrame) # zero-justified
</span><span class='line'>        volume[i] = np.sum(np.abs(curFrame))
</span><span class='line'>    return volume
</span><span class='line'>
</span><span class='line'># method 2: 10 times log10 of square sum
</span><span class='line'>def calVolumeDB(waveData, frameSize, overLap):
</span><span class='line'>    wlen = len(waveData)
</span><span class='line'>    step = frameSize - overLap
</span><span class='line'>    frameNum = int(math.ceil(wlen*1.0/step))
</span><span class='line'>    volume = np.zeros((frameNum,1))
</span><span class='line'>    for i in range(frameNum):
</span><span class='line'>        curFrame = waveData[np.arange(i*step,min(i*step+frameSize,wlen))]
</span><span class='line'>        curFrame = curFrame - np.mean(curFrame) # zero-justified
</span><span class='line'>        volume[i] = 10*np.log10(np.sum(curFrame*curFrame))
</span><span class='line'>    return volume</span></code></pre></td></tr></table></div></figure>




<p>对于给定语音文件aeiou.wav，利用上面的函数计算音量曲线的代码如下：</p>




<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>import wave
</span><span class='line'>import pylab as pl
</span><span class='line'>import numpy as np
</span><span class='line'>import Volume as vp
</span><span class='line'>
</span><span class='line'># ============ test the algorithm =============
</span><span class='line'># read wave file and get parameters.
</span><span class='line'>fw = wave.open('aeiou.wav','r')
</span><span class='line'>params = fw.getparams()
</span><span class='line'>print(params)
</span><span class='line'>nchannels, sampwidth, framerate, nframes = params[:4]
</span><span class='line'>strData = fw.readframes(nframes)
</span><span class='line'>waveData = np.fromstring(strData, dtype=np.int16)
</span><span class='line'>waveData = waveData*1.0/max(abs(waveData))  # normalization
</span><span class='line'>fw.close()
</span><span class='line'>
</span><span class='line'># calculate volume
</span><span class='line'>frameSize = 256
</span><span class='line'>overLap = 128
</span><span class='line'>volume11 = vp.calVolume(waveData,frameSize,overLap)
</span><span class='line'>volume12 = vp.calVolumeDB(waveData,frameSize,overLap)
</span><span class='line'>
</span><span class='line'># plot the wave
</span><span class='line'>time = np.arange(0, nframes)*(1.0/framerate)
</span><span class='line'>time2 = np.arange(0, len(volume11))*(frameSize-overLap)*1.0/framerate
</span><span class='line'>pl.subplot(311)
</span><span class='line'>pl.plot(time, waveData)
</span><span class='line'>pl.ylabel("Amplitude")
</span><span class='line'>pl.subplot(312)
</span><span class='line'>pl.plot(time2, volume11)
</span><span class='line'>pl.ylabel("absSum")
</span><span class='line'>pl.subplot(313)
</span><span class='line'>pl.plot(time2, volume12, c="g")
</span><span class='line'>pl.ylabel("Decibel(dB)")
</span><span class='line'>pl.xlabel("time (seconds)")
</span><span class='line'>pl.show()</span></code></pre></td></tr></table></div></figure>




<p>运行以上程序得到下图：
<center><img src="http://ibillxia.github.com/images/2013/IMAG2013051501.png"></center>
</p>




<h2>参考（References）</h2>


<p>[1]Volume (音量):http://neural.cs.nthu.edu.tw/jang/books/audiosignalprocessing/basicFeatureVolume.asp?title=5-2%20Volume%20(%AD%B5%B6q)</br>
[2]用Python做科学计算-声音的输入输出:http://hyry.dip.jp:8000/pydoc/wave_pyaudio.html</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[语音信号处理基础学习笔记之时域处理]]></title>
    <link href="http://ibillxia.github.com/blog/2013/05/08/speech-processing-in-time-domain/"/>
    <updated>2013-05-08T23:13:00+08:00</updated>
    <id>http://ibillxia.github.com/blog/2013/05/08/speech-processing-in-time-domain</id>
    <content type="html"><![CDATA[<p>语音信号的分析分为时域、频域、倒谱域等，时域分析简单、运算量小、物理意义明确，但对于语音识别而言，
更为有效的是频域的分析方法，那么为什么还有进行时域的分析呢？</p>




<p>语音信号具有时变特性，但在短时内可以看做是平稳的，所以语音的时域分析是建立在“短时”的条件下的，经研究统计，
语音信号在帧长为10ms~30ms内是相对平稳的。</p>




<p>语音信号是模拟信号，在进行处理之前，要进行数字化，模拟信号数字化的一般方法是采样，按照Nyquist采样定理进行
采样（一般在8K~10KHz）后，在进行量化（一般用8bit，也有16bit等）和编码，变为数字信号。</p>




<p>在语音信号数字化之后，就可以开始对其进行处理了，首先是预处理，由于语音信号的平均功率谱受声门激励和口鼻辐射的影响，
高频端大约在800Hz以上按6dB/倍频程跌落，为此要在预处理中进行预加重。预加重的目的是提升高频部分，是信号变得平坦，
以便于进行频谱分析或声道参数分析。预加重可以用具有6dB/倍频程的提升高频特性的预加重数字滤波器实现。预处理的另一
方面工作是分帧和加窗：分帧的帧长一般在10ms~30ms，分帧既可以是连续的，也可以是有部分over-lap；短时分析的实质是
对信号加窗，一般采用Hamming窗，其他的还有矩形窗、汉宁窗等，如下图所示。
<center><img src="http://ibillxia.github.com/images/2013/IMAG2013050801.png"></center>
</p>




<!--more-->




<p>好了，经过预处理之后就可以真正开始进行时域分析了，这里的时域分析主要包含短时平均能量、短时过零分析、短时自相
关分析以及高阶统计量分析等。</p>




<p>短时平均能量（Short Time Average Energy）可以理解为先计算信号格采样值的平方，然后用一个移动窗h(n-m)选取出一个个
短时平方序列，并将各段的平方值求和，从而得到短时能量序列。短时平均能量（En）可以用来从清音中区分浊音（浊音的En比
清音大得多），可以用来确定声母和韵母、无声与有声、连字等的分界，还可以作为一种超音段信息用于语音识别。但短时平均
能量En对于高电平信号可能产生溢出，此时可以采用短时平均幅度（Short Time Average Magnitude）来度量语音信号幅度的变化。</p>




<p>信号的幅度值从正值到负值要经过零点，从负值到正值也要经过零点，称为过零，统计信号在单位时间（如1s）内过零的次数，
就成为过零率。如果信号按段分割，就成为短时，把各段信号的过零率做统计平均，就是短时平均过零率（Short Time Average Cross 
Zero Ratio）。短时平均过零率（Zn）可以作为“频率”来理解。过零率可以用来定量的分析清音/浊音，特别是在背景噪声电平较大时
更为有效（相比短时平均能量而言），有时还可以同时结合Zn和En来进行判定。</p>




<p>如果说短时平均过零率是描述复杂波形“频率”特征的一个参数，那么短时平均上升过零间隔（Short Time Rise Zero-Crossing Inteval）
就是描述复杂波形“周期”特性的参数。研究表明：在一定噪声背景下，该参数具有很好的稳健性，对不同的语音具有很好的差异性。</p>




<p>自相关函数是偶函数，语音信号的短时自相关函数（Short Time Autocorrelation Function）可以理解为序列[x(n)x(n-k)]通过一个
冲激响应为hk(n)的数字滤波器的输出，即有Rn(k) = [x(n)x(n-k)]*hk(n)。短时自相关函数是语音信号时域分析中的一个重要参量，但是
运算量很大。短时平均幅度差函数AMDF（Short Time Average Magnitude Difference Function）与自相关函数有类似的功效，但运算量
可降低许多，所以在语音信号处理中应用广泛。</p>




<p>最后是高阶统计量了。近来高阶统计量在语音信号处理中应用也越来越多，高阶统计量一般指高阶矩(Moment)、高阶累积量(Cumulant)以及
他们的谱——高阶矩谱和高阶累积量谱。首先定义了随机变量x的（第一）特征函数（也称为矩生成函数），实际为它的密度函数f(x)的傅里叶变换。
然后定义了第二特征函数（也称为累积量生成函数），它是第一特征函数的对数。还有随机变量x的k阶矩（mk）的定义，它是x的k次幂与f(x)的
乘积在x∈R上的积分。类似的还有k阶中心矩（μk）的定义，都与概率论中的定义差不多。现在，可以对第一、二特征函数进行泰勒展开，可以得
到ck（x的k阶累积量）和mk之间的一些关系，可以发现k<4时，ck=μk，此时ck的物理意义与μk的物理意义相同，而k>=4时，则不相等。对于c3，
描述了概率分布的对称性，通过定义一个新的概念——偏度（Skewness，也称为偏态系数）来衡量。对于c4，文中为了简化，假设了x的均值为0，
然后定义了一个称为峰态（也称峰度，Kurtosis）的概念，以表示分布相对于正太分布的尖锐或平坦程度。后面两小节分别对此进行了从单个
随机变量到多个随机变量的推广的分析和随机变量服从高斯分布（正态分布）的特殊情形做了分析。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[五一登高远足]]></title>
    <link href="http://ibillxia.github.com/blog/2013/05/01/go-hiking-International-Labour-Day/"/>
    <updated>2013-05-01T22:01:00+08:00</updated>
    <id>http://ibillxia.github.com/blog/2013/05/01/go-hiking-International-Labour-Day</id>
    <content type="html"><![CDATA[<p>五一天晴气爽，登高望远，强身健体！只可惜“不畏浮云遮望眼，只缘身在最高层” 这句诗在空气严重污染的今天已不适用了！</p>




<p><img src="http://ibillxia.github.com/images/2013/IMAG2013050101.jpg">

<!-- more -->

<img src="http://ibillxia.github.com/images/2013/IMAG2013050102.jpg">

<img src="http://ibillxia.github.com/images/2013/IMAG2013050103.jpg">

<img src="http://ibillxia.github.com/images/2013/IMAG2013050104.jpg">

<img src="http://ibillxia.github.com/images/2013/IMAG2013050105.jpg">

<img src="http://ibillxia.github.com/images/2013/IMAG2013050106.jpg">

<img src="http://ibillxia.github.com/images/2013/IMAG2013050107.jpg">

<img src="http://ibillxia.github.com/images/2013/IMAG2013050108.jpg">

<img src="http://ibillxia.github.com/images/2013/IMAG2013050109.jpg">

<img src="http://ibillxia.github.com/images/2013/IMAG2013050110.jpg">

<img src="http://ibillxia.github.com/images/2013/IMAG2013050111.jpg">

<img src="http://ibillxia.github.com/images/2013/IMAG2013050112.jpg">

<img src="http://ibillxia.github.com/images/2013/IMAG2013050113.jpg">

<img src="http://ibillxia.github.com/images/2013/IMAG2013050114.jpg">

<img src="http://ibillxia.github.com/images/2013/IMAG2013050115.jpg">

<img src="http://ibillxia.github.com/images/2013/IMAG2013050116.jpg">
</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[使用Alize等工具构建说话人识别平台]]></title>
    <link href="http://ibillxia.github.com/blog/2013/04/26/building-speaker-recognition-system-using-alize-etc/"/>
    <updated>2013-04-26T22:07:00+08:00</updated>
    <id>http://ibillxia.github.com/blog/2013/04/26/building-speaker-recognition-system-using-alize-etc</id>
    <content type="html"><![CDATA[<p>前段时间有好几位同学询问如何用Alize实现说话人识别的问题，由于寒假前赶Paper，来不及详细解答，更没时间写Demo。
开学后不久抽时间写了一个Demo，并上传到了GitHub：https://github.com/ibillxia/VoicePrintReco/tree/master/Demo</p>




<p>下面将利用Alize+SPro进行简单的GMM-Based的说话人识别的基本流程总结如下：</br>
1.Features extraction 特征提取</br>
sfbcep.exe（MFCC）或slpcep.exe（LPCC）</br>

2.Silence removal 静音检测和去除</br>
NormFeat.exe 先能量规整</br>
EnergyDetector.exe 基于能量检测的静音去除</br>

3.Features Normalization 特征规整</br>
NormFeat.exe 再使用这个工具进行特征规整</br>

4.World model training</br>
TrainWorld.exe 训练UBM</br>

5.Target model training</br>
TrainWorld.exe 在训练好UBM的基础上训练training set和testing set的GMM</br>

6.Testing</br>
ComputeTest.exe 将testing set 的GMM在training set的GMM上进行测试和打分</br>

7.Score Normalization</br>
ComputeNorm.exe 将得分进行规整</br>

8. Compute EER 计算等错误率</br>
你可以查查计算EER的matlab代码，NIST SRE的官网上有下载（http://www.itl.nist.gov/iad/mig//tools/DETware_v2.1.targz.htm）。</br>
</p>




<!--more-->




<p>关于各步骤中参数的问题，可以在命令行“工具 -help”来查看该工具个参数的具体含义，另外还可参考Alize源码中各个工具的test目录中提供的实例，
而关于每个工具的作用及理论知识则需要查看相关论文。</p>




<p>常见问题及解答: http://mistral.univ-avignon.fr/mediawiki/index.php/Frequently_asked_questions</p>




<p>更多问题请在Google论坛（https://groups.google.com/forum/?fromgroups=&hl=zh-CN#!forum/alize&#8212;voice-print-recognition）提出，大家一起讨论！</p>




<h3>推荐资料</h3>


<p>
[1] ALIZE - User Manual: http://mistral.univ-avignon.fr/doc/userguide_alize.001.pdf</br>
[2] LIA_SPKDET Package documentation: http://mistral.univ-avignon.fr/doc/userguide_LIA_SpkDet.002.pdf</br>
[3] Reference System based on speech modality ALIZE/LIA RAL: http://www-clips.imag.fr/geod/User/laurent.besacier/NEW-TPs/TP-Biometrie/tools/CommentsLBInstall/doc.pdf</br>
[4] Jean-Francois Bonastre, etc. ALIZE/SpkDet: a state-of-the-art open source software for speaker recognition</br>
[5] TOMMIE GANNERT. A Speaker Veri?cation System Under The Scope: Alize</br>
[6] Alize Wiki: http://mistral.univ-avignon.fr/mediawiki/index.php/Main_Page
</p>



]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[VALSE2013]]></title>
    <link href="http://ibillxia.github.com/blog/2013/04/22/VALSE2013/"/>
    <updated>2013-04-22T22:28:00+08:00</updated>
    <id>http://ibillxia.github.com/blog/2013/04/22/VALSE2013</id>
    <content type="html"><![CDATA[<h3>学术研讨</h3>


<p>VALSE是Vision And Learning SEminar的缩写，它主要目的是为计算机视觉、图像处理、模式识别与机器学习研究领域内的中国青年学者（以70后研发
人员为主）提供一个深层次学术交流的舞台。虽然参与会议和做报告的人主要是做视觉的，但很多问题是机器学习和模式识别当中的一般性问题，所以我这
个搞语音的也去打酱油了^_^。</p>




<p>今年的VALSE在南京东南大学召开，参加会议的人数超出预期，会场爆满，仅学校的老师和公司的研究人员就占了会场大半，学生沦落到只能座最后两排，
或者座分会场（这个太不科学了-_-!）。会程安排也很紧凑，中午几乎没有休息时间，吃饭都很赶，而下午也很晚（6点半左右）才结束。这次会议有好几个
perfect的报告，但也有些不太感兴趣的，有的甚至感觉很2。除了一些报告，还有两个主题讨论会，印象中主要包括三个论题：学术界与工业界的Gap及衔接
问题，深度学习是否是计算机视觉的终极解决方案，计算机视觉要不要从生物视觉机理中受启发等。</p>




<p>闲话少说，言归正传，数萝卜下窖的讲讲这两天的经历吧。
第一天上午，第一个做报告的是MSRA的张磊，主要讲了计算机视觉的一些基本问题，从AI的历史将起，提到了Turing Test，是人工智能
的Benchmark。而CV的一个基本问题是Object Recognition，人们的研究经历了从之前的Model Based到如今的Data Driven及Big Data的过程，各种模型和方法可谓
层出不穷，然而对于真正解决问题、真正达到人类一般的视觉智能，还相差甚远。接着他讲了关于在路灯下找钥匙的故事（详询http://tongyanyan.blog.edu.cn/2006/427512.html），
听了这个故事后，感觉那个找钥匙的人很滑稽可笑，然而再想想我们自己正在做的研究，是不是在某种程度上和故事中的这个人一样呢。通过这个故事，他引出自己
的观点：要想解决Object Recognition这个问题或者说要解决CV的问题，就需要More Effective Representation & Match。接下来讲在Representation方面一些研究
人员提出的一些人工设计的Feature，而在Match方面则从Point、Line、Plane、Volume（点线面体）进行了详尽的讲述。最后还提了一下Deep Neural Network在CV中的
应用，可以discover hidden patterns。虽然对CV中的很多概念和模型方法不太了解，但感觉还是挺有收获的。</p>




<p>上午的后两个报告都是讲Sparse的，虽然之前看过关于Sparse Coding的东西，但当他们在上面讲的，主要偏重与Sparse这个问题的优化求解方法及其变形，
涉及到很多数学公式和推导，感觉很枯燥，加之晚睡早起，有点犯困，所以基本没有听进去。贾佳亚的报告还似懂非懂，而陈欢欢的Sparse Bayesian Learning
表示完全没听懂。个人感觉Sparse还是很重要的，所以在弄完Deep Learning这个专题后，我想有必要对这两个报告及其相关论文再做深入的学习和研究。</p>


<!--more-->


<p>中午3个东南大学的同学请我们实验室的在他们学校食堂吃饭，虽然不太记得他们名字了，真心感谢他们！</p>




<p>下午第一个报告是高新波的IQA&VALSE，主要讲了图像质量评价的一些东西，虽然也提到了一些生物视觉方面的东西，感觉很没趣，基本没怎么听，打了个盹。
第二个报告是俞洪波的关于生物视觉方面的东西，很感兴趣，他从深层复杂网络结构、神经元、突触、离子通道、蛋白等多个层面上讲了视觉系统的信息处理流程，
后面还提到了视觉功能柱，指出了视觉神经元具有很强的选择性，不同部位的神经元对不同方向、距离的视觉信息具有选择性的激活增强，最后还讲了一些模拟
视觉系统的计算模型，并描述了一些实验，虽然对报告题目中的Self Organization Model到底是什么还不是很清楚，但对生物视觉系统有了更进一步的了解，
而且知道了他们是怎么获取神经元激活区域的。</p>




<p>下午的第2个Session的第一个报告是颜水成的Fashion Recommendation，包括Hair Style，Makeup，Clothing，Shoes等的Recommendation，不太感冒，只是对他重复提到
的关于华人做研究的一个问题深表同感，他说华人做研究其实很不错的，能在很多TOP会议期刊发Paper甚至Best Paper，但原创性的问题却很少，我们都在提高别人的Citation，
所以华人还需要在发现问题方面多下功夫，而不是仅仅在解决问题方面。后面两个报告一个是王亦洲的General Purpose Vision，表示没听懂。最后一个报告是王晓刚的Crowd 
Video Surveillance，主要是讲在Video中识别人并跟踪人的移动，或者统计视场中人的数量之类的，只是感性的了解了一下，印象里报告中好像没有提到什么具体的CV技术，
只是举了一个人体位置跟踪的例子，还有一个用在足球视屏中运动员跟踪的例子。</p>




<p>第二天上午第一个Session是两个报告，一个是陈小武的Image/Video/3D Scene Understanding and Editing，主要分以下四个方面：Illumination Learning and Synthesis，
Labeling and Lavering and Editing，Estimating 3D Model from a single Image，Video Event Representation and Inference，总体感觉讲的内容涉及到很多东西，甚至他的
学生不仅懂CV，还要懂美术、剪纸等，而且他们每年都会发CVPR、ICCV、ECCV之类的，感觉还挺NB的。另外一个报告是非常期待的于凯的关于深度学习和大数据的报告，但听了之后，
感觉有些Depressed，因为他的报告中没有涉及Deep Learning的一些细节的东西，诸如RBM的原理及其训练等，基本上只是泛泛而谈，之前对Deep Learning做了深入的调研和学习，
自我感觉Deep Learning也没什么神秘的，虽然对Gibbs采样和CD算法的理论还没有完全理解清楚，但我觉得Deep Learning更多的是一种思想方法，在Deep Architecture中，Knowledge
通过一层一层抽象和提取后，对于Classification、Clustering等任务具有更有效Representation，而且在Training Error非常小的情况下，还是可以再Testing中获得理想的Error Ratio，
相比Shallow Architecture，不存在模型Over Fitting的问题。另外，有人提到Deep Architecture中Layer数目的确定的问题，于凯的回答是，在Neural Networks中加一层后，进行Deep 
Learning的过程，如果相对于没加该层得到的Test Error更小，并且是非常有效的性能提升，那么就加进这一层。然后同样的，再加一层，再进行Deep Learning，以此类推。</p>




<p>上午的后一个Session是关于CV在Industry中的Application，先是来在Industry中的一些研究开发人员对他们目前的工作做一些简短的介绍，感觉某些公司有严重的广告嫌疑，很是讨厌。
然后是讨论阶段，各自就CV在学术界和工业界之间的Gap发表意见，总结起来主要有以下观点：一方面学术界与工业界的Gap是必要的，学术必须要超前，这样工业界才可能将其成熟的应用；
另一方面，学术界与工业界的Gap可以通过在工业界设置研究院（比如MSRA、百度最近在硅谷设置深度学习研究院之类），这样可以加快学术成果应用于工业界的进程，学术最终的目的就是
在工业界中发挥巨大作用，服务广大民众，给社会带来价值。</p>




<p>上午的Panel严重超时了，直到快1点了才结束，去餐馆吃饭，我们跟老板说我们下午要考试，让快点上菜，结果上菜速度果然飞快，而我们吃得也很快，基本上一盘菜一会儿就吃光，
真是高效啊，哈哈！</p>




<p>下午首先是一个Panel，讨论（更确切的说是辩论）了两个主题，一个是关于计算机视觉是否要借鉴吸收生物神经视觉的结果，另一个是Deep Learning是否是CV的终极解决方案，这两个辩论
都非常精彩，笑点不断。Panel开始之前，首先是两位报告者发言，首先上台的是 @老师木（袁进辉），他自我介绍了一番，然后讲了讲生物视觉与计算机视觉的紧密联系，认为计算机视觉要想
取得重大突破，就必须借鉴生物视觉的研究的发现。另外一位是李学龙老师，很有个性，只写了一张PPT，但发言时却如滔滔江水绵绵不绝，可以听得出，他对生物视觉也非常了解，也认为计算机
视觉必须借鉴生物视觉的一些研究成果。后面的讨论非常精彩，将学术娱乐化了。这两个论题本身就很具争议性，正反两方各执其词，要辩论出个是非来，还真需要真才实学。</p>




<p>Panel完之后是两个报告，一个是吴建鑫的Approximating Additive Kernel for Large Scale Vision Tasks，没怎么听懂。另一个是张敏灵的Multi-label Learning，感觉很没趣，主要是
觉得这并不是一个新问题，但在图像标注方面确实是一个很重要的问题。</p>




<p>最后，还可以从一些微博内容中获取更多关于VALSE2013的信息，可以搜索主题\#VALSE\# 或\#VALSE2013\#，或者关注 @潘布衣（会议Chair潘刚）、@张磊MSRA、@余凯_西二旗民工、
@老师木等等。。。</p>




<h3>游玩休闲</h3>


<p>我们周五下午6点多到，下雨了。坐地铁然后走到旅馆，吃晚饭就8点了，但还好雨也听了，我们就去附近的夫子庙、秦淮河逛了逛。
<center><img src="http://ibillxia.github.com/images/2013/IMAG2013042201.jpg"></center>
第二天晚上去阅江楼逛了逛，到哪儿才发现晚上关门，坑爹啊！不过在外面远眺夜晚的阅江楼也不错，然后走了一个把小时到南京长江大桥。
<center><img src="http://ibillxia.github.com/images/2013/IMAG2013042202.jpg"></center>
第三天晚上就待在住处。因为订的第四天下午6点多的票，所以白天就可以尽情的去玩玩了。第一站来到了中山园陵
<center><img src="http://ibillxia.github.com/images/2013/IMAG2013042203.jpg"></center>
只可惜周一不开放，被挡在“天下为公”的门外。原打算接下来要去的雨花台、大屠杀纪念馆也不开放，坑爹啊！
<center><img src="http://ibillxia.github.com/images/2013/IMAG2013042204.jpg"></center>
木办法，就在里面找了一个开放的十朝历史博物馆去了
<center><img src="http://ibillxia.github.com/images/2013/IMAG2013042205.jpg"></center>
然后去总统府了，就在外面看了看
<center><img src="http://ibillxia.github.com/images/2013/IMAG2013042206.jpg"></center>
再然后去玄武门，一到哪儿就又下雨了
<center><img src="http://ibillxia.github.com/images/2013/IMAG2013042207.jpg"></center>
进里面看了看玄武湖，然后就直接回旅馆了
<center><img src="http://ibillxia.github.com/images/2013/IMAG2013042208.jpg"></center>
</p>



]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[深度学习及其在语音方面的应用]]></title>
    <link href="http://ibillxia.github.com/blog/2013/04/17/Deep-Learning-and-its-application-in-audio-and-speech-processing/"/>
    <updated>2013-04-17T22:43:00+08:00</updated>
    <id>http://ibillxia.github.com/blog/2013/04/17/Deep-Learning-and-its-application-in-audio-and-speech-processing</id>
    <content type="html"><![CDATA[<p>以下是今天在组会上讲的内容，与大家分享一下。有些地方我也没有完全理解，欢迎大家一起来讨论。</p>


<p><center>
<embed width="780"
    height="574"
    name="plugin"
    src="http://ibillxia.github.com/upload/Deep Learning - Bill Xia.pdf"
    type="application/pdf"
/>
</center></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[基于能量的模型和波尔兹曼机]]></title>
    <link href="http://ibillxia.github.com/blog/2013/04/12/Energy-Based-Models-and-Boltzmann-Machines/"/>
    <updated>2013-04-12T22:12:00+08:00</updated>
    <id>http://ibillxia.github.com/blog/2013/04/12/Energy-Based-Models-and-Boltzmann-Machines</id>
    <content type="html"><![CDATA[<p>由于深度置信网络（Deep Belief Networks，DBN）是基于限制性玻尔兹曼机（Restricted Boltzmann Machines，RBM）的深层网络结构，
所以本文重点讨论一下玻尔兹曼机（BM），以及它的学习算法——对比散度（Contrastive Divergence，CD）算法。在介绍BM前，我们首先介绍一下
基于能量的模型（Energy Based Model，EBM），因为BM是一种特殊的EBM。</p>




<h2>1. 基于能量的模型(EBM)</h2>


<p>基于能量的模型是一种具有普适意义的模型，可以说它是一种模型框架，在它的框架下囊括传统的判别模型和生成模型，图变换网络(Graph-transformer 
Networks)，条件随机场，最大化边界马尔科夫网络以及一些流形学习的方法等。EBM通过对变量的每个配置施加一个有范围限制的能量来捕获变量之间的依赖
关系。EBM有两个主要的任务，一个是推断(Inference)，它主要是在给定观察变量的情况，找到使能量值最小的那些隐变量的配置；另一个是学习(Learning)，
它主要是寻找一个恰当的能量函数，使得观察变量的能量比隐变量的能量低。</p>




<p>基于能量的概率模型通过一个能量函数来定义概率分布，
<center>$p(x) = \frac{e^{E(x)}}{Z}.$ &#8230; ①</center>
其中Z为规整因子，
<center>$Z = \sum _{x} e^{-E(x)}.$ &#8230; ②</center>
基于能量的模型可以利用使用梯度下降或随机梯度下降的方法来学习，具体而言，就是以训练集的负对数作为损失函数，
<center>$l(\theta,D) = -L(\theta,D) = - \frac{1}{N}\sum_{x^{(i)}\in D} log p(x^{(i)}).$ &#8230; ③</center>
其中$\theta$为模型的参数，将损失函数对$\theta$求偏导，
<center>$\Delta = \frac{\partial l(\theta,D)}{\partial \theta} = - \frac{1}{N} \frac{\partial \sum log p(x^{(i)})}{\partial \theta}.$ &#8230; ④</center>
即得到损失函数下降最快的方向。</p>




<!--more-->




<h3>包含隐单元的EBMs</h3>


<p>在很多情况下，我们无法观察到样本的所有属性，或者我们需要引进一些没有观察到的变量，以增加模型的表达能力，这样得到的就是包含隐含变量的EBM，
<center>$P(x) = \sum _{h} P(x,h) = \sum _{h} \frac{e^{-E(x,h)}}{Z}.$ &#8230; ⑤</center>
其中$h$表示隐含变量。在这种情况下，为了与不包含隐含变量的模型进行统一，我们引入如下的自由能量函数，
<center>$F(x) = - log \sum_{h}e^{-E(x,h)}.$ &#8230; ⑥</center>
这样$P(x)$就可以写成，
<center>$P(x) = \frac{e^{-F(x)}}{Z}, where Z = \sum_{x} e^{-F(x)}.$ &#8230; ⑦</center>
此时，损失函数还是类似的定义，只是在进行梯度下降求解时稍微有些不同，
<center>$\Delta = - \frac{\partial log p(x)}{\partial \theta} 
= - \frac{\partial (-F(x) -log Z)}{\partial \theta} 
= \frac{\partial F(x)}{\partial \theta} - \sum_{\hat{x}} p(\hat{x}) \frac{\partial F(\hat{x})}{\partial \theta}$. &#8230; ⑧</center>
该梯度表达式中包含两项，他们都影响着模型所定义的分布密度：第一项增加训练数据的概率（通过减小对应的自由能量），而第二项则减小模型
生成的样本的概率。</p>




<p>通常，我们很难精确计算这个梯度，因为式中第一项涉及到可见单元与隐含单元的联合分布，由于归一化因子$Z(\theta)$的存在，该分布很难获取[3]。
我们只能通过一些采样方法（如Gibbs采样）获取其近似值，其具体方法将在后文中详述。</p>




<h2>2. 限制性玻尔兹曼机</h2>


<p>玻尔兹曼机（Boltzmann Machine，BM）是一种特殊形式的对数线性的马尔科夫随机场（Markov Random Field，MRF），即能量函数是自由变量的线性函数。
通过引入隐含单元，我们可以提升模型的表达能力，表示非常复杂的概率分布。</p>




<p>限制性玻尔兹曼机（RBM）进一步加一些约束，在RBM中不存在可见单元与可见单元的链接，也不存在隐含单元与隐含单元的链接，如下图所示
<center><img src="http://ibillxia.github.com/images/2013/IMAG2013041201.png"></center>
RBM的能量函数$E(v,h)$定义为，
<center>$E(v,h) = -b&#8217;v - c&#8217;h - h&#8217;Wv$.</center>
其中&#8217;表示转置，$b,c,W$为模型的参数，$b,c$分别为可见层和隐含层的偏置，$W$为可见层与隐含层的链接权重。此时，对应的自由能量为，
<center>$F(v) = -b&#8217;v - \sum_{i}log\sum_{h_{i}}e^{h_{i}(c_{i}+W_{i}v)}.$ &#8230; ⑨</center>
另外，由于RBM的特殊结构，可见层/隐含层内个单元之间是相互独立的，所以我们有，
<center>$p(h|v) = \prod _{i} p(h_{i}|v)$,</center>
<center>$p(v|h) = \prod _{j} p(v_{j}|h).$ &#8230; ⑩</center>
</p>




<h3>使用二值单元的RBM</h3>


<p>如果RBM中的每个单元都是二值的，即有$v_{j},h_{i} \in \{0,1\}$，我们可以得到，
<center>$p(h_{i}=1|v) = sigmoid(c_{i} + W_{i}v)$,</center>
<center>$p(v_{j}=1|h) = sigmoid(b_{j} + W_{j}&#8217;h).$ &#8230; ⑪</center>
而对应的自由能量函数为，
<center>$F(v) = -b&#8217;v - \sum_{i}log(1+e^{c_{i}+W_{i}v}).$ &#8230; ⑫</center>
使用梯度下降法求解模型参数时，各参数的梯度值如下[2]，
<center>$-\frac{\partial logp(v)}{\partial W_{ij}} = E_{v}[p(h_{i}|v) * v_{j}] - v_{j}^{(i)} * sigmoid(W_{i} * v^{(i)}+c_{i}),$</center>
<center>$-\frac{\partial logp(v)}{\partial c_{i}} = E_{v}[p(h_{i}|v) * v_{j}] - sigmoid(W_{i} * v^{(i)}),$</center>
<center>$-\frac{\partial logp(v)}{\partial b_{j}} = E_{v}[p(h_{i}|v) * v_{j}] - v_{j}^{(i)}.$ &#8230; ⑬</center>
</p>




<h2>3. RBM的学习</h2>


<p>前面提到了，RBM是很难学习的，即模型的参数很难确定，下面我们就具体讨论一下基于采样的近似学习方法。学习RBM的任务是求出模型的参数
$\theta = \{c, b, W\}$的值。</p>




<h3>3.1 Gibbs采样</h3>


<p>Gibbs采样是一种基于马尔科夫链蒙特卡罗(Markov Chain Monte Carlo,MCMC)策略的采样方法。对于一个$K$为随机向量$X = (X_{1},X_{2},&#8230;,X_{K})$，
假设我们无法求得关于$X$的联合分布$P(X)$，但我们知道给定$X$的其他分量时，其第$k$个分量$X_{k}$的条件分布，即$P(X_{k}|X_{k^{-}})$，其中$X_{k^{-}} = 
(X_{1},X_{2},&#8230;,X_{k-1},X_{k+1},&#8230;,X_{K})$，那么，我们可以从$X$的一个任意状态(比如[$x_{1}(0),x_{2}(0),&#8230;,x_{K}(0)$])开始，利用上述条件
分布，迭代的对其分量依次采样，随着采样次数的增加，随机变量[$x_{1}(n),x_{2}(n),&#8230;,x_{K}(n)$]的概率分布将以$n$的几何级数的速度收敛于$X$的联合
概率分布$P(X)$。也就是说，我们可以在未知联合概率分布的条件下对其进行采样。</p>




<p>基于RBM的对称结构，以及其中神经元状态的条件独立性，我们可以使用Gibbs采样方法得到服从RBM定义的分布的随机样本。在RBM中进行$k$步Gibbs采样的具体
算法为：用一个训练样本(或可见层的任何随机化状态)初始化可见层的状态$v0$，交替进行如下采样：
<center>$h_{0} \sim P(h|v_{0}), v_{1} \sim P(v|h_{0}),$</center>
<center>$h_{1} \sim P(h|v_{1}), v_{2} \sim P(v|h_{1}),$</center>
<center>$&#8230; &#8230;, v_{k+1} \sim P(v|h_{k})$.</center>
在经过步数$k$足够大的情况下，我们可以得到服从RBM所定义的分布的样本。此外，使用Gibbs采样我们也可以得到式⑧中第一项的近似。</p>




<h3>3.2 对比散度算法</h3>


<p>尽管利用Gibbs采样我们可以得到对数似然函数关于未知参数梯度的近似，但通常情况下需要使用较大的采样步数，这使得RBM的训练效率仍然不高，尤其是当观测数据
的特征维数较高时。2002年，Hinton[4]提出了RBM的一个快速学习算法，即对比散度（Contrastive Divergence，CD）。与Gibbs采样不同，Hinton指出当使用训练数据初
始化$v_{0}$时，我们仅需要使用$k$（通常k=1）步Gibbs采样变可以得到足够好的近似。在CD算法一开始，可见单元的状态被设置成一个训练样本，并利用式⑪第一个式子
来计算所有隐层单元的二值状态，在所有隐层单元的状态确定了之后，根据式⑪第二个式子来确定第$i$个可见单元$v_{i}$取值为1的概率，进而产生可见层的一个重构
(reconstruction)。然后将重构的可见层作为真实的模型代入式⑬各式中第一项，这样就可以进行梯度下降算法了。</p>




<p>在RBM中，可见单元一般等于训练数据的特征维数，而隐层单元数需要事先给定，这里设可见单元数和隐单元数分别为$n$和$m$，令$W$表示可见层与隐层间的链接权重
矩阵(m×n阶)，$a$(n维列向量)和$b$(m维列向量)分别表示可见层与隐层的偏置向量。RBM的基于CD的快速学习算法主要步骤如下：

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>//输入：一个训练样本x0; 隐层单元个数m; 学习率alpha; 最大训练周期T
</span><span class='line'>//输出：链接权重矩阵W, 可见层的偏置向量a, 隐层的偏置向量b
</span><span class='line'>//训练阶段
</span><span class='line'>初始化：令可见层单元的初始状态v1 = x0; W, a, b为随机的较小数值
</span><span class='line'>For t=1,2,...,T
</span><span class='line'>  For j=1,2,...,m //对所有隐单元
</span><span class='line'>      计算P(h1j=1|v1), 即P(h1j=1|v1) = sigmoid(bj+sum_i(v1i*Wij));
</span><span class='line'>      从条件分布P(h1j|v1)中抽取h1j ∈ {0,1}
</span><span class='line'>  EndFor
</span><span class='line'>  
</span><span class='line'>  For i=1,2,...,n //对所有可见单元
</span><span class='line'>      计算P(v2i=1|h1), 即P(v2i=1|h1) = sigmoid(ai+sum_j(Wij*h1j));
</span><span class='line'>      从条件分布P(v2i|h1)中抽取v2i ∈ {0,1}
</span><span class='line'>  EndFor
</span><span class='line'>  
</span><span class='line'>  For j=1,2,...,m //对所有隐单元
</span><span class='line'>      计算P(h2j=1|v2), 即P(h2j=1|v2) = sigmoid(bj+sum_i(v2i*Wij));
</span><span class='line'>  EndFor
</span><span class='line'>  
</span><span class='line'>  //更新RBM的参数
</span><span class='line'>  W = W + alpha *(P(h1=1|v1)v1' - P(h2=1|v2)v2');
</span><span class='line'>  a = a + alpha *(v1-v2);
</span><span class='line'>  b = b + alpha *(P(h1=1|v1) - P(h2=1|v2));
</span><span class='line'>EndFor</span></code></pre></td></tr></table></div></figure>

上述基于CD的学习算法是针对RBM的可见单元和隐层单元均为二值变量的情形，我们可以很容易的推广到这些单元为高斯变量的情形。
</p>




<p>RBM的完整实现参见https://github.com/ibillxia/DeepLearnToolbox/tree/master/DBN的Matlab代码。</p>




<h2>References</h2>


<p>
[1] Learn Deep Architectures for AI, Chapter 5.</br>
[2] Deep Learning Tutorial, Release 0.1, Chapter 9.</br>
[3] 受限波尔兹曼机简介. 张春霞. </br>
[4] Training Products of experts by minimizing contrastive divergence. GE Hinton.
</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[2013IDF声龙语音识别技术演示]]></title>
    <link href="http://ibillxia.github.com/blog/2013/04/10/Intel-Developer-Forum-2013-Nuance-Dragon-Presentation/"/>
    <updated>2013-04-10T12:57:00+08:00</updated>
    <id>http://ibillxia.github.com/blog/2013/04/10/Intel-Developer-Forum-2013-Nuance-Dragon-Presentation</id>
    <content type="html"><![CDATA[<p>2013英特尔信息技术峰会(Intel Developer Forum, IDF)上，来自Nuance的声龙语音合成和识别技术的演示，中文语音识别不给力，
笑点频出啊，哈哈</p>




<p><iframe height=560 width=780 src="http://player.youku.com/embed/XNTQwNjQ0MjUy" frameborder=0 allowfullscreen></iframe></p>



]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[卷积神经网络（CNN）]]></title>
    <link href="http://ibillxia.github.com/blog/2013/04/06/Convolutional-Neural-Networks/"/>
    <updated>2013-04-06T23:34:00+08:00</updated>
    <id>http://ibillxia.github.com/blog/2013/04/06/Convolutional-Neural-Networks</id>
    <content type="html"><![CDATA[<h2>1. 概述</h2>


<p>卷积神经网络是一种特殊的深层的神经网络模型，它的特殊性体现在两个方面，一方面它的神经元间的连接是<strong>非全连接</strong>的，
另一方面同一层中某些神经元之间的连接的<strong>权重是共享的</strong>（即相同的）。它的非全连接和权值共享的网络结构使之更类似于生物
神经网络，降低了网络模型的复杂度（对于很难学习的深层结构来说，这是非常重要的），减少了权值的数量。</p>




<p>卷积网络最初是受视觉神经机制的启发而设计的，是为识别二维形状而设计的一个多层感知器，这种网络结构对平移、比例缩放、倾斜或者共他
形式的变形具有高度不变性。1962年Hubel和Wiesel通过对猫视觉皮层细胞的研究，提出了感受野(receptive field)的概念，1984年日本学者Fukushima
基于感受野概念提出的神经认知机(neocognitron)模型，它可以看作是卷积神经网络的第一个实现网络，也是感受野概念在人工神经网络领域的首次应用。</p>




<p>神经认知机将一个视觉模式分解成许多子模式(特征)，然后进入分层递阶式相连的特征平面进行处理，它试图将视觉系统模型化，使其能够在即使物体有
位移或轻微变形的时候，也能完成识别。神经认知机能够利用位移恒定能力从激励模式中学习，并且可识别这些模式的变化形。在其后的应用研究中，Fukushima
将神经认知机主要用于手写数字的识别。随后，国内外的研究人员提出多种卷积神经网络形式，在邮政编码识别（Y. LeCun etc）、车牌识别和人脸识别等方面
得到了广泛的应用。</p>




<h2>2. CNN的结构</h2>


<p>卷积网络是为识别二维形状而特殊设计的一个多层感知器，这种网络结构对平移、比例缩放、倾斜或者共他形式的变形具有高度不变性。
这些良好的性能是网络在有监督方式下学会的，网络的结构主要有稀疏连接和权值共享两个特点，包括如下形式的约束：</br>
1 特征提取。每一个神经元从上一层的局部接受域得到突触输人，因而迫使它提取<strong>局部特征</strong>。一旦一个特征被提取出来，
只要它相对于其他特征的位置被近似地保留下来，它的精确位置就变得没有那么重要了。</br>
2 特征映射。网络的每一个计算层都是由<strong>多个特征映射组</strong>成的，每个特征映射都是平面形式的。平面中单独的神经元在约束下<strong>共享
相同的突触权值</strong>集，这种结构形式具有如下的有益效果：a.平移不变性。b.自由参数数量的缩减(通过权值共享实现)。</br>
3.子抽样。每个卷积层跟着一个实现局部平均和子抽样的计算层，由此特征映射的分辨率降低。这种操作具有使特征映射的输出对平移和其他
形式的变形的敏感度下降的作用。</p>


<!--more-->




<h3>2.1 稀疏连接(Sparse Connectivity)</h3>


<p>卷积网络通过在相邻两层之间强制使用局部连接模式来利用图像的空间局部特性，在第m层的隐层单元只与第m-1层的输入单元的局部区域有连接，第m-1层的这些局部
区域被称为空间连续的接受域。我们可以将这种结构描述如下：</br>
设第m-1层为视网膜输入层，第m层的接受域的宽度为3，也就是说该层的每个单元与且仅与输入层的3个相邻的神经元相连，第m层与第m+1层具有类似的链接规则，如下图所示。
<center><img src="http://ibillxia.github.com/images/2013/IMAG2013040201.jpg"></center>
可以看到m+1层的神经元相对于第m层的接受域的宽度也为3，但相对于输入层的接受域为5，这种结构将学习到的过滤器（对应于输入信号中被最大激活的单元）限制在局部空间
模式（因为每个单元对它接受域外的variation不做反应）。从上图也可以看出，多个这样的层堆叠起来后，会使得过滤器（不再是线性的）逐渐成为全局的（也就是覆盖到了更
大的视觉区域）。例如上图中第m+1层的神经元可以对宽度为5的输入进行一个非线性的特征编码。
</p>




<h3>2.2 权值共享(Shared Weights)</h3>


<p>在卷积网络中，每个稀疏过滤器<em>$h_{i}$</em>通过共享权值都会覆盖整个可视域，这些共享权值的单元构成一个特征映射，如下图所示。
<center><img src="http://ibillxia.github.com/images/2013/IMAG2013040202.jpg"></center>
在图中，有3个隐层单元，他们属于同一个特征映射。同种颜色的链接的权值是相同的，我们仍然可以使用梯度下降的方法来学习这些权值，只需要对原始算法做一些小的改动，
这里共享权值的梯度是所有共享参数的梯度的总和。我们不禁会问为什么要权重共享呢？一方面，重复单元能够对特征进行识别，而不考虑它在可视域中的位置。另一方面，权值
共享使得我们能更有效的进行特征抽取，因为它极大的减少了需要学习的自由变量的个数。通过控制模型的规模，卷积网络对视觉问题可以具有很好的泛化能力。
</p>




<h3>2.3 The Full Model</h3>


<p>卷积神经网络是一个多层的神经网络，每层由多个二维平面组成，而每个平面由多个独立神经元组成。网络中包含一些简单元和复杂元，分别记为S-元
和C-元。S-元聚合在一起组成S-面，S-面聚合在一起组成S-层，用Us表示。C-元、C-面和C-层(Us)之间存在类似的关系。网络的任一中间级由S-层与C-层
串接而成，而输入级只含一层，它直接接受二维视觉模式，样本特征提取步骤已嵌入到卷积神经网络模型的互联结构中。</p>




<p>一般地，Us为特征提取层，每个神经元的输入与前一层的局部感受野相连，并提取该局部的特征，一旦该局部特征被提取后，它与其他特征间的位置关系
也随之确定下来；Uc是特征映射层，网络的每个计算层由多个特征映射组成，每个特征映射为一个平面，平面上所有神经元的权值相等。特征映射结构采用
影响函数核小的sigmoid函数作为卷积网络的激活函数，使得特征映射具有位移不变性(这一句表示没看懂，那位如果看懂了，请给我讲解一下)。此外，由于
一个映射面上的神经元共享权值，因而减少了网络自由参数的个数，降低了网络参数选择的复杂度。卷积神经网络中的每一个特征提取层(S-层)都紧跟着一个
用来求局部平均与二次提取的计算层(C-层)，这种特有的两次特征提取结构使网络在识别时对输入样本有较高的畸变容忍能力。</p>




<p>下图是一个卷积网络的实例
<center><img src="http://ibillxia.github.com/images/2013/IMAG2013040203.jpg"></center>
图中的卷积网络工作流程如下，输入层由32×32个感知节点组成，接收原始图像。然后，计算流程在卷积和子抽样之间交替进行，如下所
述：第一隐藏层进行卷积，它由8个特征映射组成，每个特征映射由28×28个神经元组成，每个神经元指定一个 5×5 的接受域；第二隐藏层实现子
抽样和局部平均，它同样由 8 个特征映射组成，但其每个特征映射由14×14 个神经元组成。每个神经元具有一个 2×2 的接受域，一个可训练
系数，一个可训练偏置和一个 sigmoid 激活函数。可训练系数和偏置控制神经元的操作点。第三隐藏层进行第二次卷积，它由 20 个特征映射组
成每个特征映射由 10×10 个神经元组成。该隐藏层中的每个神经元可能具有和下一个隐藏层几个特征映射相连的突触连接，它以与第一个卷积
层相似的方式操作。第四个隐藏层进行第二次子抽样和局部平均汁算。它由 20 个特征映射组成，但每个特征映射由 5×5 个神经元组成，它以
与第一次抽样相似的方式操作。第五个隐藏层实现卷积的最后阶段，它由 120 个神经元组成，每个神经元指定一个 5×5 的接受域。最后是个全
连接层，得到输出向量。相继的计算层在卷积和抽样之间的连续交替，我们得到一个“双尖塔”的效果，也就是在每个卷积或抽样层，随着空
间分辨率下降，与相应的前一层相比特征映射的数量增加。卷积之后进行子抽样的思想是受到动物视觉系统中的“简单的”细胞后面跟着“复
杂的”细胞的想法的启发而产生的。</p>




<p>图中所示的多层感知器包含近似 100000 个突触连接，但只有大约2600 个自由参数。自由参数在数量上显著地减少是通过权值共享获得
的，学习机器的能力（以 VC 维的形式度量）因而下降，这又提高它的泛化能力。而且它对自由参数的调整通过反向传播学习的随机形式来实
现。另一个显著的特点是使用权值共享使得以并行形式实现卷积网络变得可能。这是卷积网络对全连接的多层感知器而言的另一个优点。</p>




<h2>3. CNN的学习</h2>


<p>总体而言，前面提到的卷积网络可以简化为下图所示模型：
<center><img src="http://ibillxia.github.com/images/2013/IMAG2013040204.jpg"></center>
其中，input 到C1、S4到C5、C5到output是全连接，C1到S2、C3到S4是一一对应的连接，S2到C3为了消除网络对称性，去掉了一部分连接，
可以让特征映射更具多样性。需要注意的是 C5 卷积核的尺寸要和 S4 的输出相同，只有这样才能保证输出是一维向量。</p>




<h3>3.1 卷积层的学习</h3>


<p>卷积层的典型结构如下图所示。
<center><img src="http://ibillxia.github.com/images/2013/IMAG2013040205.jpg"></center>
</p>




<p>卷积层的前馈运算是通过如下算法实现的：</br>
<center>卷积层的输出= Sigmoid( Sum(卷积) +偏移量) </center>
其中卷积核和偏移量都是可训练的。下面是其核心代码：

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>ConvolutionLayer::fprop(input,output) {
</span><span class='line'>  //取得卷积核的个数
</span><span class='line'>  int n=kernel.GetDim(0);
</span><span class='line'>  for (int i=0;i&lt;n;i++) {
</span><span class='line'>      //第i个卷积核对应输入层第a个特征映射，输出层的第b个特征映射
</span><span class='line'>      //这个卷积核可以形象的看作是从输入层第a个特征映射到输出层的第b个特征映射的一个链接
</span><span class='line'>      int a=table[i][0], b=table[i][1];
</span><span class='line'>      //用第i个卷积核和输入层第a个特征映射做卷积
</span><span class='line'>      convolution = Conv(input[a],kernel[i]);
</span><span class='line'>      //把卷积结果求和
</span><span class='line'>      sum[b] +=convolution;
</span><span class='line'>  }
</span><span class='line'>  for (i=0;i&lt;(int)bias.size();i++) {
</span><span class='line'>      //加上偏移量
</span><span class='line'>      sum[i] += bias[i];
</span><span class='line'>  }
</span><span class='line'>  //调用Sigmoid函数
</span><span class='line'>  output = Sigmoid(sum);
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>

其中，input是 n1×n2×n3 的矩阵，n1是输入层特征映射的个数，n2是输入层特征映射的宽度，n3是输入层特征映射的高度。output, sum, convolution,
bias是n1×(n2-kw+1)×(n3-kh+1)的矩阵，kw,kh是卷积核的宽度高度(图中是5×5)。kernel是卷积核矩阵。table是连接表，即如果第a输入和第b个输出之间
有连接，table里就会有[a,b]这一项，而且每个连接都对应一个卷积核。</p>




<p>卷积层的反馈运算的核心代码如下：

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>ConvolutionLayer::bprop(input,output,in_dx,out_dx) {
</span><span class='line'>  //梯度通过DSigmoid反传
</span><span class='line'>  sum_dx = DSigmoid(out_dx);
</span><span class='line'>  //计算bias的梯度
</span><span class='line'>  for (i=0;i&lt;bias.size();i++)  {
</span><span class='line'>      bias_dx[i] = sum_dx[i];
</span><span class='line'>  }
</span><span class='line'>  //取得卷积核的个数
</span><span class='line'>  int n=kernel.GetDim(0);
</span><span class='line'>  for (int i=0;i&lt;n;i++)
</span><span class='line'>  {
</span><span class='line'>      int a=table[i][0],b=table[i][1];
</span><span class='line'>      //用第i个卷积核和第b个输出层反向卷积（即输出层的一点乘卷积模板返回给输入层），并把结果累加到第a个输入层
</span><span class='line'>      input_dx[a] += DConv(sum_dx[b],kernel[i]);
</span><span class='line'>      //用同样的方法计算卷积模板的梯度
</span><span class='line'>      kernel_dx[i] += DConv(sum_dx[b],input[a]);
</span><span class='line'>  }
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>

其中in_dx,out_dx 的结构和 input,output 相同，代表的是相应点的梯度。
</p>


<p></p>




<h3>3.2 子采样层的学习</h3>


<p>子采样层的典型结构如下图所示。
<center><img src="http://ibillxia.github.com/images/2013/IMAG2013040206.jpg"></center></p>




<p>类似的字采样层的输出的计算式为：</br>
<center>输出= Sigmoid( 采样*权重 +偏移量)</center>
其核心代码如下：

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>SubSamplingLayer::fprop(input,output) {
</span><span class='line'>  int n1= input.GetDim(0);
</span><span class='line'>  int n2= input.GetDim(1);
</span><span class='line'>  int n3= input.GetDim(2);
</span><span class='line'>  for (int i=0;i&lt;n1;i++) {
</span><span class='line'>      for (int j=0;j&lt;n2;j++) {
</span><span class='line'>          for (int k=0;k&lt;n3;k++) {
</span><span class='line'>              //coeff 是可训练的权重，sw 、sh 是采样窗口的尺寸。
</span><span class='line'>              sub[i][j/sw][k/sh] += input[i][j][k]*coeff[i];
</span><span class='line'>          }
</span><span class='line'>      }
</span><span class='line'>  }
</span><span class='line'>  for (i=0;i&lt;n1;i++) {
</span><span class='line'>      //加上偏移量
</span><span class='line'>      sum[i] = sub[i] + bias[i];
</span><span class='line'>  }
</span><span class='line'>  output = Sigmoid(sum);
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>

</p>




<p>子采样层的反馈运算的核心代码如下：

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>SubSamplingLayer::bprop(input,output,in_dx,out_dx) {
</span><span class='line'>  //梯度通过DSigmoid反传
</span><span class='line'>  sum_dx = DSigmoid(out_dx);
</span><span class='line'>  //计算bias和coeff的梯度
</span><span class='line'>  for (i=0;i&lt;n1;i++) {
</span><span class='line'>      coeff_dx[i] = 0;
</span><span class='line'>      bias_dx[i] = 0;
</span><span class='line'>      for (j=0;j&lt;n2/sw;j++)
</span><span class='line'>          for (k=0;k&lt;n3/sh;k++) {
</span><span class='line'>              coeff_dx[i] += sub[j][k]*sum_dx[i][j][k];
</span><span class='line'>              bias_dx[i] += sum_dx[i][j][k]);
</span><span class='line'>          }
</span><span class='line'>  }
</span><span class='line'>  for (i=0;i&lt;n1;i++) {
</span><span class='line'>      for (j=0;j&lt;n2;j++)
</span><span class='line'>          for (k=0;k&lt;n3;k++) {
</span><span class='line'>              in_dx[i][j][k] = coeff[i]*sum_dx[i][j/sw][k/sh];
</span><span class='line'>          }
</span><span class='line'>  }
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>

</p>




<h3>3.3 全连接层的学习</h3>


<p>全连接层的学习与传统的神经网络的学习方法类似，也是使用BP算法，这里就不详述了。</p>




<p>关于CNN的完整代码可以参考https://github.com/ibillxia/DeepLearnToolbox/tree/master/CNN中的Matlab代码。</p>




<h2>References</h2>


<p>[1] Learn Deep Architectures for AI, Chapter 4.5.</br>
[2] Deep Learning Tutorial, Release 0.1, Chapter 6.</br>
[3] Convolutional Networks for Images Speech and Time-Series.</br>
[4] 基于卷积网络的三维模型特征提取. 王添翼.</br>
[5] 卷积神经网络的研究及其在车牌识别系统中的应用. 陆璐.
</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[为什么要进行傅立叶变换]]></title>
    <link href="http://ibillxia.github.com/blog/2013/04/04/why-do-Fourier-transformation/"/>
    <updated>2013-04-04T10:42:00+08:00</updated>
    <id>http://ibillxia.github.com/blog/2013/04/04/why-do-Fourier-transformation</id>
    <content type="html"><![CDATA[<h2>一、傅立叶变换的由来</h2>


<p>关于傅立叶变换，无论是书本还是在网上可以很容易找到关于傅立叶变换的描述，但是大都是些故弄玄虚的文章，太过抽象，
尽是一些让人看了就望而生畏的公式的罗列，让人很难能够从感性上得到理解，最近，我偶尔从网上看到一个关于数字信号处理
的电子书籍，是一个叫Steven W. Smith, Ph.D.外国人写的，写得非常浅显，里面有七章由浅入深地专门讲述关于离散信号的傅
立叶变换，虽然是英文文档，我还是硬着头皮看完了有关傅立叶变换的有关内容，看了有茅塞顿开的感觉，在此把我从中得到的
理解拿出来跟大家分享，希望很多被傅立叶变换迷惑的朋友能够得到一点启发，这电子书籍是免费的，有兴趣的朋友也可以从网
上下载下来看一下，URL地址是：http://www.dspguide.com/pdfbook.htm </p>




<p>要理解傅立叶变换，确实需要一定的耐心，别一下子想着傅立叶变换是怎么变换的，当然，也需要一定的高等数学基础，最基本
的是级数变换，其中傅立叶级数变换是傅立叶变换的基础公式。</p>




<h2>二、傅立叶变换的提出</h2>


<p>让我们先看看为什么会有傅立叶变换？傅立叶是一位法国数学家和物理学家的名字，英语原名是Jean Baptiste Joseph Fourier(1768-1830), 
Fourier对热传递很感兴趣，于1807年在法国科学学会上发表了一篇论文，运用正弦曲线来描述温度分布，论文里有个在当时具有争议性的决断：
任何连续周期信号可以由一组适当的正弦曲线组合而成。当时审查这个论文的人，其中有两位是历史上著名的数学家拉格朗日(Joseph Louis 
Lagrange, 1736-1813)和拉普拉斯(Pierre Simon de Laplace, 1749-1827)，当拉普拉斯和其它审查者投票通过并要发表这个论文时，拉格朗日
坚决反对，在近50年的时间里，拉格朗日坚持认为傅立叶的方法无法表示带有棱角的信号，如在方波中出现非连续变化斜率。法国科学学会屈服
于拉格朗日的威望，拒绝了傅立叶的工作，幸运的是，傅立叶还有其它事情可忙，他参加了政治运动，随拿破仑远征埃及，法国大革命后因会被
推上断头台而一直在逃避。直到拉格朗日死后15年这个论文才被发表出来。</p>




<!--more-->




<p>谁是对的呢？拉格朗日是对的：正弦曲线无法组合成一个带有棱角的信号。但是，我们可以用正弦曲线来非常逼近地表示它，逼近到两种表示方法
不存在能量差别，基于此，傅立叶是对的。</p>




<p><b>为什么我们要用正弦曲线来代替原来的曲线呢？</b>如我们也还可以用方波或三角波来代替呀，分解信号的方法是无穷的，但<b>分解信号的目的是为了
更加简单地处理原来的信号。用正余弦来表示原信号会更加简单，因为正余弦拥有原信号所不具有的性质：正弦曲线保真度。</b>一个正弦曲线信号
输入后，输出的仍是正弦曲线，只有幅度和相位可能发生变化，但是频率和波的形状仍是一样的。且只有正弦曲线才拥有这样的性质，正因如此
我们才不用方波或三角波来表示。</p>




<h2>三、傅立叶变换分类</h2>


<p>根据原信号的不同类型，我们可以把傅立叶变换分为四种类别：</br>
1 非周期性连续信号 傅立叶变换（Fourier Transform）</br>
2 周期性连续信号 傅立叶级数(Fourier Series)</br>
3 非周期性离散信号 离散时域傅立叶变换（Discrete Time Fourier Transform）</br>
4 周期性离散信号 离散傅立叶变换(Discrete Fourier Transform)
</p>




<p>下图是四种原信号图例：
<center><img src="http://ibillxia.github.com/images/2013/IMAG2013040401.jpg"></center>
</p>




<p>这四种傅立叶变换都是针对正无穷大和负无穷大的信号，即信号的的长度是无穷大的，我们知道这对于计算机处理来说是不可能的，那么有没有
针对长度有限的傅立叶变换呢？没有。因为正余弦波被定义成从负无穷小到正无穷大，我们无法把一个长度无限的信号组合成长度有限的信号。面对
这种困难，方法是把长度有限的信号表示成长度无限的信号，可以把信号无限地从左右进行延伸，延伸的部分用零来表示，这样，这个信号就可以被
看成是非周期性离解信号，我们就可以用到离散时域傅立叶变换的方法。还有，也可以把信号用复制的方法进行延伸，这样信号就变成了周期性离解
信号，这时我们就可以用离散傅立叶变换方法进行变换。这里我们要学的是离散信号，对于连续信号我们不作讨论，因为计算机只能处理离散的数值
信号，我们的最终目的是运用计算机来处理信号的。</p>




<p>但是对于非周期性的信号，我们需要用无穷多不同频率的正弦曲线来表示，这对于计算机来说是不可能实现的。所以对于离散信号的变换只有离散傅立叶
变换（DFT）才能被适用，<b>对于计算机来说只有离散的和有限长度的数据才能被处理，对于其它的变换类型只有在数学演算中才能用到</b>，在计算机面前我们
只能用DFT方法，后面我们要理解的也正是DFT方法。这里要理解的是我们使用周期性的信号目的是为了能够用数学方法来解决问题，至于考虑周期性信号
是从哪里得到或怎样得到是无意义的。</p>




<p>每种傅立叶变换都分成实数和复数两种方法，对于实数方法是最好理解的，但是复数方法就相对复杂许多了，需要懂得有关复数的理论知识，不过，如果
理解了实数离散傅立叶变换(real DFT)，再去理解复数傅立叶就更容易了，所以我们先把复数的傅立叶放到一边去，先来理解实数傅立叶变换，在后面
我们会先讲讲关于复数的基本理论，然后在理解了实数傅立叶变换的基础上再来理解复数傅立叶变换。</p>




<p>还有，这里我们所要说的变换(transform)虽然是数学意义上的变换，但跟函数变换是不同的，函数变换是符合一一映射准则的，对于离散数字信号处理（DSP），
有许多的变换：傅立叶变换、拉普拉斯变换、Z变换、希尔伯特变换、离散余弦变换等，这些都扩展了函数变换的定义，允许输入和输出有多种的值，简单地
说变换就是把一堆的数据变成另一堆的数据的方法。</p>




<h2>四、傅立叶变换的物理意义</h2>


<p>傅立叶变换是数字信号处理领域一种很重要的算法。要知道傅立叶变换算法的意义，首先要了解傅立叶原理的意义。<b>傅立叶原理表明：任何连续测量的时序或信号，
都可以表示为不同频率的正弦波信号的无限叠加。</b>而根据该原理创立的傅立叶变换算法利用直接测量到的原始信号，以累加方式来计算该信号中不同正弦波信号的频率、
振幅和相位。</p>




<p>和傅立叶变换算法对应的是反傅立叶变换算法。该反变换从本质上说也是一种累加处理，这样就可以将单独改变的正弦波信号转换成一个信号。因此，可以说，
傅立叶变换将原来难以处理的时域信号转换成了易于分析的频域信号（信号的频谱），可以利用一些工具对这些频域信号进行处理、加工。最后还可以利用傅立叶
反变换将这些频域信号转换成时域信号。</p>




<p>从现代数学的眼光来看，傅里叶变换是一种特殊的积分变换。它能将满足一定条件的某个函数表示成正弦基函数的线性组合或者积分。在不同的研究领域，傅里叶
变换具有多种不同的变体形式，如连续傅里叶变换和离散傅里叶变换。</p>




<p>在数学领域，尽管最初傅立叶分析是作为热过程的解析分析的工具，但是其思想方法仍然具有典型的还原论和分析主义的特征。&#8221;任意&#8221;的函数通过一定的分解，
都能够表示为正弦函数的线性组合的形式，而正弦函数在物理上是被充分研究而相对简单的函数类：</br>
1. 傅立叶变换是线性算子,若赋予适当的范数,它还是酉算子;</br>
2. 傅立叶变换的逆变换容易求出,而且形式与正变换非常类似;</br>
3. 正弦基函数是微分运算的本征函数,从而使得线性微分方程的求解可以转化为常系数的代数方程的求解.在线性时不变杂的卷积运算为简单的乘积运算,从而提供了
计算卷积的一种简单手段;</br>
4. 离散形式的傅立叶的物理系统内,频率是个不变的性质,从而系统对于复杂激励的响应可以通过组合其对不同频率正弦信号的响应来获取;</br>
5. 著名的卷积定理指出:傅里叶变换可以化复杂的卷积运算为简单的乘积运算，从而利用数字计算机快速的算出(其算法称为快速傅立叶变换算法(FFT))。</p>




<p>正是由于上述的良好性质,傅里叶变换在物理学、数论、组合数学、信号处理、概率、统计、密码学、声学、光学等领域都有着广泛的应用。</p>




<h2>五、图像傅立叶变换的物理意义</h2>


<p>图像的频率是表征图像中灰度变化剧烈程度的指标，是灰度在平面空间上的梯度。如：大面积的沙漠在图像中是一片灰度变化缓慢的区域，对应的频率值很低；
而对于地表属性变换剧烈的边缘区域在图像中是一片灰度变化剧烈的区域，对应的频率值较高。傅立叶变换在实际中有非常明显的物理意义，设f是一个能量有限的
模拟信号，则其傅立叶变换就表示f的谱。从纯粹的数学意义上看，傅立叶变换是将一个函数转换为一系列周期函数来处理的。从物理效果看，傅立叶变换是将图像
从空间域转换到频率域，其逆变换是将图像从频率域转换到空间域。换句话说，傅立叶变换的物理意义是将图像的灰度分布函数变换为图像的频率分布函数，傅立叶
逆变换是将图像的频率分布函数变换为灰度分布函数。</p>




<p>傅立叶变换以前，图像（未压缩的位图）是由对在连续空间（现实空间）上的采样得到一系列点的集合，我们习惯用一个二维矩阵表示空间上各点，则图像可由z=f(x,y)
来表示。由于空间是三维的，图像是二维的，因此空间中物体在另一个维度上的关系就由梯度来表示，这样我们可以通过观察图像得知物体在三维空间中的对应关系。为什么
要提梯度？因为实际上对图像进行二维傅立叶变换得到频谱图，就是图像梯度的分布图，当然频谱图上的各点与图像上各点并不存在一一对应的关系，即使在不移频的情况
下也是没有。傅立叶频谱图上我们看到的明暗不一的亮点，实际上图像上某一点与邻域点差异的强弱，即梯度的大小，也即该点的频率的大小（可以这么理解，图像中的低
频部分指低梯度的点，高频部分相反）。一般来讲，梯度大则该点的亮度强，否则该点亮度弱。这样通过观察傅立叶变换后的频谱图，也叫功率图，我们首先就可以看出，
图像的能量分布，如果频谱图中暗的点数更多，那么实际图像是比较柔和的（因为各点与邻域差异都不大，梯度相对较小），反之，如果频谱图中亮的点数多，那么实际图
像一定是尖锐的，边界分明且边界两边像素差异较大的。对频谱移频到原点以后，可以看出图像的频率分布是以原点为圆心，对称分布的。将频谱移频到圆心除了可以清晰
地看出图像频率分布以外，还有一个好处，它可以分离出有周期性规律的干扰信号，比如正弦干扰，一副带有正弦干扰，移频到原点的频谱图上可以看出除了中心以外还存
在以某一点为中心，对称分布的亮点集合，这个集合就是干扰噪音产生的，这时可以很直观的通过在该位置放置带阻滤波器消除干扰。</p>




<p>另外我还想说明以下几点： </br>
1、图像经过二维傅立叶变换后，其变换系数矩阵表明：若变换矩阵Fn原点设在中心，其频谱能量集中分布在变换系数短阵的中心附近(图中阴影区)。若所用的二维傅立叶
变换矩阵Fn的原点设在左上角，那么图像信号能量将集中在系数矩阵的四个角上。这是由二维傅立叶变换本身性质决定的。同时也表明一股图像能量集中低频区域。 </br>
2 、变换之后的图像在原点平移之前四角是低频，最亮，平移之后中间部分是低频，最亮，亮度大说明低频的能量大（幅角比较大）。</p>




<h2>六、一个关于实数离散傅立叶变换(Real DFT)的例子</h2>


<p>先来看一个变换实例，一个原始信号的长度是16，于是可以把这个信号分解9个余弦波和9个正弦波（一个长度为N的信号可以分解成N/2+1个正余弦信号，这是为什么呢？
结合下面的18个正余弦图,我想从计算机处理精度上就不难理解，一个长度为N的信号，最多只能有N/2+1个不同频率，再多的频率就超过了计算机所能所处理的精度范围），
如下图，9个正弦信号：
<center><img src="http://ibillxia.github.com/images/2013/IMAG2013040402.jpg"></center>
9个余弦信号：
<center><img src="http://ibillxia.github.com/images/2013/IMAG2013040403.jpg"></center>
</p>




<p>把以上所有信号相加即可得到原始信号，至于是怎么分别变换出9种不同频率信号的，我们先不急，先看看对于以上的变换结果，在程序中又是该怎么表示的，我们可以看看
下面这个示例图：
<center><img src="http://ibillxia.github.com/images/2013/IMAG2013040404.jpg"></center>
</p>




<p>上图中左边表示时域中的信号，右边是频域信号表示方法，从左向右表示正向转换(Forward DFT)，从右向左表示逆向转换(Inverse DFT)，用小写x[]表示信号在每个时间点上
的幅度值数组, 用大写X[]表示每种频率的副度值数组, 因为有N/2+1种频率，所以该数组长度为N/2+1，X[]数组又分两种，一种是表示余弦波的不同频率幅度值：Re X[]，另
一种是表示正弦波的不同频率幅度值：Im X[]，Re是实数(Real)的意思，Im是虚数(Imagine)的意思，采用复数的表示方法把正余弦波组合起来进行表示，但这里我们不考虑复
数的其它作用，只记住是一种组合方法而已，目的是为了便于表达（在后面我们会知道，复数形式的傅立叶变换长度是N，而不是N/2+1）。</p>




<h2>七、用Matlab实现快速傅立叶变换</h2>


<p>FFT是离散傅立叶变换的快速算法，可以将一个信号变换到频域。有些信号在时域上是很难看出什么特征的，但是如果变换到频域之后，就很容易看出特征了。这就是很多
信号分析采用FFT变换的原因。另外，FFT可以将一个信号的频谱提取出来，这在频谱分析方面也是经常用的。 </p>




<p>虽然很多人都知道FFT是什么，可以用来做什么，怎么去做，但是却不知道FFT之后的结果是什意思、如何决定要使用多少点来做FFT。 </p>




<p>现在就根据实际经验来说说FFT结果的具体物理意义。一个模拟信号，经过ADC采样之后，就变成了数字信号。采样定理告诉我们，采样频率要大于信号频率的两倍，这些我就不在此啰嗦了。 </p>




<p>采样得到的数字信号，就可以做FFT变换了。N个采样点，经过FFT之后，就可以得到N个点的FFT结果。为了方便进行FFT运算，通常N取2的整数次方。 </p>




<p>假设采样频率为Fs，信号频率F，采样点数为N。那么FFT之后结果就是一个为N点的复数。每一个点就对应着一个频率点。这个点的模值，就是该频率值下的幅度特性。
具体跟原始信号的幅度有什么关系呢？假设原始信号的峰值为A，那么FFT的结果的每个点（除了第一个点直流分量之外）的模值就是A的N/2倍。而第一个点就是直流分量，
它的模值就是直流分量的N倍。而每个点的相位呢，就是在该频率下的信号的相位。第一个点表示直流分量（即0Hz），而最后一个点N的再下一个点（实际上这个点是不存在的，
这里是假设的第N+1个点，也可以看做是将第一个点分做两半分，另一半移到最后）则表示采样频率Fs，这中间被N-1个点平均分成N等份，每个点的频率依次增加。例如某点n所
表示的频率为：Fn=(n-1)*Fs/N。由上面的公式可以看出，Fn所能分辨到频率为为Fs/N，如果采样频率Fs为1024Hz，采样点数为1024点，则可以分辨到1Hz。1024Hz的采样率采样
1024点，刚好是1秒，也就是说，采样1秒时间的信号并做FFT，则结果可以分析到1Hz，如果采样2秒时间的信号并做FFT，则结果可以分析到0.5Hz。如果要提高频率分辨力，则必
须增加采样点数，也即采样时间。频率分辨率和采样时间是倒数关系。 </p>




<p>假设FFT之后某点n用复数a+bi表示，那么这个复数的模就是$An=\sqrt{a^{2}+b^{2}}$，相位就是$Pn=atan2(b,a)$。根据以上的结果，就可以计算出n点（n≠1，且n<=N/2）
对应的信号的表达式为：$An/(N/2)*cos(2* \pi *Fn*t+Pn)$，即$2*An/N*cos(2* \pi *Fn*t+Pn)$。对于n=1点的信号，是直流分量，幅度即为A1/N。由于FFT结果的对称性，
通常我们只使用前半部分的结果，即小于采样频率一半的结果。 </p>

<p>下面以一个实际的信号来做说明。假设我们有一个信号，它含有2V的直流分量，频率为50Hz、相位为-30度、幅度为3V的交流信号，以及一个频率为75Hz、相位为90度、幅度
为1.5V的交流信号。用数学表达式就是如下：$S=2+3*cos(2*\pi*50*t-\pi*30/180)+1.5*cos(2*\pi*75*t+\pi*90/180)$。式中cos参数为弧度，所以-30度和90度要分别换算成弧度。
我们以256Hz的采样率对这个信号进行采样，总共采样256点。按照我们上面的分析，Fn=(n-1)*Fs/N，我们可以知道，每两个点之间的间距就是1Hz，第n个点的频率就是n-1。
我们的信号有3个频率：0Hz、50Hz、75Hz，应该分别在第1个点、第51个点、第76个点上出现峰值，其它各点应该接近0。实际情况如何呢？我们来看看FFT的结果的模值如图
所示。
<center><img src="http://ibillxia.github.com/images/2013/IMAG2013040405.jpg"></center>
</p>

<p>从图中我们可以看到，在第1点、第51点、和第76点附近有比较大的值。我们分别将这三个点附近的数据拿上来细看： </br>
1点： 512+0i </br>
2点： -2.6195E-14 - 1.4162E-13i </br>
3点： -2.8586E-14 - 1.1898E-13i </br>
50点：-6.2076E-13 - 2.1713E-12i </br>
51点：332.55 - 192i </br>
52点：-1.6707E-12 - 1.5241E-12i </br>
75点：-2.2199E-13 -1.0076E-12i </br>
76点：3.4315E-12 + 192i </br>
77点：-3.0263E-14 +7.5609E-13i </br>
很明显，1点、51点、76点的值都比较大，它附近的点值都很小，可以认为是0，即在那些频率点上的信号幅度为0。接着，我们来计算各点的幅度值。分别计算这三个点的模值，结果如下： </br>
1点： 512 </br>
51点：384 </br>
76点：192 </br>
按照公式，可以计算出直流分量为：$512/N=512/256=2$；50Hz信号的幅度为：$384/(N/2)=384/(256/2)=3$；75Hz信号的幅度为$192/(N/2)=192/(256/2)=1.5$。可见，从频谱分析出来的幅度是正确的。</p>
 
<p>然后再来计算相位信息。直流信号没有相位可言，不用管它。先计算50Hz信号的相位，$atan2(-192, 332.55)=-0.5236$,结果是弧度，换算为角度就是$180*(-0.5236)/pi=-30.0001$。
再计算75Hz信号的相位，$atan2(192, 3.4315E-12)=1.5708$弧度，换算成角度就是$180*1.5708/pi=90.0002$。可见，相位也是对的。根据FFT结果以及上面的分析计算，我们就可以写出
信号的表达式了，它就是我们开始提供的信号。
<center><img src="http://ibillxia.github.com/images/2013/IMAG2013040406.jpg"></center>
</p>

<p><b>总结：假设采样频率为Fs，采样点数为N，做FFT之后，某一点n（n从1开始）表示的频率为：$Fn=(n-1)*Fs/N$；该点的模值除以N/2就是对应该频率下的信号的幅度（对于直流信号是除以N）；
该点的相位即是对应该频率下的信号的相位。相位的计算可用函数atan2(b,a)计算。atan2(b,a)是求坐标为(a,b)点的角度值，范围从-pi到pi。要精确到xHz，则需要采样长度为1/x秒的信号，
并做FFT。要提高频率分辨率，就需要增加采样点数</b>，这在一些实际的应用中是不现实的，需要在较短的时间内完成分析。解决这个问题的方法有频率细分法，比较简单的方法是采样比较短
时间的信号，然后在后面补充一定数量的0，使其长度达到需要的点数，再做FFT，这在一定程度上能够提高频率分辨力。具体的频率细分法可参考相关文献。</p>

<h2>八、 让傅立叶变换从理性蜕变到感性，从抽象升华到具体</h2>
<p>（应不少网友反应说以上7部分还是不够浅显而另加的一部分，希望对大家有所启发）</p>

<p>1、我们都知道，LTI系统对谐波函数的响应也是相同频率的谐波函数，只是幅度和相位可能不同罢了，因此我们用谐波函数来表示信号正是为了导出频域的概念。
那你就会问<b>为什么我们要在频域来分析信号，它比时域分析究竟好在哪里呢？</b>这个问题非常好，我来回答你，第一，在频域观察和分析信号有助于揭示系统的本质属性，
更重要的是对于某些系统可以极大地简化其设计和分析过程。这一点想必大家都知道，我不再啰嗦！第二，从数学上来看，系统从时域到频域的转换就意味着系统的微分
或差分方程将转变为代数方程，而系统的分析也将采用描述系统的复系数代数方程而不是微分或差分方程。既然如此，那么请问？童鞋，你是喜欢跟微分差分方程玩儿呢
还是喜欢跟代数方程玩儿呢？假若你说你更喜欢跟微分差分方程玩儿。那我也无话可说啦！</p>

<p>可能你还是觉得以上所述只是一个很理性的认识，那么接下来，满足你的感性需求。其实，在生活中，我们无时无刻不在进行着傅立叶变换。（什么？我没有听错吧？！）
对的，请相信你的耳朵，你完全没有听错。我们来看人类听觉系统的处理过程：当我们听到一个声音，大脑的实际反应是什么？事实上耳朵感觉到一个时变的空气压力，
这种变化也许是一个类似于口哨声的单音。当我们听到一个口哨声时，我们所关心的并不是气压随时间的振动（它非常非常快！），而是声音的三个特征：基音、声强以及
音长。基音可以理解为频率的同义词，声强不是别的，它就是幅度。我们的耳朵—大脑系统能有效地将信号表示成三个简单的特征参数：基音、声强以及音长，并不理会气压
的快速变化过程（一个重复的变化过程）。这样耳朵—大脑系统就提取了信号的本质信息。傅立叶变换的分析过程与此类似，只不过我们从数学意义把它更加精确化和专业话罢了。</p>

<p>2、不要把傅立叶变换想得那么高深莫测，其实它就是对傅立叶级数的一种拓展。我们知道，傅立叶级数能描述无限时间的周期信号。那么，傅立叶级数能不能描述某些特殊的
无限时间的非周期信号呢？答案是，不能。但我们经常要分析处理这样的信号啊！于是傅立叶变换这个家伙现身啦！傅立叶变换就是为了使傅立叶级数能够描述所有（没错！
就是所有！）周期和非周期的无限时间信号而导出的，因而傅立叶变换是对傅立叶级数的一种拓展。</p>

<p>可能你还是觉得以上所述只是一个很抽象的认识，那么接下来，满足你的具体需求。我们先不管是怎么进行拓展的。我们先关注另外两个概念：周期信号和非周期信号。
他们的显著区别就在于：周期信号每隔一个有限的时间即基波周期To重复一次。它自始至终都将以这个基波周期To重复。而非周期信号则没有一个确定的或固定的周期，
可能在一段时间内他将重复某一段波形很多次，但不会在整个无限长时间范围都如此。我们找到一个周期信号的傅立叶级数，然后让这个信号的基波周期趋于无限，就
完成了从傅立叶级数到傅立叶变换的演变过程。因为当周期信号的基波周期趋于无限时，它的波形在有限长时间内都不会重复，这时它就不具有周期性啦！也就是说，
说一个信号具有无限长的周期和说它是一个非周期信号实际上是一回事！</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[反向传播(BP)神经网络]]></title>
    <link href="http://ibillxia.github.com/blog/2013/03/30/back-propagation-neural-networks/"/>
    <updated>2013-03-30T21:37:00+08:00</updated>
    <id>http://ibillxia.github.com/blog/2013/03/30/back-propagation-neural-networks</id>
    <content type="html"><![CDATA[<p>前面几篇文章中对神经网络和深度学习进行一些简介，包括神经网络的发展历史、基本概念和常见的几种神经网络以及神经网络的学习方法等，
本文具体来介绍一下一种非常常见的神经网络模型——反向传播(Back Propagation)神经网络。</p>




<h2>1.概述</h2>


<p>BP（Back Propagation）神经网络是1986年由Rumelhart和McCelland为首的科研小组提出，参见他们发表在Nature上的论文
<em><a href="http://www.cs.toronto.edu/~hinton/absps/naturebp.pdf">Learning representations by back-propagating errors</a></em>
值得一提的是，该文的第三作者Geoffrey E. Hinton就是在深度学习邻域率先取得突破的神犇。
</p>




<p>BP神经网络是一种按误差逆传播算法训练的多层前馈网络，是目前应用最广泛的神经网络模型之一。BP网络能学习和存贮大量的
输入-输出模式映射关系，而无需事前揭示描述这种映射关系的数学方程。它的学习规则是使用最速下降法，通过反向传播来不断
调整网络的权值和阈值，使网络的误差平方和最小。</p>




<!-- more -->




<h2>2.BP网络模型</h2>


<p>一个典型的BP神经网络模型如图1所示。</br>
<center><img src="http://ibillxia.github.com/images/2013/IMAG2013033001.jpg"></center>
<center>图1 典型的BP神经网络模型</center></p>




<p>BP神经网络与其他神经网络模型类似，不同的是，BP神经元的传输函数为非线性函数(而在感知机中为阶跃函数，在线性神经网络中为线性函数)，最常用的
是log-sigmoid函数或tan-sigmoid函数。BP神经网络(BPNN)一般为多层神经网络，图1中所示的BP神经网络的隐层的传输函数即为非线性函数，隐层可以有多层，
而输出层的传输函数为线性函数，当然也可以是非线性函数，只不过线性函数的输出结果取值范围较大，而非线性函数则限制在较小范围（如logsig函数输出
取值在(0,1)区间）。图1所示的神经网络的输入输出关系如下：</br>
1)输入层与隐层的关系：</br>
<center>$\boldsymbol{h} = \mathit{f_{1}} (\boldsymbol{W^{(1)}x}+\boldsymbol{b^{(1)}})$.</center>
其中$\boldsymbol{x}$为$m$维特征向量(列向量)，$\boldsymbol{W^{(1)}}$为$n × m$维权值矩阵，$\boldsymbol{b^{(1)}}$为$n$维的偏置(bias)向量(列向量)。</br>
2)隐层与输出层的关系：</br>
<center>$\boldsymbol{y} = \mathit{f_{2}} (\boldsymbol{W^{(2)}h}+\boldsymbol{b^{(2)}})$.</center>
</p>




<h2>3.BP网络的学习方法</h2>


<p>神经网络的关键之一是权值的确定，也即神经网络的学习，下面主要讨论一下BP神经网络的学习方法，它是一种监督学习的方法。</br>
假定我们有$q$个带label的样本(即输入)$p_{1},p_{2},&#8230;,p_{q}$，对应的label(即期望输出Target)为$T_{1},T_{2},&#8230;,T_{q}$，神经网络的实际输出
为$a2_{1},a2_{2},&#8230;,a2_{q}$，隐层的输出为$a1[.]$那么可以定义误差函数：</br>
<center>$\boldsymbol{E(W,B)} = \frac{1}{2}\sum_{k=1}^{n}(t_{k} - a2_{k})^{2} $.</center>
BP算法的目标是使得实际输出approximate期望输出，即使得训练误差最小化。BP算法利用梯度下降(Gradient Descent)法来求权值的变化及
误差的反向传播。对于图1中的BP神经网络，我们首先计算输出层的权值的变化量，从第$i$个输入到第$k$个输出的权值改变为：</br>
<center>$\Delta w2_{ki} = - \eta \frac{\partial E}{\partial w2_{ki}} &#92;
= - \eta \frac{\partial E}{\partial a2_{k}} \frac{\partial a2_{k}}{\partial w2_{ki}} &#92;
= \eta (t_{k}-a2_{k})f_{2}&#8217;a1_{i} = \eta \delta_{ki}a1_{i}$.</center>
其中$\eta$为学习速率。同理可得：</br>
<center>$\Delta b2_{ki} = - \eta \frac{\partial E}{\partial b2_{ki}} 
= - \eta \frac{\partial E}{\partial a2_{k}} \frac{\partial a2_{k}}{\partial b2_{ki}}
= \eta (t_{k}-a2_{k})f_{2}&#8217; = \eta \delta_{ki}$.</center>
而隐层的权值变化为：</br>
<center>$\Delta w1_{ij} = - \eta \frac{\partial E}{\partial w1_{ij}} 
= - \eta \frac{\partial E}{\partial a2_{k}} \frac{\partial a2_{k}}{\partial a1_{i}} \frac{\partial a1_{i}}{\partial w1_{ij}}
= \eta \sum_{k=1}^{n}(t_{k}-a2_{k})f_{2}&#8217;w2_{ki}f_{1}&#8217;p_{j} = \eta \delta_{ij}p_{j}$.</center>
其中，$\delta_{ij} = e_{i}f_{1}&#8217;, e_{i} = \sum_{k=1}^{n}\delta_{ki}w2_{ki}$</br>
同理可得，$\Delta b1_{i} = \eta \delta_{ij}$。</br>
这里我们注意到，输出层的误差为$e_{j},j=1..n$，隐层的误差为$e_{i},i=1..m$，其中$e_{i}$可以认为是$e_{j}$的加权组合，由于作用函数的
存在，$e_{j}$的等效作用为$\delta_{ji} = e_{j}f&#8217;()$。
</p>




<h2>4.BP网络的设计</h2>


<p>在进行BP网络的设计是，一般应从网络的层数、每层中的神经元个数和激活函数、初始值以及学习速率等几个方面来进行考虑，下面是一些选取的原则。</p>




<p><strong>1.网络的层数</strong></br>
理论已经证明，具有偏差和至少一个S型隐层加上一个线性输出层的网络，能够逼近任何有理函数，增加层数可以进一步降低误差，提高精度，但同时也是网络
复杂化。另外不能用仅具有非线性激活函数的单层网络来解决问题，因为能用单层网络解决的问题，用自适应线性网络也一定能解决，而且自适应线性网络的
运算速度更快，而对于只能用非线性函数解决的问题，单层精度又不够高，也只有增加层数才能达到期望的结果。
</p>




<p><strong>2.隐层神经元的个数</strong></br>
网络训练精度的提高，可以通过采用一个隐含层，而增加其神经元个数的方法来获得，这在结构实现上要比增加网络层数简单得多。一般而言，我们用精度和
训练网络的时间来恒量一个神经网络设计的好坏：</br>
（1）神经元数太少时，网络不能很好的学习，训练迭代的次数也比较多，训练精度也不高。</br>
（2）神经元数太多时，网络的功能越强大，精确度也更高，训练迭代的次数也大，可能会出现过拟合(over fitting)现象。</br>
由此，我们得到神经网络隐层神经元个数的选取原则是：在能够解决问题的前提下，再加上一两个神经元，以加快误差下降速度即可。
</p>




<p><strong>3.初始权值的选取</strong></br>
一般初始权值是取值在$(-1,1)$之间的随机数。另外威得罗等人在分析了两层网络是如何对一个函数进行训练后，提出选择初始权值量级为$\sqrt[r]{s}$的策略，
其中$r$为输入个数，$s$为第一层神经元个数。
</p>




<p><strong>4.学习速率</strong></br>
学习速率一般选取为$0.01 - 0.8$，大的学习速率可能导致系统的不稳定，但小的学习速率导致收敛太慢，需要较长的训练时间。对于较复杂的网络，
在误差曲面的不同位置可能需要不同的学习速率，为了减少寻找学习速率的训练次数及时间，比较合适的方法是采用变化的自适应学习速率，使网络在
不同的阶段设置不同大小的学习速率。
</p>




<p><strong>5.期望误差的选取</strong></br>
在设计网络的过程中，期望误差值也应当通过对比训练后确定一个合适的值，这个合适的值是相对于所需要的隐层节点数来确定的。一般情况下，可以同时对两个不同
的期望误差值的网络进行训练，最后通过综合因素来确定其中一个网络。
</p>




<h2>5.BP网络的局限性</h2>


<p>BP网络具有以下的几个问题：</br>
<strong>(1)需要较长的训练时间</strong>：这主要是由于学习速率太小所造成的，可采用变化的或自适应的学习速率来加以改进。</br>
<strong>(2)完全不能训练</strong>：这主要表现在网络的麻痹上，通常为了避免这种情况的产生，一是选取较小的初始权值，而是采用较小的学习速率。</br>
<strong>(3)局部最小值</strong>：这里采用的梯度下降法可能收敛到局部最小值，采用多层网络或较多的神经元，有可能得到更好的结果。
</p>




<h2>6.BP网络的改进</h2>


<p>BP算法改进的主要目标是加快训练速度，避免陷入局部极小值等，常见的改进方法有带动量因子算法、自适应学习速率、变化的学习速率以及作用函数后缩法等。
动量因子法的基本思想是在反向传播的基础上，在每一个权值的变化上加上一项正比于前次权值变化的值，并根据反向传播法来产生新的权值变化。而自适应学习
速率的方法则是针对一些特定的问题的。改变学习速率的方法的原则是，若连续几次迭代中，若目标函数对某个权倒数的符号相同，则这个权的学习速率增加，
反之若符号相反则减小它的学习速率。而作用函数后缩法则是将作用函数进行平移，即加上一个常数。</p>




<h2>7.BP网络实现异或</h2>


<p>见参考文献[7]或Andrew Ng. 的ML公开课的第8讲。</p>


<p>另外BP算法的讲解及C++实现参见[4]。</p>




<h2>参考文献</h2>


<p>[1]An Introduction to Back-Propagation Neural Networks: http://www.seattlerobotics.org/encoder/nov98/neural.html</br>
[2]Wiki - Backpropagation: http://en.wikipedia.org/wiki/Backpropagation</br>
[3]Chapter 7 The backpropagation algorithm of Neural Networks - A Systematic Introduction by Raúl Rojas: http://page.mi.fu-berlin.de/rojas/neural/chapter/K7.pdf</br>
[4]Back-propagation Neural Net - C++ 实现: http://www.codeproject.com/Articles/13582/Back-propagation-Neural-Net</br>
[5]《Visual C++数字图像模式识别技术及工程实践》(第3章)，求实科技 张宏林</br>
[6]《Matlab神经网络设置及应用》(第5章)，周品，清华大学出版社
</p>

]]></content>
  </entry>
  
</feed>
