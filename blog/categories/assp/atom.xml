<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: ASSP | Bill's Blog]]></title>
  <link href="http://ibillxia.github.com/blog/categories/assp/atom.xml" rel="self"/>
  <link href="http://ibillxia.github.com/"/>
  <updated>2013-05-16T23:12:14+08:00</updated>
  <id>http://ibillxia.github.com/</id>
  <author>
    <name><![CDATA[Bill Xia]]></name>
    <email><![CDATA[ibillxia@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[语音信号处理之时域分析-音高及其Python实现]]></title>
    <link href="http://ibillxia.github.com/blog/2013/05/16/audio-signal-processing-time-domain-pitch-python-realization/"/>
    <updated>2013-05-16T23:10:00+08:00</updated>
    <id>http://ibillxia.github.com/blog/2013/05/16/audio-signal-processing-time-domain-pitch-python-realization</id>
    <content type="html"><![CDATA[<h2>音高（Pitch）</h2>


<p>概念：音高（Pitch）是语音信号的一个很重要的特征，直觉上而言它表示声音频率的高低，这个频率是指基本频率（基频），也即基本周期的倒数。
若直接观察语音的波形，只要语音信号稳定，我们可以很容易的看出基本周期的存在。例如我们取一个包含256个采样点的帧，单独绘制波形图，就可以明显的
看到它的基本周期。如下图所示：
<center><img src="/images/2013/IMAG2013051601.png"></center>
其中最上面的波形为|a|的发音，中间的为上图中第一个红色双竖线（位于语音区）所对应的帧的具体波形，而最下面的是
上图中第二个红色双竖线（位于静音区）所对应的帧的具体波形。很容易看到中间的波形具有明显的周期性。
</p>


<!--more-->


<p>其代码如下：
<div class='bogus-wrapper'><notextile><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>import math
</span><span class='line'>import wave
</span><span class='line'>import numpy as np
</span><span class='line'>import pylab as pl
</span><span class='line'>
</span><span class='line'># read wave file and get parameters.
</span><span class='line'>fw = wave.open('a.wav','rb')
</span><span class='line'>params = fw.getparams()
</span><span class='line'>print(params)
</span><span class='line'>nchannels, sampwidth, framerate, nframes = params[:4]
</span><span class='line'>str_data = fw.readframes(nframes)
</span><span class='line'>wave_data = np.fromstring(str_data, dtype=np.short)
</span><span class='line'>wave_data.shape = -1, 1
</span><span class='line'>fw.close()
</span><span class='line'>
</span><span class='line'># plot the wave
</span><span class='line'>time = np.arange(0, len(wave_data)) * (1.0 / framerate)
</span><span class='line'>index1 = 10000.0 / framerate
</span><span class='line'>index2 = 10512.0 / framerate
</span><span class='line'>index3 = 15000.0 / framerate
</span><span class='line'>index4 = 15512.0 / framerate
</span><span class='line'>pl.subplot(311)
</span><span class='line'>pl.plot(time, wave_data)
</span><span class='line'>pl.plot([index1,index1],[-30000,30000],'r')
</span><span class='line'>pl.plot([index2,index2],[-30000,30000],'r')
</span><span class='line'>pl.plot([index3,index3],[-30000,30000],'r')
</span><span class='line'>pl.plot([index4,index4],[-30000,30000],'r')
</span><span class='line'>pl.ylabel("Amplitude")
</span><span class='line'>pl.xlabel("time (seconds)")
</span><span class='line'>pl.subplot(312)
</span><span class='line'>pl.plot(np.arange(512),wave_data[10000:10512])
</span><span class='line'>pl.subplot(313)
</span><span class='line'>pl.plot(np.arange(512),wave_data[15000:15512])
</span><span class='line'>pl.show()</span></code></pre></td></tr></table></div></figure></notextile></div>
</p>




<p>根据参考[1]，可以通过观察一帧的波形图来计算基音频率（感觉这种方法有点奇葩，不过很直观），然后还可以计算半音（semitone，可以参见[2]），
进而得到pitch与semitone的关系。[1]中还提到了钢琴的半音差，DS表示完全看不懂啊，有木有！！！</p>




<p>参考[2]中还简单介绍了如何改变音高、扩展音域，以及如何改变乐器的振动的弦的音高（通过改变弦长、张力、密度等），感兴趣的可以看看。</p>




<p>另外，由于生理结构的差异，男女性的音高范围不尽相同，一般而言：</br>
男性的音高范围是35~72半音，对应的频率范围是62~523Hz；</br>
女性的音高范围是45~83半音，对应的频率范围是110~1000Hz。</br>
然而，我们分辨男女的声音并不是只根据音高，还要根据音色（也即共振峰，下一篇文章中将详细介绍）。
</p>




<p>关于音高的计算，目前有很多种算法，具体将会在后续文章中详细介绍。</p>




<h2>参考（References）</h2>


<p>
[1]Pitch (音高): http://neural.cs.nthu.edu.tw/jang/books/audiosignalprocessing/basicFeaturePitch.asp</br>
[2]Wiki： http://zh.wikipedia.org/wiki/音高
</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[语音信号处理之时域分析-过零率及其Python实现]]></title>
    <link href="http://ibillxia.github.com/blog/2013/05/15/audio-signal-processing-time-domain-ZeroCR-python-realization/"/>
    <updated>2013-05-15T21:44:00+08:00</updated>
    <id>http://ibillxia.github.com/blog/2013/05/15/audio-signal-processing-time-domain-ZeroCR-python-realization</id>
    <content type="html"><![CDATA[<h2>过零率（Zero Crossing Rate）</h2>


<p>概念：过零率（Zero Crossing Rate，ZCR）是指在每帧中，语音信号通过零点（从正变为负或从负变为正）的次数。
这个特征已在语音识别和音乐信息检索领域得到广泛使用，是对敲击的声音的分类的关键特征。</p>




<p>ZCR的数学形式化定义为：
<center>$zcr = \frac{1}{T-1}\sum_{t=1}^{T-1}\pi\{s_{t}s_{t-1}<0\}$.</center>
其中$s$是采样点的值，$T$为帧长，函数$\pi\{A\}$在A为真是值为1，否则为0.
</p>




<p>特性：</br>
(1).一般而言，清音（unvoiced sound）和环境噪音的ZCR都大于浊音（voiced sound）；</br>
(2).由于清音和环境噪音的ZCR大小相近，因而不能够通过ZCR来区分它们；</br>
(3).在实际当中，过零率经常与短时能量特性相结合来进行端点检测，尤其是ZCR用来检测清音的起止点；</br>
(4).有时也可以用ZCR来进行粗略的基频估算，但这是非常不可靠的，除非有后续的修正（refine）处理过程。
</p>




<!--more-->




<h2>ZCR的Python实现</h2>


<p>ZCR的Python实现如下：
<div class='bogus-wrapper'><notextile><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>import math
</span><span class='line'>import numpy as np
</span><span class='line'>
</span><span class='line'>def ZeroCR(waveData,frameSize,overLap):
</span><span class='line'>    wlen = len(waveData)
</span><span class='line'>    step = frameSize - overLap
</span><span class='line'>    frameNum = math.ceil(wlen/step)
</span><span class='line'>    zcr = np.zeros((frameNum,1))
</span><span class='line'>    for i in range(frameNum):
</span><span class='line'>        curFrame = waveData[np.arange(i*step,min(i*step+frameSize,wlen))]
</span><span class='line'>        #To avoid DC bias, usually we need to perform mean subtraction on each frame
</span><span class='line'>        #ref: http://neural.cs.nthu.edu.tw/jang/books/audiosignalprocessing/basicFeatureZeroCrossingRate.asp
</span><span class='line'>        curFrame = curFrame - np.mean(curFrame) # zero-justified
</span><span class='line'>        zcr[i] = sum(curFrame[0:-1]*curFrame[1::]&lt;=0)
</span><span class='line'>    return zcr</span></code></pre></td></tr></table></div></figure></notextile></div>
</p>

<p>对于给定语音文件aeiou.wav，利用上面的函数计算ZCR的代码如下：
<div class='bogus-wrapper'><notextile><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>import math
</span><span class='line'>import wave
</span><span class='line'>import numpy as np
</span><span class='line'>import pylab as pl
</span><span class='line'>
</span><span class='line'># ============ test the algorithm =============
</span><span class='line'># read wave file and get parameters.
</span><span class='line'>fw = wave.open('aeiou.wav','rb')
</span><span class='line'>params = fw.getparams()
</span><span class='line'>print(params)
</span><span class='line'>nchannels, sampwidth, framerate, nframes = params[:4]
</span><span class='line'>str_data = fw.readframes(nframes)
</span><span class='line'>wave_data = np.fromstring(str_data, dtype=np.short)
</span><span class='line'>wave_data.shape = -1, 1
</span><span class='line'>#wave_data = wave_data.T
</span><span class='line'>fw.close()
</span><span class='line'>
</span><span class='line'># calculate Zero Cross Rate
</span><span class='line'>frameSize = 256
</span><span class='line'>overLap = 0
</span><span class='line'>zcr = ZeroCR(wave_data,frameSize,overLap)
</span><span class='line'>
</span><span class='line'># plot the wave
</span><span class='line'>time = np.arange(0, len(wave_data)) * (1.0 / framerate)
</span><span class='line'>time2 = np.arange(0, len(zcr)) * (len(wave_data)/len(zcr) / framerate)
</span><span class='line'>pl.subplot(211)
</span><span class='line'>pl.plot(time, wave_data)
</span><span class='line'>pl.ylabel("Amplitude")
</span><span class='line'>pl.subplot(212)
</span><span class='line'>pl.plot(time2, zcr)
</span><span class='line'>pl.ylabel("ZCR")
</span><span class='line'>pl.xlabel("time (seconds)")
</span><span class='line'>pl.show()</span></code></pre></td></tr></table></div></figure></notextile></div>
</p>

<p>运行以上程序得到下图：
<center><img src="/images/2013/IMAG2013051502.png"></center>
</p>

<h2>参考（References）</h2>
<p>
[1]Zero Crossing Rate (過零率): http://neural.cs.nthu.edu.tw/jang/books/audiosignalprocessing/basicFeatureZeroCrossingRate.asp?title=5-3%20Zero%20Crossing%20Rate%20(%B9L%B9s%B2v)&language=english</br>
[2]Wiki: http://zh.wikipedia.org/zh/过零率
</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[语音信号处理之时域分析-音量及其Python实现]]></title>
    <link href="http://ibillxia.github.com/blog/2013/05/15/audio-signal-process-time-domain-volume-python-realization/"/>
    <updated>2013-05-15T19:36:00+08:00</updated>
    <id>http://ibillxia.github.com/blog/2013/05/15/audio-signal-process-time-domain-volume-python-realization</id>
    <content type="html"><![CDATA[<h2>1.概述（Introduction）</h2>


<p>本系列文主要介绍语音信号时域的4个基本特征及其Python实现，这4个基本特征是：</br>
(1)音量（Volume）；</br>
(2)过零率（Zero-Crossing-Rate）；</br>
(3)音高（Pitch）；</br>
(4)音色（Timbre）。
</p>




<h2>2.音量（Volume）</h2>


<p>音量代表声音的强度，可由一个窗口或一帧内信号振幅的大小来衡量，一般有两种度量方法：</br>
（1）每个帧的振幅的绝对值的总和：
<center>$volume = \sum_{i=1}^{n}|s_{i}|$.</center>
其中$s_{i}$为第该帧的$i$个采样点，$n$为该帧总的采样点数。这种度量方法的计算量小，但不太符合人的听觉感受。</br>
（2）幅值平方和的常数对数的10倍：
<center>$volume = 10 * log_{10}\sum_{i=1}^{n}s_{i}^{2}$.</center>
它的单位是分贝（Decibels），是一个对数强度值，比较符合人耳对声音大小的感觉，但计算量稍复杂。
</p>


<!--more-->


<p>音量计算的Python实现如下：</p>


<p><div class='bogus-wrapper'><notextile><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>import math
</span><span class='line'>import numpy as np&lt;/p>
</span><span class='line'>
</span><span class='line'>&lt;h1>definition 1&lt;/h1>
</span><span class='line'>
</span><span class='line'>&lt;p>def calVolume(waveData, frameSize, overLap):&lt;/p>
</span><span class='line'>
</span><span class='line'>&lt;pre>&lt;code>wlen = len(waveData)
</span><span class='line'>step = frameSize - overLap
</span><span class='line'>frameNum = math.ceil(wlen/step)
</span><span class='line'>volume = np.zeros((frameNum,1))
</span><span class='line'>for i in range(frameNum):
</span><span class='line'>    curFrame = waveData[np.arange(i*step,min(i*step+frameSize,wlen))]
</span><span class='line'>    curFrame = curFrame - curFrame.mean() # zero-justified
</span><span class='line'>    volume[i] = sum(abs(curFrame))
</span><span class='line'>return volume
</span><span class='line'>&lt;/code>&lt;/pre>
</span><span class='line'>
</span><span class='line'>&lt;h1>definition 2&lt;/h1>
</span><span class='line'>
</span><span class='line'>&lt;p>def calVolumeDB(waveData, frameSize, overLap):&lt;/p>
</span><span class='line'>
</span><span class='line'>&lt;pre>&lt;code>wlen = len(waveData)
</span><span class='line'>step = frameSize - overLap
</span><span class='line'>frameNum = math.ceil(wlen/step)
</span><span class='line'>volume = np.zeros((frameNum,1))
</span><span class='line'>for i in range(frameNum):
</span><span class='line'>    curFrame = waveData[np.arange(i*step,min(i*step+frameSize,wlen))]
</span><span class='line'>    curFrame = curFrame - curFrame.mean() # zero-justified
</span><span class='line'>    curFrame.shape = -1
</span><span class='line'>    sumFrame = np.dot(curFrame,curFrame)
</span><span class='line'>    volume[i] = 10*np.log10(sumFrame)
</span><span class='line'>return volume
</span><span class='line'>&lt;/code>&lt;/pre>
</span><span class='line'>
</span><span class='line'>&lt;p></span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p>对于给定语音文件aeiou.wav，利用上面的函数计算音量曲线的代码如下：</p>


<p><div class='bogus-wrapper'><notextile><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>import wave
</span><span class='line'>import math
</span><span class='line'>import pylab as pl
</span><span class='line'>import numpy as np&lt;/p>
</span><span class='line'>
</span><span class='line'>&lt;h1>============ test the algorithm =============&lt;/h1>
</span><span class='line'>
</span><span class='line'>&lt;h1>read wave file and get parameters.&lt;/h1>
</span><span class='line'>
</span><span class='line'>&lt;p>fw = wave.open('aeiou.wav','rb')
</span><span class='line'>params = fw.getparams()
</span><span class='line'>print(params)
</span><span class='line'>nchannels, sampwidth, framerate, nframes = params[:4]
</span><span class='line'>str_data = fw.readframes(nframes)
</span><span class='line'>wave_data = np.fromstring(str_data, dtype=np.short)
</span><span class='line'>wave_data.shape = -1, 1&lt;/p>
</span><span class='line'>
</span><span class='line'>&lt;h1>wave_data = wave_data.T&lt;/h1>
</span><span class='line'>
</span><span class='line'>&lt;p>fw.close()&lt;/p>
</span><span class='line'>
</span><span class='line'>&lt;h1>calculate volume&lt;/h1>
</span><span class='line'>
</span><span class='line'>&lt;p>frameSize = 256
</span><span class='line'>overLap = 128
</span><span class='line'>volume11 = calVolume(wave_data,frameSize,overLap)
</span><span class='line'>volume12 = calVolumeDB(wave_data,frameSize,overLap)&lt;/p>
</span><span class='line'>
</span><span class='line'>&lt;h1>plot the wave&lt;/h1>
</span><span class='line'>
</span><span class='line'>&lt;p>time = np.arange(0, nframes) * (1.0 / framerate)
</span><span class='line'>time2 = np.arange(0, len(volume11)) * (1.0 / framerate)
</span><span class='line'>pl.subplot(311)
</span><span class='line'>pl.plot(time, wave_data)
</span><span class='line'>pl.ylabel("Amplitude")
</span><span class='line'>pl.subplot(312)
</span><span class='line'>pl.plot(time2, volume11)
</span><span class='line'>pl.ylabel("adsSum")
</span><span class='line'>pl.subplot(313)
</span><span class='line'>pl.plot(time2, volume12, c="g")
</span><span class='line'>pl.ylabel("Decibel")
</span><span class='line'>pl.xlabel("time (seconds)")
</span><span class='line'>pl.show()</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p>运行以上程序得到下图：
<center><img src="/images/2013/IMAG2013051501.png"></center>
</p>




<h2>参考（References）</h2>


<p>[1]Volume (音量):http://neural.cs.nthu.edu.tw/jang/books/audiosignalprocessing/basicFeatureVolume.asp?title=5-2%20Volume%20(%AD%B5%B6q)</br>
[2]用Python做科学计算-声音的输入输出:http://hyry.dip.jp:8000/pydoc/wave_pyaudio.html</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[语音信号处理基础学习笔记之时域处理]]></title>
    <link href="http://ibillxia.github.com/blog/2013/05/08/speech-processing-in-time-domain/"/>
    <updated>2013-05-08T23:13:00+08:00</updated>
    <id>http://ibillxia.github.com/blog/2013/05/08/speech-processing-in-time-domain</id>
    <content type="html"><![CDATA[<p>语音信号的分析分为时域、频域、倒谱域等，时域分析简单、运算量小、物理意义明确，但对于语音识别而言，
更为有效的是频域的分析方法，那么为什么还有进行时域的分析呢？</p>




<p>语音信号具有时变特性，但在短时内可以看做是平稳的，所以语音的时域分析是建立在“短时”的条件下的，经研究统计，
语音信号在帧长为10ms~30ms内是相对平稳的。</p>




<p>语音信号是模拟信号，在进行处理之前，要进行数字化，模拟信号数字化的一般方法是采样，按照Nyquist采样定理进行
采样（一般在8K~10KHz）后，在进行量化（一般用8bit，也有16bit等）和编码，变为数字信号。</p>




<p>在语音信号数字化之后，就可以开始对其进行处理了，首先是预处理，由于语音信号的平均功率谱受声门激励和口鼻辐射的影响，
高频端大约在800Hz以上按6dB/倍频程跌落，为此要在预处理中进行预加重。预加重的目的是提升高频部分，是信号变得平坦，
以便于进行频谱分析或声道参数分析。预加重可以用具有6dB/倍频程的提升高频特性的预加重数字滤波器实现。预处理的另一
方面工作是分帧和加窗：分帧的帧长一般在10ms~30ms，分帧既可以是连续的，也可以是有部分over-lap；短时分析的实质是
对信号加窗，一般采用Hamming窗，其他的还有矩形窗、汉宁窗等，如下图所示。
<center><img src="/images/2013/IMAG2013050801.png"></center>
</p>




<!--more-->




<p>好了，经过预处理之后就可以真正开始进行时域分析了，这里的时域分析主要包含短时平均能量、短时过零分析、短时自相
关分析以及高阶统计量分析等。</p>




<p>短时平均能量（Short Time Average Energy）可以理解为先计算信号格采样值的平方，然后用一个移动窗h(n-m)选取出一个个
短时平方序列，并将各段的平方值求和，从而得到短时能量序列。短时平均能量（En）可以用来从清音中区分浊音（浊音的En比
清音大得多），可以用来确定声母和韵母、无声与有声、连字等的分界，还可以作为一种超音段信息用于语音识别。但短时平均
能量En对于高电平信号可能产生溢出，此时可以采用短时平均幅度（Short Time Average Magnitude）来度量语音信号幅度的变化。</p>




<p>信号的幅度值从正值到负值要经过零点，从负值到正值也要经过零点，称为过零，统计信号在单位时间（如1s）内过零的次数，
就成为过零率。如果信号按段分割，就成为短时，把各段信号的过零率做统计平均，就是短时平均过零率（Short Time Average Cross 
Zero Ratio）。短时平均过零率（Zn）可以作为“频率”来理解。过零率可以用来定量的分析清音/浊音，特别是在背景噪声电平较大时
更为有效（相比短时平均能量而言），有时还可以同时结合Zn和En来进行判定。</p>




<p>如果说短时平均过零率是描述复杂波形“频率”特征的一个参数，那么短时平均上升过零间隔（Short Time Rise Zero-Crossing Inteval）
就是描述复杂波形“周期”特性的参数。研究表明：在一定噪声背景下，该参数具有很好的稳健性，对不同的语音具有很好的差异性。</p>




<p>自相关函数是偶函数，语音信号的短时自相关函数（Short Time Autocorrelation Function）可以理解为序列[x(n)x(n-k)]通过一个
冲激响应为hk(n)的数字滤波器的输出，即有Rn(k) = [x(n)x(n-k)]*hk(n)。短时自相关函数是语音信号时域分析中的一个重要参量，但是
运算量很大。短时平均幅度差函数AMDF（Short Time Average Magnitude Difference Function）与自相关函数有类似的功效，但运算量
可降低许多，所以在语音信号处理中应用广泛。</p>




<p>最后是高阶统计量了。近来高阶统计量在语音信号处理中应用也越来越多，高阶统计量一般指高阶矩(Moment)、高阶累积量(Cumulant)以及
他们的谱——高阶矩谱和高阶累积量谱。首先定义了随机变量x的（第一）特征函数（也称为矩生成函数），实际为它的密度函数f(x)的傅里叶变换。
然后定义了第二特征函数（也称为累积量生成函数），它是第一特征函数的对数。还有随机变量x的k阶矩（mk）的定义，它是x的k次幂与f(x)的
乘积在x∈R上的积分。类似的还有k阶中心矩（μk）的定义，都与概率论中的定义差不多。现在，可以对第一、二特征函数进行泰勒展开，可以得
到ck（x的k阶累积量）和mk之间的一些关系，可以发现k<4时，ck=μk，此时ck的物理意义与μk的物理意义相同，而k>=4时，则不相等。对于c3，
描述了概率分布的对称性，通过定义一个新的概念——偏度（Skewness，也称为偏态系数）来衡量。对于c4，文中为了简化，假设了x的均值为0，
然后定义了一个称为峰态（也称峰度，Kurtosis）的概念，以表示分布相对于正太分布的尖锐或平坦程度。后面两小节分别对此进行了从单个
随机变量到多个随机变量的推广的分析和随机变量服从高斯分布（正态分布）的特殊情形做了分析。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[使用Alize等工具构建说话人识别平台]]></title>
    <link href="http://ibillxia.github.com/blog/2013/04/26/building-speaker-recognition-system-using-alize-etc/"/>
    <updated>2013-04-26T22:07:00+08:00</updated>
    <id>http://ibillxia.github.com/blog/2013/04/26/building-speaker-recognition-system-using-alize-etc</id>
    <content type="html"><![CDATA[<p>前段时间有好几位同学询问如何用Alize实现说话人识别的问题，由于寒假前赶Paper，来不及详细解答，更没时间写Demo。
开学后不久抽时间写了一个Demo，并上传到了GitHub：https://github.com/ibillxia/VoicePrintReco/tree/master/Demo</p>




<p>下面将利用Alize+SPro进行简单的GMM-Based的说话人识别的基本流程总结如下：</br>
1.Features extraction 特征提取</br>
sfbcep.exe（MFCC）或slpcep.exe（LPCC）</br>

2.Silence removal 静音检测和去除</br>
NormFeat.exe 先能量规整</br>
EnergyDetector.exe 基于能量检测的静音去除</br>

3.Features Normalization 特征规整</br>
NormFeat.exe 再使用这个工具进行特征规整</br>

4.World model training</br>
TrainWorld.exe 训练UBM</br>

5.Target model training</br>
TrainWorld.exe 在训练好UBM的基础上训练training set和testing set的GMM</br>

6.Testing</br>
ComputeTest.exe 将testing set 的GMM在training set的GMM上进行测试和打分</br>

7.Score Normalization</br>
ComputeNorm.exe 将得分进行规整</br>

8. Compute EER 计算等错误率</br>
你可以查查计算EER的matlab代码，NIST SRE的官网上有下载（http://www.itl.nist.gov/iad/mig//tools/DETware_v2.1.targz.htm）。</br>
</p>




<!--more-->




<p>关于各步骤中参数的问题，可以在命令行“工具 -help”来查看该工具个参数的具体含义，另外还可参考Alize源码中各个工具的test目录中提供的实例，
而关于每个工具的作用及理论知识则需要查看相关论文。</p>




<p>常见问题及解答: http://mistral.univ-avignon.fr/mediawiki/index.php/Frequently_asked_questions</p>




<p>更多问题请在Google论坛（https://groups.google.com/forum/?fromgroups=&hl=zh-CN#!forum/alize---voice-print-recognition）提出，大家一起讨论！</p>




<h3>推荐资料</h3>


<p>
[1] ALIZE - User Manual: http://mistral.univ-avignon.fr/doc/userguide_alize.001.pdf</br>
[2] LIA_SPKDET Package documentation: http://mistral.univ-avignon.fr/doc/userguide_LIA_SpkDet.002.pdf</br>
[3] Reference System based on speech modality ALIZE/LIA RAL: http://www-clips.imag.fr/geod/User/laurent.besacier/NEW-TPs/TP-Biometrie/tools/CommentsLBInstall/doc.pdf</br>
[4] Jean-Francois Bonastre, etc. ALIZE/SpkDet: a state-of-the-art open source software for speaker recognition</br>
[5] TOMMIE GANNERT. A Speaker Veri?cation System Under The Scope: Alize</br>
[6] Alize Wiki: http://mistral.univ-avignon.fr/mediawiki/index.php/Main_Page
</p>



]]></content>
  </entry>
  
</feed>
