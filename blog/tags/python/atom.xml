<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Tag: Python | Bill's Blog]]></title>
  <link href="http://ibillxia.github.com/blog/tags/python/atom.xml" rel="self"/>
  <link href="http://ibillxia.github.com/"/>
  <updated>2013-05-24T23:18:59+08:00</updated>
  <id>http://ibillxia.github.com/</id>
  <author>
    <name><![CDATA[Bill Xia]]></name>
    <email><![CDATA[ibillxia@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[语音信号处理之时域分析-端点检测及Python实现]]></title>
    <link href="http://ibillxia.github.com/blog/2013/05/22/audio-signal-processing-time-domain-Voice-Activity-Detection/"/>
    <updated>2013-05-22T22:22:00+08:00</updated>
    <id>http://ibillxia.github.com/blog/2013/05/22/audio-signal-processing-time-domain-Voice-Activity-Detection</id>
    <content type="html"><![CDATA[<h2>端点检测</h2>


<p>端点检测（End-Point Detection，EPD）的目标是要决定信号的语音开始和结束的位置，所以又可以称为Speech Detection或Voice Activity Detection（VAD）。
端点检测在语音预处理中扮演着一个非常重要的角色。</p>




<p>常见的端点检测方法大致可以分为如下两类：</br>
（1）时域（Time Domain）的方法：计算量比较小，因此比较容易移植到计算能力较差的嵌入式平台</br>
（a）音量：只使用音量来进行端检，是最简单的方法，但是容易对清音造成误判。另外，不同的音量计算方法得到的结果也不尽相同，至于那种方法更好也没有定论。</br>
（b）音量和过零率：以音量为主，过零率为辅，可以对清音进行较精密的检测。</br>
（2）频域（Frequency Domain）的方法：计算量相对较大。</br>
（a）频谱的变化性（Variance）：有声音的频谱变化较规律，可以作为一个判断标准。</br>
（b）频谱的Entropy：有规律的频谱的Entropy一般较小，这也可以作为一个端检的判断标准。
</p>




<p>下面我们分别从这两个方面来探讨端检的具体方法和过程。</p>




<!--more-->




<h2>时域的端检方法</h2>


<p>时域的端检方法分为只用音量的方法和同时使用音量和过零率的方法。只使用音量的方法最简单计算量也最小，我们只需要设定一个音量阈值，任何音量小于该阈值的帧
被认为是静音（silence）。这种方法的关键在于如何选取这个阈值，一种常用的方法是使用一些带标签的数据来训练得到一个阈值，使得误差最小。</p>




<p>下面我们来看看最简单的、不需要训练的方法，其代码如下：
<div class='bogus-wrapper'><notextile><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>import wave
</span><span class='line'>import numpy as np
</span><span class='line'>import matplotlib.pyplot as plt
</span><span class='line'>import Volume as vp
</span><span class='line'>
</span><span class='line'>def findIndex(vol,thres):
</span><span class='line'>    l = len(vol)
</span><span class='line'>    ii = 0
</span><span class='line'>    index = np.zeros(4,dtype=np.int16)
</span><span class='line'>    for i in range(l-1):
</span><span class='line'>        if((vol[i]-thres)*(vol[i+1]-thres)&lt;0):
</span><span class='line'>            index[ii]=i
</span><span class='line'>            ii = ii+1
</span><span class='line'>    return index[[0,-1]]
</span><span class='line'>
</span><span class='line'>fw = wave.open('sunday.wav','r')
</span><span class='line'>params = fw.getparams()
</span><span class='line'>nchannels, sampwidth, framerate, nframes = params[:4]
</span><span class='line'>strData = fw.readframes(nframes)
</span><span class='line'>waveData = np.fromstring(strData, dtype=np.int16)
</span><span class='line'>waveData = waveData*1.0/max(abs(waveData))  # normalization
</span><span class='line'>fw.close()
</span><span class='line'>
</span><span class='line'>frameSize = 256
</span><span class='line'>overLap = 128
</span><span class='line'>vol = vp.calVolume(waveData,frameSize,overLap)
</span><span class='line'>threshold1 = max(vol)*0.10
</span><span class='line'>threshold2 = min(vol)*10.0
</span><span class='line'>threshold3 = max(vol)*0.05+min(vol)*5.0
</span><span class='line'>
</span><span class='line'>time = np.arange(0,nframes) * (1.0/framerate)
</span><span class='line'>frame = np.arange(0,len(vol)) * (nframes*1.0/len(vol)/framerate)
</span><span class='line'>index1 = findIndex(vol,threshold1)*(nframes*1.0/len(vol)/framerate)
</span><span class='line'>index2 = findIndex(vol,threshold2)*(nframes*1.0/len(vol)/framerate)
</span><span class='line'>index3 = findIndex(vol,threshold3)*(nframes*1.0/len(vol)/framerate)
</span><span class='line'>end = nframes * (1.0/framerate)
</span><span class='line'>
</span><span class='line'>plt.subplot(211)
</span><span class='line'>plt.plot(time,waveData,color="black")
</span><span class='line'>plt.plot([index1,index1],[-1,1],'-r')
</span><span class='line'>plt.plot([index2,index2],[-1,1],'-g')
</span><span class='line'>plt.plot([index3,index3],[-1,1],'-b')
</span><span class='line'>plt.ylabel('Amplitude')
</span><span class='line'>
</span><span class='line'>plt.subplot(212)
</span><span class='line'>plt.plot(frame,vol,color="black")
</span><span class='line'>plt.plot([0,end],[threshold1,threshold1],'-r', label="threshold 1")
</span><span class='line'>plt.plot([0,end],[threshold2,threshold2],'-g', label="threshold 2")
</span><span class='line'>plt.plot([0,end],[threshold3,threshold3],'-b', label="threshold 3")
</span><span class='line'>plt.legend()
</span><span class='line'>plt.ylabel('Volume(absSum)')
</span><span class='line'>plt.xlabel('time(seconds)')
</span><span class='line'>plt.show()</span></code></pre></td></tr></table></div></figure></notextile></div>
其中计算音量的函数calVolume参见<a href="http://ibillxia.github.io/blog/2013/05/15/audio-signal-process-time-domain-volume-python-realization/">
音量及其Python实现</a>一文。程序的运行结果如下图：
<center><img src="/images/2013/IMAG2013052201.png"></center>
</p>




<p>这里采用了三种设置阈值的方法，但这几种设置方法对所有的输入都是相同的，对于一些特定的语音数据可能得不到很好的结果，比如杂音较强、清音较多或音量
变化较大等语音信号，此时单一阈值的方法的效果就不太好了，虽然我们可以通过增加帧与帧之间的重叠部分，但相对而言计算量会比较大。下面我们利用一些更多的
特征来进行端点加测，例如使用过零率等信息，其过程如下：</br>
（1）以较高音量阈值($\tau _{u}$)为标准，找到初步的端点；</br>
（2）将端点前后延伸到低音量阈值($\tau _{l}$)处；</br>
（3）再将端点前后延伸到过零率阈值($\tau _{zc}$)处，以包含语音中清音的部分。</br>
这种方法需要确定三个阈值($\tau _{u}$,$\tau _{l}$,$\tau _{zc}$)，可以用各种搜寻方法来调整这三个参数。其示意图(参考[1])如下：
<center><img src="/images/2013/IMAG2013052202.png"></center>
我们在同一个图中绘制出音量和过零率的阈值图如下：
<center><img src="/images/2013/IMAG2013052203.png"></center>
可以看到我们可以通过过零率的阈值来把错分的清音加入到语音部分来。上图使用到的阈值还是和音量的阈值选取方法相同，比较简单直接。
</p>




<p>另外，我们还可以连续对波形进行微分，再计算音量，这样就可以凸显清音的部分，从而将其正确划分出来，详见参考[1]。</p>




<h2>频域的端检方法</h2>


<p>有声音的信号在频谱上会有重复的谐波结构，因此我们也可以使用频谱的变化性（Variation）或Entropy来进行端点检测，可以参见如下链接：
http://neural.cs.nthu.edu.tw/jang/books/audiosignalprocessing/paper/endPointDetection/</p>




<p>总之，端点检测是语音预处理的重头戏，其实现方法也是五花八门，本文只给出了最简单最原始也最好理解的几种方法，这些方法要真正做到实用，还需要针对一些
特殊的情况在做一些精细的设置和处理，但对于一般的应用场景应该还是基本够用的。</p>




<h2>参考（References）</h2>


<p>
[1]EPD in Time Domain: http://neural.cs.nthu.edu.tw/jang/books/audiosignalprocessing/epdTimeDomain.asp?title=6-2%20EPD%20in%20Time%20Domain</br>
[2]EPD in Frequency Domain: http://neural.cs.nthu.edu.tw/jang/books/audiosignalprocessing/epdFreqDomain.asp?title=6-3%20EPD%20in%20Frequency%20Domain
</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[语音信号处理之时域分析-音色及其Python实现]]></title>
    <link href="http://ibillxia.github.com/blog/2013/05/18/audio-signal-processing-time-domain-timbre-python-realization/"/>
    <updated>2013-05-18T21:57:00+08:00</updated>
    <id>http://ibillxia.github.com/blog/2013/05/18/audio-signal-processing-time-domain-timbre-python-realization</id>
    <content type="html"><![CDATA[<h2>音色（Timbre）</h2>


<p>音色是一个很模糊的概念，它泛指语音的内容，例如“天书”这两个字的发音，虽然都是一声（即他们的音高应该是相同或接近的），
但由于音色不同，我们可以分辨这两个音。直觉而言，音色的不同，意味着基本波形的不同，因此我们可以用基本周期的波形来代表音色。
</p>




<p>若要从基本周期的波形来直接分析音色是一件很困难的事情。通常我们的做法是将每一个帧进行频谱分析（Spectral Analysis），算出一个
帧如何分解为不同频率的分量，然后才能进行对比或分析。在频谱分析中，最常用的方法就是快速傅里叶变换（Fast Fourier Transform，FFT），
这是一个相当常用的方法，可以讲在时域（Time Domain）的信号转换成频域（Frequency Domain）的信号，并进而知道每个频率的信号强度。</p>




<p>语谱图（Spectrogram）就是语音频谱图，一般是通过处理接收的时域信号得到频谱图，因此只要有足够时间长度的时域信号就可以(时间长度
为保证频率分辨率)。专业点讲，语谱图就是频谱分析视图，如果针对语音数据的话，叫语谱图。语谱图的横坐标是时间，纵坐标是频率，坐标点
值为语音数据能量，因而语谱图很好的表达了语音的音色随时间变化的趋势。有些经验丰富的人能够通过看语谱图而知道对应的语音信号的内容，
这种技术成为Spectrogram Reading。</p>




<!--more-->




<h2>Python绘制语谱图</h2>


<p>如果是用Matlab，绘制语谱图并不难，网上资料也一堆一堆的。但是，如果要想用Python来绘制呢？网上相关资料很少很少，万幸中找到了参考[4]，
但是，[4]中提供的程序是不能运行的，还需要安装几个库，特别是Audiolab这个，折腾了我好半天，最终安装了，但运行时发现这个audiolab根本无法
import进来，因为ms与numpy的版本有冲突，出现了什么“numpy.dtype does not appear to be the correct type object”，弄了好半天也没有解决，
后来才发现其实不需要audiolab也可以的，因为其实audiolab只是读取不同格式（扩展名）的语音文件的一个lib而已，并不涉及到绘制语谱图的东西。</p>




<p>
闲话少说了，上代码吧，其实看看这代码也挺简单的，就调一个matplotlib.pyplot.specgram()就可以了。
<div class='bogus-wrapper'><notextile><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>import wave
</span><span class='line'>import numpy as np
</span><span class='line'>import matplotlib.pyplot as plt
</span><span class='line'>
</span><span class='line'>fw = wave.open('aeiou.wav','r')
</span><span class='line'>soundInfo = fw.readframes(-1)
</span><span class='line'>soundInfo = np.fromstring(soundInfo,np.int16)
</span><span class='line'>f = fw.getframerate()
</span><span class='line'>fw.close()
</span><span class='line'>
</span><span class='line'>plt.subplot(211)
</span><span class='line'>plt.plot(soundInfo)
</span><span class='line'>plt.ylabel('Amplitude')
</span><span class='line'>plt.title('Wave from and spectrogram of aeiou.wav')
</span><span class='line'>
</span><span class='line'>plt.subplot(212)
</span><span class='line'>plt.specgram(soundInfo,Fs = f, scale_by_freq = True, sides = 'default')
</span><span class='line'>plt.ylabel('Frequency')
</span><span class='line'>plt.xlabel('time(seconds)')
</span><span class='line'>plt.show()</span></code></pre></td></tr></table></div></figure></notextile></div>
</p>




<p>程序运行的效果如下图：
<center><img src="/images/2013/IMAG2013051801.png"></center>
虽然程序简单，但还有一些小bug，比如subplot(212)的xlabel和ylabel无法显示，这个问题暂时还没有解决。（更新：这个问题已解决，把mpp.show()放到
最后一行就可以了，顺便图也更新了）</p>




<p>另外，就是关于这个语谱图具体是如何绘制的，这一点涉及到FFT和短时能量的计算，短时能量在<a href="">前文中</a>
已经讲过了，这里不再赘述。关于FFT将在后续文章中讨论。</p>




<h2>参考（References）</h2>


<p>
[1]Timbre (音色): http://neural.cs.nthu.edu.tw/jang/books/audiosignalprocessing/basicFeatureTimber.asp?title=5-5</br>
[2]Wiki - 音色: http://zh.wikipedia.org/wiki/音色</br>
[3]语谱图： http://blog.csdn.net/wuxiaoer717/article/details/6941339</br>
[4]How to plot spectrogram with Python：http://jaganadhg.freeflux.net/blog/archive/2009/07/23/how-to-plot-spectrogram-with-python.html
</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[语音信号处理之时域分析-音高及其Python实现]]></title>
    <link href="http://ibillxia.github.com/blog/2013/05/16/audio-signal-processing-time-domain-pitch-python-realization/"/>
    <updated>2013-05-16T23:10:00+08:00</updated>
    <id>http://ibillxia.github.com/blog/2013/05/16/audio-signal-processing-time-domain-pitch-python-realization</id>
    <content type="html"><![CDATA[<h2>音高（Pitch）</h2>


<p>概念：音高（Pitch）是语音信号的一个很重要的特征，直觉上而言它表示声音频率的高低，这个频率是指基本频率（基频），也即基本周期的倒数。
若直接观察语音的波形，只要语音信号稳定，我们可以很容易的看出基本周期的存在。例如我们取一个包含256个采样点的帧，单独绘制波形图，就可以明显的
看到它的基本周期。如下图所示：
<center><img src="/images/2013/IMAG2013051601.png"></center>
其中最上面的波形为|a|的发音，中间的为上图中红色双竖线（位于语音区）所对应的帧的具体波形，而最下面的是上图中绿色双竖线（位于静音区）所
对应的帧的具体波形。很容易看到中间的波形具有明显的周期性。
</p>


<!--more-->


<p>其代码如下：
<div class='bogus-wrapper'><notextile><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>import wave
</span><span class='line'>import numpy as np
</span><span class='line'>import pylab as pl
</span><span class='line'>
</span><span class='line'># ============ test the algorithm =============
</span><span class='line'># read wave file and get parameters.
</span><span class='line'>fw = wave.open('a.wav','rb')
</span><span class='line'>params = fw.getparams()
</span><span class='line'>print(params)
</span><span class='line'>nchannels, sampwidth, framerate, nframes = params[:4]
</span><span class='line'>strData = fw.readframes(nframes)
</span><span class='line'>waveData = np.fromstring(strData, dtype=np.int16)
</span><span class='line'>waveData = waveData*1.0/max(abs(waveData))  # normalization
</span><span class='line'>fw.close()
</span><span class='line'>
</span><span class='line'># plot the wave
</span><span class='line'>time = np.arange(0, len(waveData)) * (1.0 / framerate)
</span><span class='line'>
</span><span class='line'>index1 = 10000.0 / framerate
</span><span class='line'>index2 = 10512.0 / framerate
</span><span class='line'>index3 = 15000.0 / framerate
</span><span class='line'>index4 = 15512.0 / framerate
</span><span class='line'>
</span><span class='line'>pl.subplot(311)
</span><span class='line'>pl.plot(time, waveData)
</span><span class='line'>pl.plot([index1,index1],[-1,1],'r')
</span><span class='line'>pl.plot([index2,index2],[-1,1],'r')
</span><span class='line'>pl.plot([index3,index3],[-1,1],'g')
</span><span class='line'>pl.plot([index4,index4],[-1,1],'g')
</span><span class='line'>pl.xlabel("time (seconds)")
</span><span class='line'>pl.ylabel("Amplitude")
</span><span class='line'>
</span><span class='line'>pl.subplot(312)
</span><span class='line'>pl.plot(np.arange(512),waveData[10000:10512],'r')
</span><span class='line'>pl.plot([59,59],[-1,1],'b')
</span><span class='line'>pl.plot([169,169],[-1,1],'b')
</span><span class='line'>print(1/( (169-59)*1.0/framerate ))
</span><span class='line'>pl.xlabel("index in 1 frame")
</span><span class='line'>pl.ylabel("Amplitude")
</span><span class='line'>
</span><span class='line'>pl.subplot(313)
</span><span class='line'>pl.plot(np.arange(512),waveData[15000:15512],'g')
</span><span class='line'>pl.xlabel("index in 1 frame")
</span><span class='line'>pl.ylabel("Amplitude")
</span><span class='line'>pl.show()</span></code></pre></td></tr></table></div></figure></notextile></div>
</p>




<p>根据参考[1]，可以通过观察一帧的波形图来计算基音频率（感觉这种方法有点奇葩，不过很直观。例如这里的基频为：1/( (169-59)*1.0/framerate )=145.45Hz），
然后还可以计算半音（semitone，可以参见[2]），进而得到pitch与semitone的关系。[1]中还提到了钢琴的半音差，DS表示完全看不懂啊，有木有！！！</p>




<p>参考[2]中还简单介绍了如何改变音高、扩展音域，以及如何改变乐器的振动的弦的音高（通过改变弦长、张力、密度等），感兴趣的可以看看。</p>




<p>另外，由于生理结构的差异，男女性的音高范围不尽相同，一般而言：</br>
·男性的音高范围是35~72半音，对应的频率范围是62~523Hz；</br>
·女性的音高范围是45~83半音，对应的频率范围是110~1000Hz。</br>
然而，我们分辨男女的声音并不是只根据音高，还要根据音色（也即共振峰，下一篇文章中将详细介绍）。
</p>




<p>关于音高的计算，目前有很多种算法，具体将会在后续文章中详细介绍。</p>




<h2>参考（References）</h2>


<p>
[1]Pitch (音高): http://neural.cs.nthu.edu.tw/jang/books/audiosignalprocessing/basicFeaturePitch.asp</br>
[2]Wiki： http://zh.wikipedia.org/wiki/音高
</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[语音信号处理之时域分析-过零率及其Python实现]]></title>
    <link href="http://ibillxia.github.com/blog/2013/05/15/audio-signal-processing-time-domain-ZeroCR-python-realization/"/>
    <updated>2013-05-15T21:44:00+08:00</updated>
    <id>http://ibillxia.github.com/blog/2013/05/15/audio-signal-processing-time-domain-ZeroCR-python-realization</id>
    <content type="html"><![CDATA[<h2>过零率（Zero Crossing Rate）</h2>


<p>概念：过零率（Zero Crossing Rate，ZCR）是指在每帧中，语音信号通过零点（从正变为负或从负变为正）的次数。
这个特征已在语音识别和音乐信息检索领域得到广泛使用，是对敲击的声音的分类的关键特征。</p>




<p>ZCR的数学形式化定义为：
<center>$zcr = \frac{1}{T-1}\sum_{t=1}^{T-1}\pi\{s_{t}s_{t-1}<0\}$.</center>
其中$s$是采样点的值，$T$为帧长，函数$\pi\{A\}$在A为真是值为1，否则为0.
</p>




<p>特性：</br>
(1).一般而言，清音（unvoiced sound）和环境噪音的ZCR都大于浊音（voiced sound）；</br>
(2).由于清音和环境噪音的ZCR大小相近，因而不能够通过ZCR来区分它们；</br>
(3).在实际当中，过零率经常与短时能量特性相结合来进行端点检测，尤其是ZCR用来检测清音的起止点；</br>
(4).有时也可以用ZCR来进行粗略的基频估算，但这是非常不可靠的，除非有后续的修正（refine）处理过程。
</p>




<!--more-->




<h2>ZCR的Python实现</h2>


<p>ZCR的Python实现如下：
<div class='bogus-wrapper'><notextile><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>import math
</span><span class='line'>import numpy as np
</span><span class='line'>
</span><span class='line'>def ZeroCR(waveData,frameSize,overLap):
</span><span class='line'>    wlen = len(waveData)
</span><span class='line'>    step = frameSize - overLap
</span><span class='line'>    frameNum = math.ceil(wlen/step)
</span><span class='line'>    zcr = np.zeros((frameNum,1))
</span><span class='line'>    for i in range(frameNum):
</span><span class='line'>        curFrame = waveData[np.arange(i*step,min(i*step+frameSize,wlen))]
</span><span class='line'>        #To avoid DC bias, usually we need to perform mean subtraction on each frame
</span><span class='line'>        #ref: http://neural.cs.nthu.edu.tw/jang/books/audiosignalprocessing/basicFeatureZeroCrossingRate.asp
</span><span class='line'>        curFrame = curFrame - np.mean(curFrame) # zero-justified
</span><span class='line'>        zcr[i] = sum(curFrame[0:-1]*curFrame[1::]&lt;=0)
</span><span class='line'>    return zcr</span></code></pre></td></tr></table></div></figure></notextile></div>
</p>

<p>对于给定语音文件aeiou.wav，利用上面的函数计算ZCR的代码如下：
<div class='bogus-wrapper'><notextile><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>import math
</span><span class='line'>import wave
</span><span class='line'>import numpy as np
</span><span class='line'>import pylab as pl
</span><span class='line'>
</span><span class='line'># ============ test the algorithm =============
</span><span class='line'># read wave file and get parameters.
</span><span class='line'>fw = wave.open('aeiou.wav','rb')
</span><span class='line'>params = fw.getparams()
</span><span class='line'>print(params)
</span><span class='line'>nchannels, sampwidth, framerate, nframes = params[:4]
</span><span class='line'>str_data = fw.readframes(nframes)
</span><span class='line'>wave_data = np.fromstring(str_data, dtype=np.short)
</span><span class='line'>wave_data.shape = -1, 1
</span><span class='line'>#wave_data = wave_data.T
</span><span class='line'>fw.close()
</span><span class='line'>
</span><span class='line'># calculate Zero Cross Rate
</span><span class='line'>frameSize = 256
</span><span class='line'>overLap = 0
</span><span class='line'>zcr = ZeroCR(wave_data,frameSize,overLap)
</span><span class='line'>
</span><span class='line'># plot the wave
</span><span class='line'>time = np.arange(0, len(wave_data)) * (1.0 / framerate)
</span><span class='line'>time2 = np.arange(0, len(zcr)) * (len(wave_data)/len(zcr) / framerate)
</span><span class='line'>pl.subplot(211)
</span><span class='line'>pl.plot(time, wave_data)
</span><span class='line'>pl.ylabel("Amplitude")
</span><span class='line'>pl.subplot(212)
</span><span class='line'>pl.plot(time2, zcr)
</span><span class='line'>pl.ylabel("ZCR")
</span><span class='line'>pl.xlabel("time (seconds)")
</span><span class='line'>pl.show()</span></code></pre></td></tr></table></div></figure></notextile></div>
</p>

<p>运行以上程序得到下图：
<center><img src="/images/2013/IMAG2013051502.png"></center>
</p>

<h2>参考（References）</h2>
<p>
[1]Zero Crossing Rate (過零率): http://neural.cs.nthu.edu.tw/jang/books/audiosignalprocessing/basicFeatureZeroCrossingRate.asp?title=5-3%20Zero%20Crossing%20Rate%20(%B9L%B9s%B2v)&language=english</br>
[2]Wiki: http://zh.wikipedia.org/zh/过零率
</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[语音信号处理之时域分析-音量及其Python实现]]></title>
    <link href="http://ibillxia.github.com/blog/2013/05/15/audio-signal-process-time-domain-volume-python-realization/"/>
    <updated>2013-05-15T19:36:00+08:00</updated>
    <id>http://ibillxia.github.com/blog/2013/05/15/audio-signal-process-time-domain-volume-python-realization</id>
    <content type="html"><![CDATA[<h2>1.概述（Introduction）</h2>


<p>本系列文主要介绍语音信号时域的4个基本特征及其Python实现，这4个基本特征是：</br>
(1)音量（Volume）；</br>
(2)过零率（Zero-Crossing-Rate）；</br>
(3)音高（Pitch）；</br>
(4)音色（Timbre）。
</p>




<h2>2.音量（Volume）</h2>


<p>音量代表声音的强度，可由一个窗口或一帧内信号振幅的大小来衡量，一般有两种度量方法：</br>
（1）每个帧的振幅的绝对值的总和：
<center>$volume = \sum_{i=1}^{n}|s_{i}|$.</center>
其中$s_{i}$为第该帧的$i$个采样点，$n$为该帧总的采样点数。这种度量方法的计算量小，但不太符合人的听觉感受。</br>
（2）幅值平方和的常数对数的10倍：
<center>$volume = 10 * log_{10}\sum_{i=1}^{n}s_{i}^{2}$.</center>
它的单位是分贝（Decibels），是一个对数强度值，比较符合人耳对声音大小的感觉，但计算量稍复杂。
</p>


<!--more-->


<p>音量计算的Python实现如下：</p>


<p><div class='bogus-wrapper'><notextile><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>import math
</span><span class='line'>import numpy as np&lt;/p>
</span><span class='line'>
</span><span class='line'>&lt;h1>method 1: absSum&lt;/h1>
</span><span class='line'>
</span><span class='line'>&lt;p>def calVolume(waveData, frameSize, overLap):&lt;/p>
</span><span class='line'>
</span><span class='line'>&lt;pre>&lt;code>wlen = len(waveData)
</span><span class='line'>step = frameSize - overLap
</span><span class='line'>frameNum = int(math.ceil(wlen*1.0/step))
</span><span class='line'>volume = np.zeros((frameNum,1))
</span><span class='line'>for i in range(frameNum):
</span><span class='line'>    curFrame = waveData[np.arange(i*step,min(i*step+frameSize,wlen))]
</span><span class='line'>    curFrame = curFrame - np.median(curFrame) # zero-justified
</span><span class='line'>    volume[i] = np.sum(np.abs(curFrame))
</span><span class='line'>return volume
</span><span class='line'>&lt;/code>&lt;/pre>
</span><span class='line'>
</span><span class='line'>&lt;h1>method 2: 10 times log10 of square sum&lt;/h1>
</span><span class='line'>
</span><span class='line'>&lt;p>def calVolumeDB(waveData, frameSize, overLap):&lt;/p>
</span><span class='line'>
</span><span class='line'>&lt;pre>&lt;code>wlen = len(waveData)
</span><span class='line'>step = frameSize - overLap
</span><span class='line'>frameNum = int(math.ceil(wlen*1.0/step))
</span><span class='line'>volume = np.zeros((frameNum,1))
</span><span class='line'>for i in range(frameNum):
</span><span class='line'>    curFrame = waveData[np.arange(i*step,min(i*step+frameSize,wlen))]
</span><span class='line'>    curFrame = curFrame - np.mean(curFrame) # zero-justified
</span><span class='line'>    volume[i] = 10*np.log10(np.sum(curFrame*curFrame))
</span><span class='line'>return volume
</span><span class='line'>&lt;/code>&lt;/pre>
</span><span class='line'>
</span><span class='line'>&lt;p></span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p>对于给定语音文件aeiou.wav，利用上面的函数计算音量曲线的代码如下：</p>


<p><div class='bogus-wrapper'><notextile><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>import wave
</span><span class='line'>import pylab as pl
</span><span class='line'>import numpy as np
</span><span class='line'>import Volume as vp&lt;/p>
</span><span class='line'>
</span><span class='line'>&lt;h1>============ test the algorithm =============&lt;/h1>
</span><span class='line'>
</span><span class='line'>&lt;h1>read wave file and get parameters.&lt;/h1>
</span><span class='line'>
</span><span class='line'>&lt;p>fw = wave.open('aeiou.wav','r')
</span><span class='line'>params = fw.getparams()
</span><span class='line'>print(params)
</span><span class='line'>nchannels, sampwidth, framerate, nframes = params[:4]
</span><span class='line'>strData = fw.readframes(nframes)
</span><span class='line'>waveData = np.fromstring(strData, dtype=np.int16)
</span><span class='line'>waveData = waveData*1.0/max(abs(waveData))  # normalization
</span><span class='line'>fw.close()&lt;/p>
</span><span class='line'>
</span><span class='line'>&lt;h1>calculate volume&lt;/h1>
</span><span class='line'>
</span><span class='line'>&lt;p>frameSize = 256
</span><span class='line'>overLap = 128
</span><span class='line'>volume11 = vp.calVolume(waveData,frameSize,overLap)
</span><span class='line'>volume12 = vp.calVolumeDB(waveData,frameSize,overLap)&lt;/p>
</span><span class='line'>
</span><span class='line'>&lt;h1>plot the wave&lt;/h1>
</span><span class='line'>
</span><span class='line'>&lt;p>time = np.arange(0, nframes)&lt;em>(1.0/framerate)
</span><span class='line'>time2 = np.arange(0, len(volume11))&lt;/em>(frameSize-overLap)*1.0/framerate
</span><span class='line'>pl.subplot(311)
</span><span class='line'>pl.plot(time, waveData)
</span><span class='line'>pl.ylabel("Amplitude")
</span><span class='line'>pl.subplot(312)
</span><span class='line'>pl.plot(time2, volume11)
</span><span class='line'>pl.ylabel("absSum")
</span><span class='line'>pl.subplot(313)
</span><span class='line'>pl.plot(time2, volume12, c="g")
</span><span class='line'>pl.ylabel("Decibel(dB)")
</span><span class='line'>pl.xlabel("time (seconds)")
</span><span class='line'>pl.show()</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p>运行以上程序得到下图：
<center><img src="/images/2013/IMAG2013051501.png"></center>
</p>




<h2>参考（References）</h2>


<p>[1]Volume (音量):http://neural.cs.nthu.edu.tw/jang/books/audiosignalprocessing/basicFeatureVolume.asp?title=5-2%20Volume%20(%AD%B5%B6q)</br>
[2]用Python做科学计算-声音的输入输出:http://hyry.dip.jp:8000/pydoc/wave_pyaudio.html</p>

]]></content>
  </entry>
  
</feed>
